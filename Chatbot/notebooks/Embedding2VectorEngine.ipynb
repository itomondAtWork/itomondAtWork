{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from gen_ai_hub.proxy.core.proxy_clients import get_proxy_client\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# トークンを取得する関数\n",
    "def get_token():\n",
    "    auth_url = os.getenv('AICORE_AUTH_URL')\n",
    "    client_id = os.getenv('AICORE_CLIENT_ID')\n",
    "    client_secret = os.getenv('AICORE_CLIENT_SECRET')\n",
    "\n",
    "    token_url = f\"{auth_url}/oauth/token\"\n",
    "    data = {\n",
    "        'grant_type': 'client_credentials',\n",
    "        'client_id': client_id,\n",
    "        'client_secret': client_secret\n",
    "    }\n",
    "\n",
    "    response = requests.post(token_url, data=data)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    token = response.json().get('access_token')\n",
    "    if not token:\n",
    "        raise ValueError(\"トークンが取得できませんでした。\")\n",
    "\n",
    "    return token\n",
    "\n",
    "# トークンを取得\n",
    "token = get_token()\n",
    "print(f'Token: {token}')\n",
    "\n",
    "# プロキシクライアントの取得\n",
    "proxy_client = get_proxy_client('gen-ai-hub', token=token)\n",
    "\n",
    "# デプロイメント一覧を取得\n",
    "deployments_list = proxy_client.deployments\n",
    "print(\"model_name, deployment_id, config_name\")\n",
    "for deployment in deployments_list:\n",
    "    print(f\"{deployment.model_name}, {deployment.deployment_id}, {deployment.config_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain.openai import OpenAIEmbeddings\n",
    "\n",
    "# 取得したデプロイメントIDを使用\n",
    "deployment_id = \"d8b1e5eb7341b1f3\"\n",
    "\n",
    "# embedding_modelの設定\n",
    "embedding_model = OpenAIEmbeddings(deployment_id=deployment_id, proxy_client=proxy_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from hdbcli import dbapi\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain.vectorstores import HanaDB\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "from gen_ai_hub.proxy.langchain.openai import OpenAIEmbeddings\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ログの設定\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 環境変数の読み込み\n",
    "load_dotenv()\n",
    "\n",
    "# 埋め込みモデルの設定\n",
    "embedding_model = OpenAIEmbeddings(deployment_id=os.getenv('EMBEDDING_DEPLOYMENT_ID'))\n",
    "\n",
    "def get_embedding(input_text):\n",
    "    return embedding_model.embed_query(input_text)\n",
    "\n",
    "# データベースの接続と初期化\n",
    "def connect_database():\n",
    "    try:\n",
    "        connection = dbapi.connect(\n",
    "            address=os.getenv(\"HANA_DB_ADDRESS\"),\n",
    "            port=int(os.getenv(\"HANA_DB_PORT\")),  # ポートは整数に変換\n",
    "            user=os.getenv(\"HANA_DB_USER\"),\n",
    "            password=os.getenv(\"HANA_DB_PASSWORD\"),\n",
    "            autocommit=True,\n",
    "            sslValidateCertificate=False,\n",
    "        )\n",
    "        db = HanaDB(\n",
    "            connection=connection,\n",
    "            embedding=embedding_model,\n",
    "            table_name=\"FILE_EMBEDDINGS\",\n",
    "        )\n",
    "        db.delete(filter={})  # データベースの初期化\n",
    "        logging.info(\"データベースに接続し、初期化しました。\")\n",
    "        return connection, db\n",
    "    except Exception as e:\n",
    "        logging.error(f\"データベース接続エラー: {e}\")\n",
    "        raise\n",
    "\n",
    "# JSONファイルを処理してDBに格納\n",
    "def process_file(file_path, db, text_splitter, chunk_file_name):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            metadata = data.get('metadata', {})\n",
    "\n",
    "        loader = JSONLoader(file_path, jq_schema='.content', text_content=False)\n",
    "        documents = loader.load()\n",
    "        for doc in documents:\n",
    "            if doc.page_content:  # 空でないことを確認\n",
    "                if os.path.basename(file_path) == chunk_file_name:\n",
    "                    chunks = text_splitter.split_text(doc.page_content)\n",
    "                    for i, chunk in tqdm(enumerate(chunks)):\n",
    "                        chunk_doc = doc.copy()  # 元の文書オブジェクトをコピー\n",
    "                        chunk_doc.page_content = chunk\n",
    "                        embedding = get_embedding(chunk)\n",
    "                        chunk_doc.metadata['embedding'] = embedding\n",
    "                        chunk_doc.metadata['source_filename'] = metadata['filename']\n",
    "                        chunk_doc.metadata['chunk_id'] = f\"{metadata['filename']}_chunk_{i+1}\"\n",
    "                        db.add_documents([chunk_doc])\n",
    "                else:\n",
    "                    doc.metadata['embedding'] = get_embedding(doc.page_content)\n",
    "                    doc.metadata['source_filename'] = metadata['filename']\n",
    "                    db.add_documents([doc])\n",
    "\n",
    "                logging.info(f\"Processed and added JSON file: {file_path}\")\n",
    "            else:\n",
    "                logging.warning(f\"Skipping empty document: {file_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "def process_json_files(json_folder, db, chunk_file_name):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=100)  # チャンクサイズとオーバーラップを設定\n",
    "    json_files = [os.path.join(json_folder, file) for file in os.listdir(json_folder) if not os.path.isdir(os.path.join(json_folder, file))]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [executor.submit(process_file, file_path, db, text_splitter, chunk_file_name) for file_path in json_files]\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing JSON files\"):\n",
    "            future.result()  # 例外をキャッチするため\n",
    "\n",
    "# メインフロー\n",
    "def main():\n",
    "    json_folder = \"../data/all_JSONs\"\n",
    "    chunk_file_name = \"SAPAICORE.json\"  # チャンクしたいファイル名を指定\n",
    "    try:\n",
    "        connection, db = connect_database()\n",
    "        process_json_files(json_folder, db, chunk_file_name)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"エラーが発生しました: {e}\")\n",
    "    finally:\n",
    "        connection.close()\n",
    "        logging.info(\"データベース接続を閉じました。\")\n",
    "\n",
    "# 実行部分\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
