<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html><head><title>Python: module gen_ai_hub.proxy.langchain.openai</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head><body bgcolor="#f0f0f8">

<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="heading">
<tr bgcolor="#7799ee">
<td valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial">&nbsp;<br><big><big><strong><a href="gen_ai_hub.html"><font color="#ffffff">gen_ai_hub</font></a>.<a href="gen_ai_hub.proxy.html"><font color="#ffffff">proxy</font></a>.<a href="gen_ai_hub.proxy.langchain.html"><font color="#ffffff">langchain</font></a>.openai</strong></big></big></font></td
><td align=right valign=bottom
><font color="#ffffff" face="helvetica, arial"><a href=".">index</a><br><a href="file:/home/jenkins/agent/workspace/ation_generative-ai-hub-sdk_main/gen_ai_hub/proxy/langchain/openai.py">/home/jenkins/agent/workspace/ation_generative-ai-hub-sdk_main/gen_ai_hub/proxy/langchain/openai.py</a></font></td></tr></table>
    <p></p>
<p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ee77aa">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Classes</strong></big></font></td></tr>
    
<tr><td bgcolor="#ee77aa"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><dl>
<dt><font face="helvetica, arial"><a href="gen_ai_hub.proxy.langchain.base.html#BaseAuth">gen_ai_hub.proxy.langchain.base.BaseAuth</a>(<a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a>)
</font></dt><dd>
<dl>
<dt><font face="helvetica, arial"><a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a>
</font></dt><dd>
<dl>
<dt><font face="helvetica, arial"><a href="gen_ai_hub.proxy.langchain.openai.html#ChatOpenAI">ChatOpenAI</a>(<a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a>, <a href="langchain_openai.chat_models.base.html#ChatOpenAI">langchain_openai.chat_models.base.ChatOpenAI</a>)
</font></dt><dt><font face="helvetica, arial"><a href="gen_ai_hub.proxy.langchain.openai.html#OpenAI">OpenAI</a>(<a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a>, <a href="langchain_openai.llms.base.html#OpenAI">langchain_openai.llms.base.OpenAI</a>)
</font></dt><dt><font face="helvetica, arial"><a href="gen_ai_hub.proxy.langchain.openai.html#OpenAIEmbeddings">OpenAIEmbeddings</a>(<a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a>, <a href="langchain_openai.embeddings.base.html#OpenAIEmbeddings">langchain_openai.embeddings.base.OpenAIEmbeddings</a>)
</font></dt></dl>
</dd>
</dl>
</dd>
</dl>
 <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="ChatOpenAI">class <strong>ChatOpenAI</strong></a>(<a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a>, <a href="langchain_openai.chat_models.base.html#ChatOpenAI">langchain_openai.chat_models.base.ChatOpenAI</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt><a href="#ChatOpenAI">ChatOpenAI</a>(*args,&nbsp;name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;cache:&nbsp;Optional[bool]&nbsp;=&nbsp;None,&nbsp;verbose:&nbsp;bool&nbsp;=&nbsp;None,&nbsp;callbacks:&nbsp;Union[List[langchain_core.callbacks.base.BaseCallbackHandler],&nbsp;langchain_core.callbacks.base.BaseCallbackManager,&nbsp;NoneType]&nbsp;=&nbsp;None,&nbsp;callback_manager:&nbsp;Optional[langchain_core.callbacks.base.BaseCallbackManager]&nbsp;=&nbsp;None,&nbsp;tags:&nbsp;Optional[List[str]]&nbsp;=&nbsp;None,&nbsp;metadata:&nbsp;Optional[Dict[str,&nbsp;Any]]&nbsp;=&nbsp;None,&nbsp;client:&nbsp;Any&nbsp;=&nbsp;None,&nbsp;async_client:&nbsp;Any&nbsp;=&nbsp;None,&nbsp;model_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;temperature:&nbsp;float&nbsp;=&nbsp;0.7,&nbsp;model_kwargs:&nbsp;Dict[str,&nbsp;Any]&nbsp;=&nbsp;None,&nbsp;api_key:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;base_url:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;organization:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;openai_proxy:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;timeout:&nbsp;Union[float,&nbsp;Tuple[float,&nbsp;float],&nbsp;Any,&nbsp;NoneType]&nbsp;=&nbsp;None,&nbsp;max_retries:&nbsp;int&nbsp;=&nbsp;2,&nbsp;streaming:&nbsp;bool&nbsp;=&nbsp;False,&nbsp;n:&nbsp;int&nbsp;=&nbsp;1,&nbsp;max_tokens:&nbsp;Optional[int]&nbsp;=&nbsp;None,&nbsp;tiktoken_model_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;default_headers:&nbsp;Optional[Mapping[str,&nbsp;str]]&nbsp;=&nbsp;None,&nbsp;default_query:&nbsp;Optional[Mapping[str,&nbsp;object]]&nbsp;=&nbsp;None,&nbsp;http_client:&nbsp;Optional[Any]&nbsp;=&nbsp;None,&nbsp;proxy_client:&nbsp;Optional[Any]&nbsp;=&nbsp;None,&nbsp;deployment_id:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;config_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;config_id:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;proxy_model_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;api_version:&nbsp;Optional[str]&nbsp;=&nbsp;None)&nbsp;-&amp;gt;&nbsp;None<br>
&nbsp;<br>
<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%"><dl><dt>Method resolution order:</dt>
<dd><a href="gen_ai_hub.proxy.langchain.openai.html#ChatOpenAI">ChatOpenAI</a></dd>
<dd><a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a></dd>
<dd><a href="gen_ai_hub.proxy.langchain.base.html#BaseAuth">gen_ai_hub.proxy.langchain.base.BaseAuth</a></dd>
<dd><a href="langchain_openai.chat_models.base.html#ChatOpenAI">langchain_openai.chat_models.base.ChatOpenAI</a></dd>
<dd><a href="langchain_core.language_models.chat_models.html#BaseChatModel">langchain_core.language_models.chat_models.BaseChatModel</a></dd>
<dd><a href="langchain_core.language_models.base.html#BaseLanguageModel">langchain_core.language_models.base.BaseLanguageModel</a></dd>
<dd><a href="langchain_core.runnables.base.html#RunnableSerializable">langchain_core.runnables.base.RunnableSerializable</a></dd>
<dd><a href="langchain_core.load.serializable.html#Serializable">langchain_core.load.serializable.Serializable</a></dd>
<dd><a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a></dd>
<dd><a href="pydantic.v1.utils.html#Representation">pydantic.v1.utils.Representation</a></dd>
<dd><a href="langchain_core.runnables.base.html#Runnable">langchain_core.runnables.base.Runnable</a></dd>
<dd><a href="typing.html#Generic">typing.Generic</a></dd>
<dd><a href="abc.html#ABC">abc.ABC</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="ChatOpenAI-__init__"><strong>__init__</strong></a>(self, *args, **kwargs)</dt><dd><tt>Create&nbsp;a&nbsp;new&nbsp;model&nbsp;by&nbsp;parsing&nbsp;and&nbsp;validating&nbsp;input&nbsp;data&nbsp;from&nbsp;keyword&nbsp;arguments.<br>
&nbsp;<br>
Raises&nbsp;ValidationError&nbsp;if&nbsp;the&nbsp;input&nbsp;data&nbsp;cannot&nbsp;be&nbsp;parsed&nbsp;to&nbsp;form&nbsp;a&nbsp;valid&nbsp;model.</tt></dd></dl>

<hr>
Class methods defined here:<br>
<dl><dt><a name="ChatOpenAI-validate_environment"><strong>validate_environment</strong></a>(values: Dict) -&gt; Dict<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Validate&nbsp;that&nbsp;api&nbsp;key&nbsp;and&nbsp;python&nbsp;package&nbsp;exists&nbsp;in&nbsp;environment.</tt></dd></dl>

<hr>
Static methods defined here:<br>
<dl><dt><a name="ChatOpenAI-__json_encoder__"><strong>__json_encoder__</strong></a> = pydantic_encoder(obj: Any) -&gt; Any</dt></dl>

<dl><dt><a name="ChatOpenAI-__new__"><strong>__new__</strong></a>(cls, **data: Any)</dt><dd><tt>Initialize&nbsp;the&nbsp;<a href="#OpenAI">OpenAI</a>&nbsp;object.<br>
:param&nbsp;data:&nbsp;Additional&nbsp;data&nbsp;to&nbsp;initialize&nbsp;the&nbsp;object<br>
:type&nbsp;data:&nbsp;Any<br>
:return:&nbsp;The&nbsp;initialized&nbsp;<a href="#OpenAI">OpenAI</a>&nbsp;object<br>
:rtype:&nbsp;OpenAIBase</tt></dd></dl>

<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>__abstractmethods__</strong> = frozenset()</dl>

<dl><dt><strong>__annotations__</strong> = {'model_name': typing.Optional[str], 'openai_api_version': typing.Optional[str]}</dl>

<dl><dt><strong>__class_vars__</strong> = set()</dl>

<dl><dt><strong>__config__</strong> = &lt;class 'pydantic.v1.config.Config'&gt;</dl>

<dl><dt><strong>__custom_root_type__</strong> = False</dl>

<dl><dt><strong>__exclude_fields__</strong> = {'async_client': True, 'callback_manager': True, 'callbacks': True, 'client': True, 'metadata': True, 'tags': True}</dl>

<dl><dt><strong>__fields__</strong> = {'async_client': ModelField(name='async_client', type=Optional[Any], required=False, default=None), 'cache': ModelField(name='cache', type=Optional[bool], required=False, default=None), 'callback_manager': ModelField(name='callback_manager', type=Optiona...seCallbackManager], required=False, default=None), 'callbacks': ModelField(name='callbacks', type=Union[List[lan...Manager, NoneType], required=False, default=None), 'client': ModelField(name='client', type=Optional[Any], required=False, default=None), 'config_id': ModelField(name='config_id', type=Optional[str], required=False, default=None), 'config_name': ModelField(name='config_name', type=Optional[str], required=False, default=None), 'default_headers': ModelField(name='default_headers', type=Optional[Mapping[str, str]], required=False, default=None), 'default_query': ModelField(name='default_query', type=Optional[Mapping[str, object]], required=False, default=None), 'deployment_id': ModelField(name='deployment_id', type=Optional[str], required=False, default=None), ...}</dl>

<dl><dt><strong>__hash__</strong> = None</dl>

<dl><dt><strong>__include_fields__</strong> = None</dl>

<dl><dt><strong>__parameters__</strong> = ()</dl>

<dl><dt><strong>__post_root_validators__</strong> = [(False, &lt;function BaseChatModel.raise_deprecation&gt;), (False, &lt;function ChatOpenAI.validate_environment&gt;)]</dl>

<dl><dt><strong>__pre_root_validators__</strong> = [&lt;function ChatOpenAI.build_extra&gt;]</dl>

<dl><dt><strong>__private_attributes__</strong> = {'_lc_kwargs': ModelPrivateAttr(default=PydanticUndefined, default_factory=&lt;class 'dict'&gt;)}</dl>

<dl><dt><strong>__schema_cache__</strong> = {}</dl>

<dl><dt><strong>__signature__</strong> = &lt;Signature (*args, name: Optional[str] = None, c...None, api_version: Optional[str] = None) -&gt; None&gt;</dl>

<dl><dt><strong>__validators__</strong> = {}</dl>

<hr>
Class methods inherited from <a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a>:<br>
<dl><dt><a name="ChatOpenAI-validate_clients"><strong>validate_clients</strong></a>(values: Dict) -&gt; Dict<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<hr>
Methods inherited from <a href="langchain_openai.chat_models.base.html#ChatOpenAI">langchain_openai.chat_models.base.ChatOpenAI</a>:<br>
<dl><dt><a name="ChatOpenAI-bind_functions"><strong>bind_functions</strong></a>(self, functions: 'Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]]', function_call: "Optional[Union[_FunctionCall, str, Literal['auto', 'none']]]" = None, **kwargs: 'Any') -&gt; 'Runnable[LanguageModelInput, BaseMessage]'</dt><dd><tt>Bind&nbsp;functions&nbsp;(and&nbsp;other&nbsp;objects)&nbsp;to&nbsp;this&nbsp;chat&nbsp;model.<br>
&nbsp;<br>
Assumes&nbsp;model&nbsp;is&nbsp;compatible&nbsp;with&nbsp;<a href="#OpenAI">OpenAI</a>&nbsp;function-calling&nbsp;API.<br>
&nbsp;<br>
NOTE:&nbsp;Using&nbsp;bind_tools&nbsp;is&nbsp;recommended&nbsp;instead,&nbsp;as&nbsp;the&nbsp;`functions`&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;`function_call`&nbsp;request&nbsp;parameters&nbsp;are&nbsp;officially&nbsp;marked&nbsp;as&nbsp;deprecated&nbsp;by<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#OpenAI">OpenAI</a>.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;functions:&nbsp;A&nbsp;list&nbsp;of&nbsp;function&nbsp;definitions&nbsp;to&nbsp;bind&nbsp;to&nbsp;this&nbsp;chat&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Can&nbsp;be&nbsp;&nbsp;a&nbsp;dictionary,&nbsp;pydantic&nbsp;model,&nbsp;or&nbsp;callable.&nbsp;Pydantic<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;models&nbsp;and&nbsp;callables&nbsp;will&nbsp;be&nbsp;automatically&nbsp;converted&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;their&nbsp;schema&nbsp;dictionary&nbsp;representation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;function_call:&nbsp;Which&nbsp;function&nbsp;to&nbsp;require&nbsp;the&nbsp;model&nbsp;to&nbsp;call.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Must&nbsp;be&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;single&nbsp;provided&nbsp;function&nbsp;or<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"auto"&nbsp;to&nbsp;automatically&nbsp;determine&nbsp;which&nbsp;function&nbsp;to&nbsp;call<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(if&nbsp;any).<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Any&nbsp;additional&nbsp;parameters&nbsp;to&nbsp;pass&nbsp;to&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`~langchain.runnable.Runnable`&nbsp;constructor.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-bind_tools"><strong>bind_tools</strong></a>(self, tools: 'Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]]', *, tool_choice: "Optional[Union[dict, str, Literal['auto', 'none']]]" = None, **kwargs: 'Any') -&gt; 'Runnable[LanguageModelInput, BaseMessage]'</dt><dd><tt>Bind&nbsp;tool-like&nbsp;objects&nbsp;to&nbsp;this&nbsp;chat&nbsp;model.<br>
&nbsp;<br>
Assumes&nbsp;model&nbsp;is&nbsp;compatible&nbsp;with&nbsp;<a href="#OpenAI">OpenAI</a>&nbsp;tool-calling&nbsp;API.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tools:&nbsp;A&nbsp;list&nbsp;of&nbsp;tool&nbsp;definitions&nbsp;to&nbsp;bind&nbsp;to&nbsp;this&nbsp;chat&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Can&nbsp;be&nbsp;&nbsp;a&nbsp;dictionary,&nbsp;pydantic&nbsp;model,&nbsp;callable,&nbsp;or&nbsp;BaseTool.&nbsp;Pydantic<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;models,&nbsp;callables,&nbsp;and&nbsp;BaseTools&nbsp;will&nbsp;be&nbsp;automatically&nbsp;converted&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;their&nbsp;schema&nbsp;dictionary&nbsp;representation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;tool_choice:&nbsp;Which&nbsp;tool&nbsp;to&nbsp;require&nbsp;the&nbsp;model&nbsp;to&nbsp;call.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Must&nbsp;be&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;single&nbsp;provided&nbsp;function&nbsp;or<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"auto"&nbsp;to&nbsp;automatically&nbsp;determine&nbsp;which&nbsp;function&nbsp;to&nbsp;call<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(if&nbsp;any),&nbsp;or&nbsp;a&nbsp;dict&nbsp;of&nbsp;the&nbsp;form:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{"type":&nbsp;"function",&nbsp;"function":&nbsp;{"name":&nbsp;&lt;&lt;tool_name&gt;&gt;}}.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Any&nbsp;additional&nbsp;parameters&nbsp;to&nbsp;pass&nbsp;to&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`~langchain.runnable.Runnable`&nbsp;constructor.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-get_num_tokens_from_messages"><strong>get_num_tokens_from_messages</strong></a>(self, messages: 'List[BaseMessage]') -&gt; 'int'</dt><dd><tt>Calculate&nbsp;num&nbsp;tokens&nbsp;for&nbsp;gpt-3.5-turbo&nbsp;and&nbsp;gpt-4&nbsp;with&nbsp;tiktoken&nbsp;package.<br>
&nbsp;<br>
Official&nbsp;documentation:&nbsp;<a href="https://github.com/openai/openai-cookbook/blob/">https://github.com/openai/openai-cookbook/blob/</a><br>
main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-get_token_ids"><strong>get_token_ids</strong></a>(self, text: 'str') -&gt; 'List[int]'</dt><dd><tt>Get&nbsp;the&nbsp;tokens&nbsp;present&nbsp;in&nbsp;the&nbsp;text&nbsp;with&nbsp;tiktoken&nbsp;package.</tt></dd></dl>

<hr>
Class methods inherited from <a href="langchain_openai.chat_models.base.html#ChatOpenAI">langchain_openai.chat_models.base.ChatOpenAI</a>:<br>
<dl><dt><a name="ChatOpenAI-build_extra"><strong>build_extra</strong></a>(values: 'Dict[str, Any]') -&gt; 'Dict[str, Any]'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Build&nbsp;extra&nbsp;kwargs&nbsp;from&nbsp;additional&nbsp;params&nbsp;that&nbsp;were&nbsp;passed&nbsp;in.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-get_lc_namespace"><strong>get_lc_namespace</strong></a>() -&gt; 'List[str]'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Get&nbsp;the&nbsp;namespace&nbsp;of&nbsp;the&nbsp;langchain&nbsp;object.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-is_lc_serializable"><strong>is_lc_serializable</strong></a>() -&gt; 'bool'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Return&nbsp;whether&nbsp;this&nbsp;model&nbsp;can&nbsp;be&nbsp;serialized&nbsp;by&nbsp;Langchain.</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_openai.chat_models.base.html#ChatOpenAI">langchain_openai.chat_models.base.ChatOpenAI</a>:<br>
<dl><dt><strong>lc_attributes</strong></dt>
<dd><tt>List&nbsp;of&nbsp;attribute&nbsp;names&nbsp;that&nbsp;should&nbsp;be&nbsp;included&nbsp;in&nbsp;the&nbsp;serialized&nbsp;kwargs.<br>
&nbsp;<br>
These&nbsp;attributes&nbsp;must&nbsp;be&nbsp;accepted&nbsp;by&nbsp;the&nbsp;constructor.</tt></dd>
</dl>
<dl><dt><strong>lc_secrets</strong></dt>
<dd><tt>A&nbsp;map&nbsp;of&nbsp;constructor&nbsp;argument&nbsp;names&nbsp;to&nbsp;secret&nbsp;ids.<br>
&nbsp;<br>
For&nbsp;example,<br>
&nbsp;&nbsp;&nbsp;&nbsp;{"openai_api_key":&nbsp;"OPENAI_API_KEY"}</tt></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="langchain_openai.chat_models.base.html#ChatOpenAI">langchain_openai.chat_models.base.ChatOpenAI</a>:<br>
<dl><dt><strong>Config</strong> = &lt;class 'langchain_openai.chat_models.base.ChatOpenAI.Config'&gt;<dd><tt>Configuration&nbsp;for&nbsp;this&nbsp;pydantic&nbsp;object.</tt></dl>

<hr>
Methods inherited from <a href="langchain_core.language_models.chat_models.html#BaseChatModel">langchain_core.language_models.chat_models.BaseChatModel</a>:<br>
<dl><dt><a name="ChatOpenAI-__call__"><strong>__call__</strong></a>(self, messages: 'List[BaseMessage]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -&gt; 'BaseMessage'</dt><dd><tt>[*Deprecated*][*Deprecated*]&nbsp;<br>
&nbsp;<br>
Notes<br>
-----<br>
..&nbsp;deprecated::&nbsp;0.1.7<br>
&nbsp;&nbsp;&nbsp;Use&nbsp;invoke&nbsp;instead.</tt></dd></dl>

<dl><dt>async <a name="ChatOpenAI-agenerate"><strong>agenerate</strong></a>(self, messages: 'List[List[BaseMessage]]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -&gt; 'LLMResult'</dt><dd><tt>Asynchronously&nbsp;pass&nbsp;a&nbsp;sequence&nbsp;of&nbsp;prompts&nbsp;to&nbsp;a&nbsp;model&nbsp;and&nbsp;return&nbsp;generations.<br>
&nbsp;<br>
This&nbsp;method&nbsp;should&nbsp;make&nbsp;use&nbsp;of&nbsp;batched&nbsp;calls&nbsp;for&nbsp;models&nbsp;that&nbsp;expose&nbsp;a&nbsp;batched<br>
API.<br>
&nbsp;<br>
Use&nbsp;this&nbsp;method&nbsp;when&nbsp;you&nbsp;want&nbsp;to:<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;take&nbsp;advantage&nbsp;of&nbsp;batched&nbsp;calls,<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;need&nbsp;more&nbsp;output&nbsp;from&nbsp;the&nbsp;model&nbsp;than&nbsp;just&nbsp;the&nbsp;top&nbsp;generated&nbsp;value,<br>
&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;are&nbsp;building&nbsp;chains&nbsp;that&nbsp;are&nbsp;agnostic&nbsp;to&nbsp;the&nbsp;underlying&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;(e.g.,&nbsp;pure&nbsp;text&nbsp;completion&nbsp;models&nbsp;vs&nbsp;chat&nbsp;models).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;messages:&nbsp;List&nbsp;of&nbsp;list&nbsp;of&nbsp;messages.<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop:&nbsp;Stop&nbsp;words&nbsp;to&nbsp;use&nbsp;when&nbsp;generating.&nbsp;Model&nbsp;output&nbsp;is&nbsp;cut&nbsp;off&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;occurrence&nbsp;of&nbsp;any&nbsp;of&nbsp;these&nbsp;substrings.<br>
&nbsp;&nbsp;&nbsp;&nbsp;callbacks:&nbsp;Callbacks&nbsp;to&nbsp;pass&nbsp;through.&nbsp;Used&nbsp;for&nbsp;executing&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality,&nbsp;such&nbsp;as&nbsp;logging&nbsp;or&nbsp;streaming,&nbsp;throughout&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Arbitrary&nbsp;additional&nbsp;keyword&nbsp;arguments.&nbsp;These&nbsp;are&nbsp;usually&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;model&nbsp;provider&nbsp;API&nbsp;call.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;LLMResult,&nbsp;which&nbsp;contains&nbsp;a&nbsp;list&nbsp;of&nbsp;candidate&nbsp;Generations&nbsp;for&nbsp;each&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;and&nbsp;additional&nbsp;model&nbsp;provider-specific&nbsp;output.</tt></dd></dl>

<dl><dt>async <a name="ChatOpenAI-agenerate_prompt"><strong>agenerate_prompt</strong></a>(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -&gt; 'LLMResult'</dt><dd><tt>Asynchronously&nbsp;pass&nbsp;a&nbsp;sequence&nbsp;of&nbsp;prompts&nbsp;and&nbsp;return&nbsp;model&nbsp;generations.<br>
&nbsp;<br>
This&nbsp;method&nbsp;should&nbsp;make&nbsp;use&nbsp;of&nbsp;batched&nbsp;calls&nbsp;for&nbsp;models&nbsp;that&nbsp;expose&nbsp;a&nbsp;batched<br>
API.<br>
&nbsp;<br>
Use&nbsp;this&nbsp;method&nbsp;when&nbsp;you&nbsp;want&nbsp;to:<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;take&nbsp;advantage&nbsp;of&nbsp;batched&nbsp;calls,<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;need&nbsp;more&nbsp;output&nbsp;from&nbsp;the&nbsp;model&nbsp;than&nbsp;just&nbsp;the&nbsp;top&nbsp;generated&nbsp;value,<br>
&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;are&nbsp;building&nbsp;chains&nbsp;that&nbsp;are&nbsp;agnostic&nbsp;to&nbsp;the&nbsp;underlying&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;(e.g.,&nbsp;pure&nbsp;text&nbsp;completion&nbsp;models&nbsp;vs&nbsp;chat&nbsp;models).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompts:&nbsp;List&nbsp;of&nbsp;PromptValues.&nbsp;A&nbsp;PromptValue&nbsp;is&nbsp;an&nbsp;object&nbsp;that&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;converted&nbsp;to&nbsp;match&nbsp;the&nbsp;format&nbsp;of&nbsp;any&nbsp;language&nbsp;model&nbsp;(string&nbsp;for&nbsp;pure<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text&nbsp;generation&nbsp;models&nbsp;and&nbsp;BaseMessages&nbsp;for&nbsp;chat&nbsp;models).<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop:&nbsp;Stop&nbsp;words&nbsp;to&nbsp;use&nbsp;when&nbsp;generating.&nbsp;Model&nbsp;output&nbsp;is&nbsp;cut&nbsp;off&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;occurrence&nbsp;of&nbsp;any&nbsp;of&nbsp;these&nbsp;substrings.<br>
&nbsp;&nbsp;&nbsp;&nbsp;callbacks:&nbsp;Callbacks&nbsp;to&nbsp;pass&nbsp;through.&nbsp;Used&nbsp;for&nbsp;executing&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality,&nbsp;such&nbsp;as&nbsp;logging&nbsp;or&nbsp;streaming,&nbsp;throughout&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Arbitrary&nbsp;additional&nbsp;keyword&nbsp;arguments.&nbsp;These&nbsp;are&nbsp;usually&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;model&nbsp;provider&nbsp;API&nbsp;call.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;LLMResult,&nbsp;which&nbsp;contains&nbsp;a&nbsp;list&nbsp;of&nbsp;candidate&nbsp;Generations&nbsp;for&nbsp;each&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;and&nbsp;additional&nbsp;model&nbsp;provider-specific&nbsp;output.</tt></dd></dl>

<dl><dt>async <a name="ChatOpenAI-ainvoke"><strong>ainvoke</strong></a>(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -&gt; 'BaseMessage'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;ainvoke,&nbsp;calls&nbsp;invoke&nbsp;from&nbsp;a&nbsp;thread.<br>
&nbsp;<br>
The&nbsp;default&nbsp;implementation&nbsp;allows&nbsp;usage&nbsp;of&nbsp;async&nbsp;code&nbsp;even&nbsp;if<br>
the&nbsp;runnable&nbsp;did&nbsp;not&nbsp;implement&nbsp;a&nbsp;native&nbsp;async&nbsp;version&nbsp;of&nbsp;invoke.<br>
&nbsp;<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;run&nbsp;asynchronously.</tt></dd></dl>

<dl><dt>async <a name="ChatOpenAI-apredict"><strong>apredict</strong></a>(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'str'</dt><dd><tt>[*Deprecated*][*Deprecated*]&nbsp;<br>
&nbsp;<br>
Notes<br>
-----<br>
..&nbsp;deprecated::&nbsp;0.1.7<br>
&nbsp;&nbsp;&nbsp;Use&nbsp;ainvoke&nbsp;instead.</tt></dd></dl>

<dl><dt>async <a name="ChatOpenAI-apredict_messages"><strong>apredict_messages</strong></a>(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'BaseMessage'</dt><dd><tt>[*Deprecated*][*Deprecated*]&nbsp;<br>
&nbsp;<br>
Notes<br>
-----<br>
..&nbsp;deprecated::&nbsp;0.1.7<br>
&nbsp;&nbsp;&nbsp;Use&nbsp;ainvoke&nbsp;instead.</tt></dd></dl>

<dl><dt>async <a name="ChatOpenAI-astream"><strong>astream</strong></a>(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -&gt; 'AsyncIterator[BaseMessageChunk]'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;astream,&nbsp;which&nbsp;calls&nbsp;ainvoke.<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;support&nbsp;streaming&nbsp;output.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-call_as_llm"><strong>call_as_llm</strong></a>(self, message: 'str', stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -&gt; 'str'</dt><dd><tt>[*Deprecated*][*Deprecated*]&nbsp;<br>
&nbsp;<br>
Notes<br>
-----<br>
..&nbsp;deprecated::&nbsp;0.1.7<br>
&nbsp;&nbsp;&nbsp;Use&nbsp;invoke&nbsp;instead.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-dict"><strong>dict</strong></a>(self, **kwargs: 'Any') -&gt; 'Dict'</dt><dd><tt>Return&nbsp;a&nbsp;dictionary&nbsp;of&nbsp;the&nbsp;LLM.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-generate"><strong>generate</strong></a>(self, messages: 'List[List[BaseMessage]]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -&gt; 'LLMResult'</dt><dd><tt>Pass&nbsp;a&nbsp;sequence&nbsp;of&nbsp;prompts&nbsp;to&nbsp;the&nbsp;model&nbsp;and&nbsp;return&nbsp;model&nbsp;generations.<br>
&nbsp;<br>
This&nbsp;method&nbsp;should&nbsp;make&nbsp;use&nbsp;of&nbsp;batched&nbsp;calls&nbsp;for&nbsp;models&nbsp;that&nbsp;expose&nbsp;a&nbsp;batched<br>
API.<br>
&nbsp;<br>
Use&nbsp;this&nbsp;method&nbsp;when&nbsp;you&nbsp;want&nbsp;to:<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;take&nbsp;advantage&nbsp;of&nbsp;batched&nbsp;calls,<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;need&nbsp;more&nbsp;output&nbsp;from&nbsp;the&nbsp;model&nbsp;than&nbsp;just&nbsp;the&nbsp;top&nbsp;generated&nbsp;value,<br>
&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;are&nbsp;building&nbsp;chains&nbsp;that&nbsp;are&nbsp;agnostic&nbsp;to&nbsp;the&nbsp;underlying&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;(e.g.,&nbsp;pure&nbsp;text&nbsp;completion&nbsp;models&nbsp;vs&nbsp;chat&nbsp;models).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;messages:&nbsp;List&nbsp;of&nbsp;list&nbsp;of&nbsp;messages.<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop:&nbsp;Stop&nbsp;words&nbsp;to&nbsp;use&nbsp;when&nbsp;generating.&nbsp;Model&nbsp;output&nbsp;is&nbsp;cut&nbsp;off&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;occurrence&nbsp;of&nbsp;any&nbsp;of&nbsp;these&nbsp;substrings.<br>
&nbsp;&nbsp;&nbsp;&nbsp;callbacks:&nbsp;Callbacks&nbsp;to&nbsp;pass&nbsp;through.&nbsp;Used&nbsp;for&nbsp;executing&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality,&nbsp;such&nbsp;as&nbsp;logging&nbsp;or&nbsp;streaming,&nbsp;throughout&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Arbitrary&nbsp;additional&nbsp;keyword&nbsp;arguments.&nbsp;These&nbsp;are&nbsp;usually&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;model&nbsp;provider&nbsp;API&nbsp;call.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;LLMResult,&nbsp;which&nbsp;contains&nbsp;a&nbsp;list&nbsp;of&nbsp;candidate&nbsp;Generations&nbsp;for&nbsp;each&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;and&nbsp;additional&nbsp;model&nbsp;provider-specific&nbsp;output.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-generate_prompt"><strong>generate_prompt</strong></a>(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -&gt; 'LLMResult'</dt><dd><tt>Pass&nbsp;a&nbsp;sequence&nbsp;of&nbsp;prompts&nbsp;to&nbsp;the&nbsp;model&nbsp;and&nbsp;return&nbsp;model&nbsp;generations.<br>
&nbsp;<br>
This&nbsp;method&nbsp;should&nbsp;make&nbsp;use&nbsp;of&nbsp;batched&nbsp;calls&nbsp;for&nbsp;models&nbsp;that&nbsp;expose&nbsp;a&nbsp;batched<br>
API.<br>
&nbsp;<br>
Use&nbsp;this&nbsp;method&nbsp;when&nbsp;you&nbsp;want&nbsp;to:<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;take&nbsp;advantage&nbsp;of&nbsp;batched&nbsp;calls,<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;need&nbsp;more&nbsp;output&nbsp;from&nbsp;the&nbsp;model&nbsp;than&nbsp;just&nbsp;the&nbsp;top&nbsp;generated&nbsp;value,<br>
&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;are&nbsp;building&nbsp;chains&nbsp;that&nbsp;are&nbsp;agnostic&nbsp;to&nbsp;the&nbsp;underlying&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;(e.g.,&nbsp;pure&nbsp;text&nbsp;completion&nbsp;models&nbsp;vs&nbsp;chat&nbsp;models).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompts:&nbsp;List&nbsp;of&nbsp;PromptValues.&nbsp;A&nbsp;PromptValue&nbsp;is&nbsp;an&nbsp;object&nbsp;that&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;converted&nbsp;to&nbsp;match&nbsp;the&nbsp;format&nbsp;of&nbsp;any&nbsp;language&nbsp;model&nbsp;(string&nbsp;for&nbsp;pure<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text&nbsp;generation&nbsp;models&nbsp;and&nbsp;BaseMessages&nbsp;for&nbsp;chat&nbsp;models).<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop:&nbsp;Stop&nbsp;words&nbsp;to&nbsp;use&nbsp;when&nbsp;generating.&nbsp;Model&nbsp;output&nbsp;is&nbsp;cut&nbsp;off&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;occurrence&nbsp;of&nbsp;any&nbsp;of&nbsp;these&nbsp;substrings.<br>
&nbsp;&nbsp;&nbsp;&nbsp;callbacks:&nbsp;Callbacks&nbsp;to&nbsp;pass&nbsp;through.&nbsp;Used&nbsp;for&nbsp;executing&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality,&nbsp;such&nbsp;as&nbsp;logging&nbsp;or&nbsp;streaming,&nbsp;throughout&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Arbitrary&nbsp;additional&nbsp;keyword&nbsp;arguments.&nbsp;These&nbsp;are&nbsp;usually&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;model&nbsp;provider&nbsp;API&nbsp;call.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;LLMResult,&nbsp;which&nbsp;contains&nbsp;a&nbsp;list&nbsp;of&nbsp;candidate&nbsp;Generations&nbsp;for&nbsp;each&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;and&nbsp;additional&nbsp;model&nbsp;provider-specific&nbsp;output.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-invoke"><strong>invoke</strong></a>(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -&gt; 'BaseMessage'</dt><dd><tt>Transform&nbsp;a&nbsp;single&nbsp;input&nbsp;into&nbsp;an&nbsp;output.&nbsp;Override&nbsp;to&nbsp;implement.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;invoking&nbsp;the&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;config&nbsp;supports&nbsp;standard&nbsp;keys&nbsp;like&nbsp;'tags',&nbsp;'metadata'&nbsp;for&nbsp;tracing<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;purposes,&nbsp;'max_concurrency'&nbsp;for&nbsp;controlling&nbsp;how&nbsp;much&nbsp;work&nbsp;to&nbsp;do<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;parallel,&nbsp;and&nbsp;other&nbsp;keys.&nbsp;Please&nbsp;refer&nbsp;to&nbsp;the&nbsp;RunnableConfig<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;more&nbsp;details.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;output&nbsp;of&nbsp;the&nbsp;runnable.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-predict"><strong>predict</strong></a>(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'str'</dt><dd><tt>[*Deprecated*][*Deprecated*]&nbsp;<br>
&nbsp;<br>
Notes<br>
-----<br>
..&nbsp;deprecated::&nbsp;0.1.7<br>
&nbsp;&nbsp;&nbsp;Use&nbsp;invoke&nbsp;instead.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-predict_messages"><strong>predict_messages</strong></a>(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'BaseMessage'</dt><dd><tt>[*Deprecated*][*Deprecated*]&nbsp;<br>
&nbsp;<br>
Notes<br>
-----<br>
..&nbsp;deprecated::&nbsp;0.1.7<br>
&nbsp;&nbsp;&nbsp;Use&nbsp;invoke&nbsp;instead.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-stream"><strong>stream</strong></a>(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -&gt; 'Iterator[BaseMessageChunk]'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;stream,&nbsp;which&nbsp;calls&nbsp;invoke.<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;support&nbsp;streaming&nbsp;output.</tt></dd></dl>

<hr>
Class methods inherited from <a href="langchain_core.language_models.chat_models.html#BaseChatModel">langchain_core.language_models.chat_models.BaseChatModel</a>:<br>
<dl><dt><a name="ChatOpenAI-raise_deprecation"><strong>raise_deprecation</strong></a>(values: 'Dict') -&gt; 'Dict'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Raise&nbsp;deprecation&nbsp;warning&nbsp;if&nbsp;callback_manager&nbsp;is&nbsp;used.</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_core.language_models.chat_models.html#BaseChatModel">langchain_core.language_models.chat_models.BaseChatModel</a>:<br>
<dl><dt><strong>OutputType</strong></dt>
<dd><tt>Get&nbsp;the&nbsp;output&nbsp;type&nbsp;for&nbsp;this&nbsp;runnable.</tt></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="langchain_core.language_models.chat_models.html#BaseChatModel">langchain_core.language_models.chat_models.BaseChatModel</a>:<br>
<dl><dt><strong>__orig_bases__</strong> = (langchain_core.language_models.base.BaseLanguageModel[langchain_core.messages.base.BaseMessage], &lt;class 'abc.ABC'&gt;)</dl>

<hr>
Methods inherited from <a href="langchain_core.language_models.base.html#BaseLanguageModel">langchain_core.language_models.base.BaseLanguageModel</a>:<br>
<dl><dt><a name="ChatOpenAI-get_num_tokens"><strong>get_num_tokens</strong></a>(self, text: 'str') -&gt; 'int'</dt><dd><tt>Get&nbsp;the&nbsp;number&nbsp;of&nbsp;tokens&nbsp;present&nbsp;in&nbsp;the&nbsp;text.<br>
&nbsp;<br>
Useful&nbsp;for&nbsp;checking&nbsp;if&nbsp;an&nbsp;input&nbsp;will&nbsp;fit&nbsp;in&nbsp;a&nbsp;model's&nbsp;context&nbsp;window.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;text:&nbsp;The&nbsp;string&nbsp;input&nbsp;to&nbsp;tokenize.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;integer&nbsp;number&nbsp;of&nbsp;tokens&nbsp;in&nbsp;the&nbsp;text.</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_core.language_models.base.html#BaseLanguageModel">langchain_core.language_models.base.BaseLanguageModel</a>:<br>
<dl><dt><strong>InputType</strong></dt>
<dd><tt>Get&nbsp;the&nbsp;input&nbsp;type&nbsp;for&nbsp;this&nbsp;runnable.</tt></dd>
</dl>
<hr>
Methods inherited from <a href="langchain_core.runnables.base.html#RunnableSerializable">langchain_core.runnables.base.RunnableSerializable</a>:<br>
<dl><dt><a name="ChatOpenAI-configurable_alternatives"><strong>configurable_alternatives</strong></a>(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -&gt; 'RunnableSerializable[Input, Output]'</dt></dl>

<dl><dt><a name="ChatOpenAI-configurable_fields"><strong>configurable_fields</strong></a>(self, **kwargs: 'AnyConfigurableField') -&gt; 'RunnableSerializable[Input, Output]'</dt></dl>

<hr>
Data descriptors inherited from <a href="langchain_core.runnables.base.html#RunnableSerializable">langchain_core.runnables.base.RunnableSerializable</a>:<br>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<hr>
Methods inherited from <a href="langchain_core.load.serializable.html#Serializable">langchain_core.load.serializable.Serializable</a>:<br>
<dl><dt><a name="ChatOpenAI-__repr_args__"><strong>__repr_args__</strong></a>(self) -&gt; Any</dt><dd><tt>Returns&nbsp;the&nbsp;attributes&nbsp;to&nbsp;show&nbsp;in&nbsp;__str__,&nbsp;__repr__,&nbsp;and&nbsp;__pretty__&nbsp;this&nbsp;is&nbsp;generally&nbsp;overridden.<br>
&nbsp;<br>
Can&nbsp;either&nbsp;return:<br>
*&nbsp;name&nbsp;-&nbsp;value&nbsp;pairs,&nbsp;e.g.:&nbsp;`[('foo_name',&nbsp;'foo'),&nbsp;('bar_name',&nbsp;['b',&nbsp;'a',&nbsp;'r'])]`<br>
*&nbsp;or,&nbsp;just&nbsp;values,&nbsp;e.g.:&nbsp;`[(None,&nbsp;'foo'),&nbsp;(None,&nbsp;['b',&nbsp;'a',&nbsp;'r'])]`</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-to_json"><strong>to_json</strong></a>(self) -&gt; Union[langchain_core.load.serializable.SerializedConstructor, langchain_core.load.serializable.SerializedNotImplemented]</dt></dl>

<dl><dt><a name="ChatOpenAI-to_json_not_implemented"><strong>to_json_not_implemented</strong></a>(self) -&gt; langchain_core.load.serializable.SerializedNotImplemented</dt></dl>

<hr>
Class methods inherited from <a href="langchain_core.load.serializable.html#Serializable">langchain_core.load.serializable.Serializable</a>:<br>
<dl><dt><a name="ChatOpenAI-lc_id"><strong>lc_id</strong></a>() -&gt; List[str]<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>A&nbsp;unique&nbsp;identifier&nbsp;for&nbsp;this&nbsp;class&nbsp;for&nbsp;serialization&nbsp;purposes.<br>
&nbsp;<br>
The&nbsp;unique&nbsp;identifier&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;strings&nbsp;that&nbsp;describes&nbsp;the&nbsp;path<br>
to&nbsp;the&nbsp;object.</tt></dd></dl>

<hr>
Methods inherited from <a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a>:<br>
<dl><dt><a name="ChatOpenAI-__eq__"><strong>__eq__</strong></a>(self, other: Any) -&gt; bool</dt><dd><tt>Return&nbsp;self==value.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__getstate__"><strong>__getstate__</strong></a>(self) -&gt; 'DictAny'</dt></dl>

<dl><dt><a name="ChatOpenAI-__iter__"><strong>__iter__</strong></a>(self) -&gt; 'TupleGenerator'</dt><dd><tt>so&nbsp;`<a href="#ChatOpenAI-dict">dict</a>(model)`&nbsp;works</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__setattr__"><strong>__setattr__</strong></a>(self, name, value)</dt><dd><tt>Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__setstate__"><strong>__setstate__</strong></a>(self, state: 'DictAny') -&gt; None</dt></dl>

<dl><dt><a name="ChatOpenAI-copy"><strong>copy</strong></a>(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -&gt; 'Model'</dt><dd><tt>Duplicate&nbsp;a&nbsp;model,&nbsp;optionally&nbsp;choose&nbsp;which&nbsp;fields&nbsp;to&nbsp;include,&nbsp;exclude&nbsp;and&nbsp;change.<br>
&nbsp;<br>
:param&nbsp;include:&nbsp;fields&nbsp;to&nbsp;include&nbsp;in&nbsp;new&nbsp;model<br>
:param&nbsp;exclude:&nbsp;fields&nbsp;to&nbsp;exclude&nbsp;from&nbsp;new&nbsp;model,&nbsp;as&nbsp;with&nbsp;values&nbsp;this&nbsp;takes&nbsp;precedence&nbsp;over&nbsp;include<br>
:param&nbsp;update:&nbsp;values&nbsp;to&nbsp;change/add&nbsp;in&nbsp;the&nbsp;new&nbsp;model.&nbsp;Note:&nbsp;the&nbsp;data&nbsp;is&nbsp;not&nbsp;validated&nbsp;before&nbsp;creating<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;new&nbsp;model:&nbsp;you&nbsp;should&nbsp;trust&nbsp;this&nbsp;data<br>
:param&nbsp;deep:&nbsp;set&nbsp;to&nbsp;`True`&nbsp;to&nbsp;make&nbsp;a&nbsp;deep&nbsp;copy&nbsp;of&nbsp;the&nbsp;model<br>
:return:&nbsp;new&nbsp;model&nbsp;instance</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-json"><strong>json</strong></a>(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -&gt; str</dt><dd><tt>Generate&nbsp;a&nbsp;JSON&nbsp;representation&nbsp;of&nbsp;the&nbsp;model,&nbsp;`include`&nbsp;and&nbsp;`exclude`&nbsp;arguments&nbsp;as&nbsp;per&nbsp;`<a href="#ChatOpenAI-dict">dict</a>()`.<br>
&nbsp;<br>
`encoder`&nbsp;is&nbsp;an&nbsp;optional&nbsp;function&nbsp;to&nbsp;supply&nbsp;as&nbsp;`default`&nbsp;to&nbsp;json.dumps(),&nbsp;other&nbsp;arguments&nbsp;as&nbsp;per&nbsp;`json.dumps()`.</tt></dd></dl>

<hr>
Class methods inherited from <a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a>:<br>
<dl><dt><a name="ChatOpenAI-__get_validators__"><strong>__get_validators__</strong></a>() -&gt; 'CallableGenerator'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatOpenAI-__try_update_forward_refs__"><strong>__try_update_forward_refs__</strong></a>(**localns: Any) -&gt; None<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Same&nbsp;as&nbsp;update_forward_refs&nbsp;but&nbsp;will&nbsp;not&nbsp;raise&nbsp;exception<br>
when&nbsp;forward&nbsp;references&nbsp;are&nbsp;not&nbsp;defined.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-construct"><strong>construct</strong></a>(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Creates&nbsp;a&nbsp;new&nbsp;model&nbsp;setting&nbsp;__dict__&nbsp;and&nbsp;__fields_set__&nbsp;from&nbsp;trusted&nbsp;or&nbsp;pre-validated&nbsp;data.<br>
Default&nbsp;values&nbsp;are&nbsp;respected,&nbsp;but&nbsp;no&nbsp;other&nbsp;validation&nbsp;is&nbsp;performed.<br>
Behaves&nbsp;as&nbsp;if&nbsp;`Config.extra&nbsp;=&nbsp;'allow'`&nbsp;was&nbsp;set&nbsp;since&nbsp;it&nbsp;adds&nbsp;all&nbsp;passed&nbsp;values</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-from_orm"><strong>from_orm</strong></a>(obj: Any) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatOpenAI-parse_file"><strong>parse_file</strong></a>(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatOpenAI-parse_obj"><strong>parse_obj</strong></a>(obj: Any) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatOpenAI-parse_raw"><strong>parse_raw</strong></a>(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatOpenAI-schema"><strong>schema</strong></a>(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -&gt; 'DictStrAny'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatOpenAI-schema_json"><strong>schema_json</strong></a>(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -&gt; str<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatOpenAI-update_forward_refs"><strong>update_forward_refs</strong></a>(**localns: Any) -&gt; None<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Try&nbsp;to&nbsp;update&nbsp;ForwardRefs&nbsp;on&nbsp;fields&nbsp;based&nbsp;on&nbsp;this&nbsp;Model,&nbsp;globalns&nbsp;and&nbsp;localns.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-validate"><strong>validate</strong></a>(value: Any) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<hr>
Data descriptors inherited from <a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__fields_set__</strong></dt>
</dl>
<hr>
Methods inherited from <a href="pydantic.v1.utils.html#Representation">pydantic.v1.utils.Representation</a>:<br>
<dl><dt><a name="ChatOpenAI-__pretty__"><strong>__pretty__</strong></a>(self, fmt: Callable[[Any], Any], **kwargs: Any) -&gt; Generator[Any, NoneType, NoneType]</dt><dd><tt>Used&nbsp;by&nbsp;devtools&nbsp;(<a href="https://python-devtools.helpmanual.io/">https://python-devtools.helpmanual.io/</a>)&nbsp;to&nbsp;provide&nbsp;a&nbsp;human&nbsp;readable&nbsp;representations&nbsp;of&nbsp;objects</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__repr__"><strong>__repr__</strong></a>(self) -&gt; str</dt><dd><tt>Return&nbsp;repr(self).</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__repr_name__"><strong>__repr_name__</strong></a>(self) -&gt; str</dt><dd><tt>Name&nbsp;of&nbsp;the&nbsp;instance's&nbsp;class,&nbsp;used&nbsp;in&nbsp;__repr__.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__repr_str__"><strong>__repr_str__</strong></a>(self, join_str: str) -&gt; str</dt></dl>

<dl><dt><a name="ChatOpenAI-__rich_repr__"><strong>__rich_repr__</strong></a>(self) -&gt; 'RichReprResult'</dt><dd><tt>Get&nbsp;fields&nbsp;for&nbsp;Rich&nbsp;library</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__str__"><strong>__str__</strong></a>(self) -&gt; str</dt><dd><tt>Return&nbsp;str(self).</tt></dd></dl>

<hr>
Methods inherited from <a href="langchain_core.runnables.base.html#Runnable">langchain_core.runnables.base.Runnable</a>:<br>
<dl><dt><a name="ChatOpenAI-__or__"><strong>__or__</strong></a>(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Callable[[Iterator[Any]], Iterator[Other]], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -&gt; 'RunnableSerializable[Input, Other]'</dt><dd><tt>Compose&nbsp;this&nbsp;runnable&nbsp;with&nbsp;another&nbsp;object&nbsp;to&nbsp;create&nbsp;a&nbsp;RunnableSequence.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__ror__"><strong>__ror__</strong></a>(self, other: 'Union[Runnable[Other, Any], Callable[[Other], Any], Callable[[Iterator[Other]], Iterator[Any]], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -&gt; 'RunnableSerializable[Other, Output]'</dt><dd><tt>Compose&nbsp;this&nbsp;runnable&nbsp;with&nbsp;another&nbsp;object&nbsp;to&nbsp;create&nbsp;a&nbsp;RunnableSequence.</tt></dd></dl>

<dl><dt>async <a name="ChatOpenAI-abatch"><strong>abatch</strong></a>(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -&gt; 'List[Output]'</dt><dd><tt>Default&nbsp;implementation&nbsp;runs&nbsp;ainvoke&nbsp;in&nbsp;parallel&nbsp;using&nbsp;asyncio.gather.<br>
&nbsp;<br>
The&nbsp;default&nbsp;implementation&nbsp;of&nbsp;batch&nbsp;works&nbsp;well&nbsp;for&nbsp;IO&nbsp;bound&nbsp;runnables.<br>
&nbsp;<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;batch&nbsp;more&nbsp;efficiently;<br>
e.g.,&nbsp;if&nbsp;the&nbsp;underlying&nbsp;runnable&nbsp;uses&nbsp;an&nbsp;API&nbsp;which&nbsp;supports&nbsp;a&nbsp;batch&nbsp;mode.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-assign"><strong>assign</strong></a>(self, **kwargs: 'Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any], Mapping[str, Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]]]]') -&gt; 'RunnableSerializable[Any, Any]'</dt><dd><tt>Assigns&nbsp;new&nbsp;fields&nbsp;to&nbsp;the&nbsp;dict&nbsp;output&nbsp;of&nbsp;this&nbsp;runnable.<br>
Returns&nbsp;a&nbsp;new&nbsp;runnable.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-astream_events"><strong>astream_events</strong></a>(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: "Literal['v1']", include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'AsyncIterator[StreamEvent]'</dt><dd><tt>[*Beta*]&nbsp;&nbsp;Generate&nbsp;a&nbsp;stream&nbsp;of&nbsp;events.<br>
&nbsp;<br>
Use&nbsp;to&nbsp;create&nbsp;an&nbsp;iterator&nbsp;ove&nbsp;StreamEvents&nbsp;that&nbsp;provide&nbsp;real-time&nbsp;information<br>
about&nbsp;the&nbsp;progress&nbsp;of&nbsp;the&nbsp;runnable,&nbsp;including&nbsp;StreamEvents&nbsp;from&nbsp;intermediate<br>
results.<br>
&nbsp;<br>
A&nbsp;StreamEvent&nbsp;is&nbsp;a&nbsp;dictionary&nbsp;with&nbsp;the&nbsp;following&nbsp;schema:<br>
&nbsp;<br>
*&nbsp;``event``:&nbsp;str&nbsp;-&nbsp;Event&nbsp;names&nbsp;are&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;format:&nbsp;on_[runnable_type]_(start|stream|end).<br>
*&nbsp;``name``:&nbsp;str&nbsp;-&nbsp;The&nbsp;name&nbsp;of&nbsp;the&nbsp;runnable&nbsp;that&nbsp;generated&nbsp;the&nbsp;event.<br>
*&nbsp;``run_id``:&nbsp;str&nbsp;-&nbsp;randomly&nbsp;generated&nbsp;ID&nbsp;associated&nbsp;with&nbsp;the&nbsp;given&nbsp;execution&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;runnable&nbsp;that&nbsp;emitted&nbsp;the&nbsp;event.<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;child&nbsp;runnable&nbsp;that&nbsp;gets&nbsp;invoked&nbsp;as&nbsp;part&nbsp;of&nbsp;the&nbsp;execution&nbsp;of&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;parent&nbsp;runnable&nbsp;is&nbsp;assigned&nbsp;its&nbsp;own&nbsp;unique&nbsp;ID.<br>
*&nbsp;``tags``:&nbsp;Optional[List[str]]&nbsp;-&nbsp;The&nbsp;tags&nbsp;of&nbsp;the&nbsp;runnable&nbsp;that&nbsp;generated<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;event.<br>
*&nbsp;``metadata``:&nbsp;Optional[Dict[str,&nbsp;Any]]&nbsp;-&nbsp;The&nbsp;metadata&nbsp;of&nbsp;the&nbsp;runnable<br>
&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;generated&nbsp;the&nbsp;event.<br>
*&nbsp;``data``:&nbsp;Dict[str,&nbsp;Any]<br>
&nbsp;<br>
&nbsp;<br>
Below&nbsp;is&nbsp;a&nbsp;table&nbsp;that&nbsp;illustrates&nbsp;some&nbsp;evens&nbsp;that&nbsp;might&nbsp;be&nbsp;emitted&nbsp;by&nbsp;various<br>
chains.&nbsp;Metadata&nbsp;fields&nbsp;have&nbsp;been&nbsp;omitted&nbsp;from&nbsp;the&nbsp;table&nbsp;for&nbsp;brevity.<br>
Chain&nbsp;definitions&nbsp;have&nbsp;been&nbsp;included&nbsp;after&nbsp;the&nbsp;table.<br>
&nbsp;<br>
|&nbsp;event&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;chunk&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;input&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;output&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|----------------------|------------------|---------------------------------|-----------------------------------------------|-------------------------------------------------|<br>
|&nbsp;on_chat_model_start&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"messages":&nbsp;[[SystemMessage,&nbsp;HumanMessage]]}&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_chat_model_stream&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;AIMessageChunk(content="hello")&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_chat_model_end&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"messages":&nbsp;[[SystemMessage,&nbsp;HumanMessage]]}&nbsp;|&nbsp;{"generations":&nbsp;[...],&nbsp;"llm_output":&nbsp;None,&nbsp;...}&nbsp;|<br>
|&nbsp;on_llm_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{'input':&nbsp;'hello'}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_llm_stream&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;'Hello'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_llm_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;'Hello&nbsp;human!'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_chain_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;format_docs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_chain_stream&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;format_docs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;"hello&nbsp;world!,&nbsp;goodbye&nbsp;world!"&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_chain_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;format_docs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[Document(...)]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;"hello&nbsp;world!,&nbsp;goodbye&nbsp;world!"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_tool_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;some_tool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"x":&nbsp;1,&nbsp;"y":&nbsp;"2"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_tool_stream&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;some_tool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"x":&nbsp;1,&nbsp;"y":&nbsp;"2"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_tool_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;some_tool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"x":&nbsp;1,&nbsp;"y":&nbsp;"2"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_retriever_start&nbsp;&nbsp;&nbsp;|&nbsp;[retriever&nbsp;name]&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"query":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_retriever_chunk&nbsp;&nbsp;&nbsp;|&nbsp;[retriever&nbsp;name]&nbsp;|&nbsp;{documents:&nbsp;[...]}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_retriever_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[retriever&nbsp;name]&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"query":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{documents:&nbsp;[...]}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_prompt_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[template_name]&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"question":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_prompt_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[template_name]&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"question":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;ChatPromptValue(messages:&nbsp;[SystemMessage,&nbsp;...])&nbsp;|<br>
&nbsp;<br>
Here&nbsp;are&nbsp;declarations&nbsp;associated&nbsp;with&nbsp;the&nbsp;events&nbsp;shown&nbsp;above:<br>
&nbsp;<br>
`format_docs`:<br>
&nbsp;<br>
```python<br>
def&nbsp;format_docs(docs:&nbsp;List[Document])&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;'''Format&nbsp;the&nbsp;docs.'''<br>
&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;",&nbsp;".join([doc.page_content&nbsp;for&nbsp;doc&nbsp;in&nbsp;docs])<br>
&nbsp;<br>
format_docs&nbsp;=&nbsp;RunnableLambda(format_docs)<br>
```<br>
&nbsp;<br>
`some_tool`:<br>
&nbsp;<br>
```python<br>
@tool<br>
def&nbsp;some_tool(x:&nbsp;int,&nbsp;y:&nbsp;str)&nbsp;-&gt;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;'''Some_tool.'''<br>
&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;{"x":&nbsp;x,&nbsp;"y":&nbsp;y}<br>
```<br>
&nbsp;<br>
`prompt`:<br>
&nbsp;<br>
```python<br>
template&nbsp;=&nbsp;ChatPromptTemplate.from_messages(<br>
&nbsp;&nbsp;&nbsp;&nbsp;[("system",&nbsp;"You&nbsp;are&nbsp;Cat&nbsp;Agent&nbsp;007"),&nbsp;("human",&nbsp;"{question}")]<br>
).<a href="#ChatOpenAI-with_config">with_config</a>({"run_name":&nbsp;"my_template",&nbsp;"tags":&nbsp;["my_template"]})<br>
```<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;reverse(s:&nbsp;str)&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;s[::-1]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;RunnableLambda(func=reverse)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;events&nbsp;=&nbsp;[<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;event&nbsp;async&nbsp;for&nbsp;event&nbsp;in&nbsp;chain.<a href="#ChatOpenAI-astream_events">astream_events</a>("hello",&nbsp;version="v1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;will&nbsp;produce&nbsp;the&nbsp;following&nbsp;events&nbsp;(run_id&nbsp;has&nbsp;been&nbsp;omitted&nbsp;for&nbsp;brevity):<br>
&nbsp;&nbsp;&nbsp;&nbsp;[<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"data":&nbsp;{"input":&nbsp;"hello"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"event":&nbsp;"on_chain_start",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata":&nbsp;{},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"reverse",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;[],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"data":&nbsp;{"chunk":&nbsp;"olleh"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"event":&nbsp;"on_chain_stream",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata":&nbsp;{},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"reverse",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;[],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"data":&nbsp;{"output":&nbsp;"olleh"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"event":&nbsp;"on_chain_end",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata":&nbsp;{},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"reverse",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;[],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;]<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;version:&nbsp;The&nbsp;version&nbsp;of&nbsp;the&nbsp;schema&nbsp;to&nbsp;use.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Currently&nbsp;only&nbsp;version&nbsp;1&nbsp;is&nbsp;available.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;No&nbsp;default&nbsp;will&nbsp;be&nbsp;assigned&nbsp;until&nbsp;the&nbsp;API&nbsp;is&nbsp;stabilized.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_names:&nbsp;Only&nbsp;include&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_types:&nbsp;Only&nbsp;include&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_tags:&nbsp;Only&nbsp;include&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_names:&nbsp;Exclude&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_types:&nbsp;Exclude&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_tags:&nbsp;Exclude&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;These&nbsp;will&nbsp;be&nbsp;passed&nbsp;to&nbsp;astream_log&nbsp;as&nbsp;this&nbsp;implementation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;astream_events&nbsp;is&nbsp;built&nbsp;on&nbsp;top&nbsp;of&nbsp;astream_log.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;async&nbsp;stream&nbsp;of&nbsp;StreamEvents.[*Beta*]&nbsp;Generate&nbsp;a&nbsp;stream&nbsp;of&nbsp;events.<br>
&nbsp;<br>
Use&nbsp;to&nbsp;create&nbsp;an&nbsp;iterator&nbsp;ove&nbsp;StreamEvents&nbsp;that&nbsp;provide&nbsp;real-time&nbsp;information<br>
about&nbsp;the&nbsp;progress&nbsp;of&nbsp;the&nbsp;runnable,&nbsp;including&nbsp;StreamEvents&nbsp;from&nbsp;intermediate<br>
results.<br>
&nbsp;<br>
A&nbsp;StreamEvent&nbsp;is&nbsp;a&nbsp;dictionary&nbsp;with&nbsp;the&nbsp;following&nbsp;schema:<br>
&nbsp;<br>
*&nbsp;``event``:&nbsp;str&nbsp;-&nbsp;Event&nbsp;names&nbsp;are&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;format:&nbsp;on_[runnable_type]_(start|stream|end).<br>
*&nbsp;``name``:&nbsp;str&nbsp;-&nbsp;The&nbsp;name&nbsp;of&nbsp;the&nbsp;runnable&nbsp;that&nbsp;generated&nbsp;the&nbsp;event.<br>
*&nbsp;``run_id``:&nbsp;str&nbsp;-&nbsp;randomly&nbsp;generated&nbsp;ID&nbsp;associated&nbsp;with&nbsp;the&nbsp;given&nbsp;execution&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;runnable&nbsp;that&nbsp;emitted&nbsp;the&nbsp;event.<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;child&nbsp;runnable&nbsp;that&nbsp;gets&nbsp;invoked&nbsp;as&nbsp;part&nbsp;of&nbsp;the&nbsp;execution&nbsp;of&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;parent&nbsp;runnable&nbsp;is&nbsp;assigned&nbsp;its&nbsp;own&nbsp;unique&nbsp;ID.<br>
*&nbsp;``tags``:&nbsp;Optional[List[str]]&nbsp;-&nbsp;The&nbsp;tags&nbsp;of&nbsp;the&nbsp;runnable&nbsp;that&nbsp;generated<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;event.<br>
*&nbsp;``metadata``:&nbsp;Optional[Dict[str,&nbsp;Any]]&nbsp;-&nbsp;The&nbsp;metadata&nbsp;of&nbsp;the&nbsp;runnable<br>
&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;generated&nbsp;the&nbsp;event.<br>
*&nbsp;``data``:&nbsp;Dict[str,&nbsp;Any]<br>
&nbsp;<br>
&nbsp;<br>
Below&nbsp;is&nbsp;a&nbsp;table&nbsp;that&nbsp;illustrates&nbsp;some&nbsp;evens&nbsp;that&nbsp;might&nbsp;be&nbsp;emitted&nbsp;by&nbsp;various<br>
chains.&nbsp;Metadata&nbsp;fields&nbsp;have&nbsp;been&nbsp;omitted&nbsp;from&nbsp;the&nbsp;table&nbsp;for&nbsp;brevity.<br>
Chain&nbsp;definitions&nbsp;have&nbsp;been&nbsp;included&nbsp;after&nbsp;the&nbsp;table.<br>
&nbsp;<br>
|&nbsp;event&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;chunk&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;input&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;output&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|----------------------|------------------|---------------------------------|-----------------------------------------------|-------------------------------------------------|<br>
|&nbsp;on_chat_model_start&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"messages":&nbsp;[[SystemMessage,&nbsp;HumanMessage]]}&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_chat_model_stream&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;AIMessageChunk(content="hello")&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_chat_model_end&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"messages":&nbsp;[[SystemMessage,&nbsp;HumanMessage]]}&nbsp;|&nbsp;{"generations":&nbsp;[...],&nbsp;"llm_output":&nbsp;None,&nbsp;...}&nbsp;|<br>
|&nbsp;on_llm_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{'input':&nbsp;'hello'}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_llm_stream&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;'Hello'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_llm_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;'Hello&nbsp;human!'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_chain_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;format_docs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_chain_stream&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;format_docs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;"hello&nbsp;world!,&nbsp;goodbye&nbsp;world!"&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_chain_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;format_docs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[Document(...)]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;"hello&nbsp;world!,&nbsp;goodbye&nbsp;world!"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_tool_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;some_tool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"x":&nbsp;1,&nbsp;"y":&nbsp;"2"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_tool_stream&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;some_tool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"x":&nbsp;1,&nbsp;"y":&nbsp;"2"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_tool_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;some_tool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"x":&nbsp;1,&nbsp;"y":&nbsp;"2"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_retriever_start&nbsp;&nbsp;&nbsp;|&nbsp;[retriever&nbsp;name]&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"query":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_retriever_chunk&nbsp;&nbsp;&nbsp;|&nbsp;[retriever&nbsp;name]&nbsp;|&nbsp;{documents:&nbsp;[...]}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_retriever_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[retriever&nbsp;name]&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"query":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{documents:&nbsp;[...]}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_prompt_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[template_name]&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"question":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_prompt_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[template_name]&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"question":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;ChatPromptValue(messages:&nbsp;[SystemMessage,&nbsp;...])&nbsp;|<br>
&nbsp;<br>
Here&nbsp;are&nbsp;declarations&nbsp;associated&nbsp;with&nbsp;the&nbsp;events&nbsp;shown&nbsp;above:<br>
&nbsp;<br>
`format_docs`:<br>
&nbsp;<br>
```python<br>
def&nbsp;format_docs(docs:&nbsp;List[Document])&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;'''Format&nbsp;the&nbsp;docs.'''<br>
&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;",&nbsp;".join([doc.page_content&nbsp;for&nbsp;doc&nbsp;in&nbsp;docs])<br>
&nbsp;<br>
format_docs&nbsp;=&nbsp;RunnableLambda(format_docs)<br>
```<br>
&nbsp;<br>
`some_tool`:<br>
&nbsp;<br>
```python<br>
@tool<br>
def&nbsp;some_tool(x:&nbsp;int,&nbsp;y:&nbsp;str)&nbsp;-&gt;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;'''Some_tool.'''<br>
&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;{"x":&nbsp;x,&nbsp;"y":&nbsp;y}<br>
```<br>
&nbsp;<br>
`prompt`:<br>
&nbsp;<br>
```python<br>
template&nbsp;=&nbsp;ChatPromptTemplate.from_messages(<br>
&nbsp;&nbsp;&nbsp;&nbsp;[("system",&nbsp;"You&nbsp;are&nbsp;Cat&nbsp;Agent&nbsp;007"),&nbsp;("human",&nbsp;"{question}")]<br>
).<a href="#ChatOpenAI-with_config">with_config</a>({"run_name":&nbsp;"my_template",&nbsp;"tags":&nbsp;["my_template"]})<br>
```<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;reverse(s:&nbsp;str)&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;s[::-1]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;RunnableLambda(func=reverse)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;events&nbsp;=&nbsp;[<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;event&nbsp;async&nbsp;for&nbsp;event&nbsp;in&nbsp;chain.<a href="#ChatOpenAI-astream_events">astream_events</a>("hello",&nbsp;version="v1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;will&nbsp;produce&nbsp;the&nbsp;following&nbsp;events&nbsp;(run_id&nbsp;has&nbsp;been&nbsp;omitted&nbsp;for&nbsp;brevity):<br>
&nbsp;&nbsp;&nbsp;&nbsp;[<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"data":&nbsp;{"input":&nbsp;"hello"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"event":&nbsp;"on_chain_start",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata":&nbsp;{},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"reverse",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;[],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"data":&nbsp;{"chunk":&nbsp;"olleh"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"event":&nbsp;"on_chain_stream",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata":&nbsp;{},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"reverse",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;[],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"data":&nbsp;{"output":&nbsp;"olleh"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"event":&nbsp;"on_chain_end",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata":&nbsp;{},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"reverse",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;[],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;]<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;version:&nbsp;The&nbsp;version&nbsp;of&nbsp;the&nbsp;schema&nbsp;to&nbsp;use.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Currently&nbsp;only&nbsp;version&nbsp;1&nbsp;is&nbsp;available.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;No&nbsp;default&nbsp;will&nbsp;be&nbsp;assigned&nbsp;until&nbsp;the&nbsp;API&nbsp;is&nbsp;stabilized.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_names:&nbsp;Only&nbsp;include&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_types:&nbsp;Only&nbsp;include&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_tags:&nbsp;Only&nbsp;include&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_names:&nbsp;Exclude&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_types:&nbsp;Exclude&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_tags:&nbsp;Exclude&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;These&nbsp;will&nbsp;be&nbsp;passed&nbsp;to&nbsp;astream_log&nbsp;as&nbsp;this&nbsp;implementation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;astream_events&nbsp;is&nbsp;built&nbsp;on&nbsp;top&nbsp;of&nbsp;astream_log.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;async&nbsp;stream&nbsp;of&nbsp;StreamEvents.<br>
&nbsp;<br>
Notes<br>
-----<br>
..&nbsp;beta::<br>
&nbsp;&nbsp;&nbsp;This&nbsp;API&nbsp;is&nbsp;in&nbsp;beta&nbsp;and&nbsp;may&nbsp;change&nbsp;in&nbsp;the&nbsp;future.</tt></dd></dl>

<dl><dt>async <a name="ChatOpenAI-astream_log"><strong>astream_log</strong></a>(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'</dt><dd><tt>Stream&nbsp;all&nbsp;output&nbsp;from&nbsp;a&nbsp;runnable,&nbsp;as&nbsp;reported&nbsp;to&nbsp;the&nbsp;callback&nbsp;system.<br>
This&nbsp;includes&nbsp;all&nbsp;inner&nbsp;runs&nbsp;of&nbsp;LLMs,&nbsp;Retrievers,&nbsp;Tools,&nbsp;etc.<br>
&nbsp;<br>
Output&nbsp;is&nbsp;streamed&nbsp;as&nbsp;Log&nbsp;objects,&nbsp;which&nbsp;include&nbsp;a&nbsp;list&nbsp;of<br>
jsonpatch&nbsp;ops&nbsp;that&nbsp;describe&nbsp;how&nbsp;the&nbsp;state&nbsp;of&nbsp;the&nbsp;run&nbsp;has&nbsp;changed&nbsp;in&nbsp;each<br>
step,&nbsp;and&nbsp;the&nbsp;final&nbsp;state&nbsp;of&nbsp;the&nbsp;run.<br>
&nbsp;<br>
The&nbsp;jsonpatch&nbsp;ops&nbsp;can&nbsp;be&nbsp;applied&nbsp;in&nbsp;order&nbsp;to&nbsp;construct&nbsp;state.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;diff:&nbsp;Whether&nbsp;to&nbsp;yield&nbsp;diffs&nbsp;between&nbsp;each&nbsp;step,&nbsp;or&nbsp;the&nbsp;current&nbsp;state.<br>
&nbsp;&nbsp;&nbsp;&nbsp;with_streamed_output_list:&nbsp;Whether&nbsp;to&nbsp;yield&nbsp;the&nbsp;streamed_output&nbsp;list.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_names:&nbsp;Only&nbsp;include&nbsp;logs&nbsp;with&nbsp;these&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_types:&nbsp;Only&nbsp;include&nbsp;logs&nbsp;with&nbsp;these&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_tags:&nbsp;Only&nbsp;include&nbsp;logs&nbsp;with&nbsp;these&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_names:&nbsp;Exclude&nbsp;logs&nbsp;with&nbsp;these&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_types:&nbsp;Exclude&nbsp;logs&nbsp;with&nbsp;these&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_tags:&nbsp;Exclude&nbsp;logs&nbsp;with&nbsp;these&nbsp;tags.</tt></dd></dl>

<dl><dt>async <a name="ChatOpenAI-atransform"><strong>atransform</strong></a>(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -&gt; 'AsyncIterator[Output]'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;atransform,&nbsp;which&nbsp;buffers&nbsp;input&nbsp;and&nbsp;calls&nbsp;astream.<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;start&nbsp;producing&nbsp;output&nbsp;while<br>
input&nbsp;is&nbsp;still&nbsp;being&nbsp;generated.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-batch"><strong>batch</strong></a>(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -&gt; 'List[Output]'</dt><dd><tt>Default&nbsp;implementation&nbsp;runs&nbsp;invoke&nbsp;in&nbsp;parallel&nbsp;using&nbsp;a&nbsp;thread&nbsp;pool&nbsp;executor.<br>
&nbsp;<br>
The&nbsp;default&nbsp;implementation&nbsp;of&nbsp;batch&nbsp;works&nbsp;well&nbsp;for&nbsp;IO&nbsp;bound&nbsp;runnables.<br>
&nbsp;<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;batch&nbsp;more&nbsp;efficiently;<br>
e.g.,&nbsp;if&nbsp;the&nbsp;underlying&nbsp;runnable&nbsp;uses&nbsp;an&nbsp;API&nbsp;which&nbsp;supports&nbsp;a&nbsp;batch&nbsp;mode.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-bind"><strong>bind</strong></a>(self, **kwargs: 'Any') -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;arguments&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-config_schema"><strong>config_schema</strong></a>(self, *, include: 'Optional[Sequence[str]]' = None) -&gt; 'Type[BaseModel]'</dt><dd><tt>The&nbsp;type&nbsp;of&nbsp;config&nbsp;this&nbsp;runnable&nbsp;accepts&nbsp;specified&nbsp;as&nbsp;a&nbsp;pydantic&nbsp;model.<br>
&nbsp;<br>
To&nbsp;mark&nbsp;a&nbsp;field&nbsp;as&nbsp;configurable,&nbsp;see&nbsp;the&nbsp;`configurable_fields`<br>
and&nbsp;`configurable_alternatives`&nbsp;methods.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;include:&nbsp;A&nbsp;list&nbsp;of&nbsp;fields&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;config&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;config.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-get_graph"><strong>get_graph</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'Graph'</dt><dd><tt>Return&nbsp;a&nbsp;graph&nbsp;representation&nbsp;of&nbsp;this&nbsp;runnable.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-get_input_schema"><strong>get_input_schema</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'Type[BaseModel]'</dt><dd><tt>Get&nbsp;a&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;input&nbsp;to&nbsp;the&nbsp;runnable.<br>
&nbsp;<br>
Runnables&nbsp;that&nbsp;leverage&nbsp;the&nbsp;configurable_fields&nbsp;and&nbsp;configurable_alternatives<br>
methods&nbsp;will&nbsp;have&nbsp;a&nbsp;dynamic&nbsp;input&nbsp;schema&nbsp;that&nbsp;depends&nbsp;on&nbsp;which<br>
configuration&nbsp;the&nbsp;runnable&nbsp;is&nbsp;invoked&nbsp;with.<br>
&nbsp;<br>
This&nbsp;method&nbsp;allows&nbsp;to&nbsp;get&nbsp;an&nbsp;input&nbsp;schema&nbsp;for&nbsp;a&nbsp;specific&nbsp;configuration.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;generating&nbsp;the&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;input.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-get_name"><strong>get_name</strong></a>(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -&gt; 'str'</dt><dd><tt>Get&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;runnable.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-get_output_schema"><strong>get_output_schema</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'Type[BaseModel]'</dt><dd><tt>Get&nbsp;a&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;output&nbsp;to&nbsp;the&nbsp;runnable.<br>
&nbsp;<br>
Runnables&nbsp;that&nbsp;leverage&nbsp;the&nbsp;configurable_fields&nbsp;and&nbsp;configurable_alternatives<br>
methods&nbsp;will&nbsp;have&nbsp;a&nbsp;dynamic&nbsp;output&nbsp;schema&nbsp;that&nbsp;depends&nbsp;on&nbsp;which<br>
configuration&nbsp;the&nbsp;runnable&nbsp;is&nbsp;invoked&nbsp;with.<br>
&nbsp;<br>
This&nbsp;method&nbsp;allows&nbsp;to&nbsp;get&nbsp;an&nbsp;output&nbsp;schema&nbsp;for&nbsp;a&nbsp;specific&nbsp;configuration.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;generating&nbsp;the&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;output.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-get_prompts"><strong>get_prompts</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'List[BasePromptTemplate]'</dt></dl>

<dl><dt><a name="ChatOpenAI-map"><strong>map</strong></a>(self) -&gt; 'Runnable[List[Input], List[Output]]'</dt><dd><tt>Return&nbsp;a&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;maps&nbsp;a&nbsp;list&nbsp;of&nbsp;inputs&nbsp;to&nbsp;a&nbsp;list&nbsp;of&nbsp;outputs,<br>
by&nbsp;calling&nbsp;<a href="#ChatOpenAI-invoke">invoke</a>()&nbsp;with&nbsp;each&nbsp;input.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-pick"><strong>pick</strong></a>(self, keys: 'Union[str, List[str]]') -&gt; 'RunnableSerializable[Any, Any]'</dt><dd><tt>Pick&nbsp;keys&nbsp;from&nbsp;the&nbsp;dict&nbsp;output&nbsp;of&nbsp;this&nbsp;runnable.<br>
Returns&nbsp;a&nbsp;new&nbsp;runnable.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-pipe"><strong>pipe</strong></a>(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -&gt; 'RunnableSerializable[Input, Other]'</dt><dd><tt>Compose&nbsp;this&nbsp;runnable&nbsp;with&nbsp;another&nbsp;object&nbsp;to&nbsp;create&nbsp;a&nbsp;RunnableSequence.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-transform"><strong>transform</strong></a>(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -&gt; 'Iterator[Output]'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;transform,&nbsp;which&nbsp;buffers&nbsp;input&nbsp;and&nbsp;then&nbsp;calls&nbsp;stream.<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;start&nbsp;producing&nbsp;output&nbsp;while<br>
input&nbsp;is&nbsp;still&nbsp;being&nbsp;generated.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-with_config"><strong>with_config</strong></a>(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;config&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-with_fallbacks"><strong>with_fallbacks</strong></a>(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (&lt;class 'Exception'&gt;,), exception_key: 'Optional[str]' = None) -&gt; 'RunnableWithFallbacksT[Input, Output]'</dt><dd><tt>Add&nbsp;fallbacks&nbsp;to&nbsp;a&nbsp;runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fallbacks:&nbsp;A&nbsp;sequence&nbsp;of&nbsp;runnables&nbsp;to&nbsp;try&nbsp;if&nbsp;the&nbsp;original&nbsp;runnable&nbsp;fails.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exceptions_to_handle:&nbsp;A&nbsp;tuple&nbsp;of&nbsp;exception&nbsp;types&nbsp;to&nbsp;handle.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exception_key:&nbsp;If&nbsp;string&nbsp;is&nbsp;specified&nbsp;then&nbsp;handled&nbsp;exceptions&nbsp;will&nbsp;be&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;fallbacks&nbsp;as&nbsp;part&nbsp;of&nbsp;the&nbsp;input&nbsp;under&nbsp;the&nbsp;specified&nbsp;key.&nbsp;If&nbsp;None,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;exceptions&nbsp;will&nbsp;not&nbsp;be&nbsp;passed&nbsp;to&nbsp;fallbacks.&nbsp;If&nbsp;used,&nbsp;the&nbsp;base&nbsp;runnable<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;its&nbsp;fallbacks&nbsp;must&nbsp;accept&nbsp;a&nbsp;dictionary&nbsp;as&nbsp;input.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;will&nbsp;try&nbsp;the&nbsp;original&nbsp;runnable,&nbsp;and&nbsp;then&nbsp;each<br>
&nbsp;&nbsp;&nbsp;&nbsp;fallback&nbsp;in&nbsp;order,&nbsp;upon&nbsp;failures.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-with_listeners"><strong>with_listeners</strong></a>(self, *, on_start: 'Optional[Listener]' = None, on_end: 'Optional[Listener]' = None, on_error: 'Optional[Listener]' = None) -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;lifecycle&nbsp;listeners&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
on_start:&nbsp;Called&nbsp;before&nbsp;the&nbsp;runnable&nbsp;starts&nbsp;running,&nbsp;with&nbsp;the&nbsp;Run&nbsp;object.<br>
on_end:&nbsp;Called&nbsp;after&nbsp;the&nbsp;runnable&nbsp;finishes&nbsp;running,&nbsp;with&nbsp;the&nbsp;Run&nbsp;object.<br>
on_error:&nbsp;Called&nbsp;if&nbsp;the&nbsp;runnable&nbsp;throws&nbsp;an&nbsp;error,&nbsp;with&nbsp;the&nbsp;Run&nbsp;object.<br>
&nbsp;<br>
The&nbsp;Run&nbsp;object&nbsp;contains&nbsp;information&nbsp;about&nbsp;the&nbsp;run,&nbsp;including&nbsp;its&nbsp;id,<br>
type,&nbsp;input,&nbsp;output,&nbsp;error,&nbsp;start_time,&nbsp;end_time,&nbsp;and&nbsp;any&nbsp;tags&nbsp;or&nbsp;metadata<br>
added&nbsp;to&nbsp;the&nbsp;run.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-with_retry"><strong>with_retry</strong></a>(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (&lt;class 'Exception'&gt;,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Create&nbsp;a&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;retries&nbsp;the&nbsp;original&nbsp;runnable&nbsp;on&nbsp;exceptions.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;retry_if_exception_type:&nbsp;A&nbsp;tuple&nbsp;of&nbsp;exception&nbsp;types&nbsp;to&nbsp;retry&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;wait_exponential_jitter:&nbsp;Whether&nbsp;to&nbsp;add&nbsp;jitter&nbsp;to&nbsp;the&nbsp;wait&nbsp;time<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;between&nbsp;retries<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop_after_attempt:&nbsp;The&nbsp;maximum&nbsp;number&nbsp;of&nbsp;attempts&nbsp;to&nbsp;make&nbsp;before&nbsp;giving&nbsp;up<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;retries&nbsp;the&nbsp;original&nbsp;runnable&nbsp;on&nbsp;exceptions.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-with_types"><strong>with_types</strong></a>(self, *, input_type: 'Optional[Type[Input]]' = None, output_type: 'Optional[Type[Output]]' = None) -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;input&nbsp;and&nbsp;output&nbsp;types&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_core.runnables.base.html#Runnable">langchain_core.runnables.base.Runnable</a>:<br>
<dl><dt><strong>config_specs</strong></dt>
<dd><tt>List&nbsp;configurable&nbsp;fields&nbsp;for&nbsp;this&nbsp;runnable.</tt></dd>
</dl>
<dl><dt><strong>input_schema</strong></dt>
<dd><tt>The&nbsp;type&nbsp;of&nbsp;input&nbsp;this&nbsp;runnable&nbsp;accepts&nbsp;specified&nbsp;as&nbsp;a&nbsp;pydantic&nbsp;model.</tt></dd>
</dl>
<dl><dt><strong>output_schema</strong></dt>
<dd><tt>The&nbsp;type&nbsp;of&nbsp;output&nbsp;this&nbsp;runnable&nbsp;produces&nbsp;specified&nbsp;as&nbsp;a&nbsp;pydantic&nbsp;model.</tt></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="langchain_core.runnables.base.html#Runnable">langchain_core.runnables.base.Runnable</a>:<br>
<dl><dt><strong>name</strong> = None</dl>

<hr>
Class methods inherited from <a href="typing.html#Generic">typing.Generic</a>:<br>
<dl><dt><a name="ChatOpenAI-__class_getitem__"><strong>__class_getitem__</strong></a>(params)<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatOpenAI-__init_subclass__"><strong>__init_subclass__</strong></a>(*args, **kwargs)<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>This&nbsp;method&nbsp;is&nbsp;called&nbsp;when&nbsp;a&nbsp;class&nbsp;is&nbsp;subclassed.<br>
&nbsp;<br>
The&nbsp;default&nbsp;implementation&nbsp;does&nbsp;nothing.&nbsp;It&nbsp;may&nbsp;be<br>
overridden&nbsp;to&nbsp;extend&nbsp;subclasses.</tt></dd></dl>

</td></tr></table> <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="OpenAI">class <strong>OpenAI</strong></a>(<a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a>, <a href="langchain_openai.llms.base.html#OpenAI">langchain_openai.llms.base.OpenAI</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt><a href="#OpenAI">OpenAI</a>(*,&nbsp;name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;cache:&nbsp;Optional[bool]&nbsp;=&nbsp;None,&nbsp;verbose:&nbsp;bool&nbsp;=&nbsp;None,&nbsp;callbacks:&nbsp;Union[List[langchain_core.callbacks.base.BaseCallbackHandler],&nbsp;langchain_core.callbacks.base.BaseCallbackManager,&nbsp;NoneType]&nbsp;=&nbsp;None,&nbsp;tags:&nbsp;Optional[List[str]]&nbsp;=&nbsp;None,&nbsp;metadata:&nbsp;Optional[Dict[str,&nbsp;Any]]&nbsp;=&nbsp;None,&nbsp;callback_manager:&nbsp;Optional[langchain_core.callbacks.base.BaseCallbackManager]&nbsp;=&nbsp;None,&nbsp;client:&nbsp;Any&nbsp;=&nbsp;None,&nbsp;async_client:&nbsp;Any&nbsp;=&nbsp;None,&nbsp;model_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;temperature:&nbsp;float&nbsp;=&nbsp;0.7,&nbsp;max_tokens:&nbsp;int&nbsp;=&nbsp;256,&nbsp;top_p:&nbsp;float&nbsp;=&nbsp;1,&nbsp;frequency_penalty:&nbsp;float&nbsp;=&nbsp;0,&nbsp;presence_penalty:&nbsp;float&nbsp;=&nbsp;0,&nbsp;n:&nbsp;int&nbsp;=&nbsp;1,&nbsp;best_of:&nbsp;int&nbsp;=&nbsp;1,&nbsp;model_kwargs:&nbsp;Dict[str,&nbsp;Any]&nbsp;=&nbsp;None,&nbsp;api_key:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;base_url:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;organization:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;openai_proxy:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;batch_size:&nbsp;int&nbsp;=&nbsp;20,&nbsp;timeout:&nbsp;Union[float,&nbsp;Tuple[float,&nbsp;float],&nbsp;Any,&nbsp;NoneType]&nbsp;=&nbsp;None,&nbsp;logit_bias:&nbsp;Optional[Dict[str,&nbsp;float]]&nbsp;=&nbsp;None,&nbsp;max_retries:&nbsp;int&nbsp;=&nbsp;2,&nbsp;streaming:&nbsp;bool&nbsp;=&nbsp;False,&nbsp;allowed_special:&nbsp;Union[Literal['all'],&nbsp;AbstractSet[str]]&nbsp;=&nbsp;set(),&nbsp;disallowed_special:&nbsp;Union[Literal['all'],&nbsp;Collection[str]]&nbsp;=&nbsp;'all',&nbsp;tiktoken_model_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;default_headers:&nbsp;Optional[Mapping[str,&nbsp;str]]&nbsp;=&nbsp;None,&nbsp;default_query:&nbsp;Optional[Mapping[str,&nbsp;object]]&nbsp;=&nbsp;None,&nbsp;http_client:&nbsp;Optional[Any]&nbsp;=&nbsp;None,&nbsp;proxy_client:&nbsp;Optional[Any]&nbsp;=&nbsp;None,&nbsp;deployment_id:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;config_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;config_id:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;proxy_model_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;api_version:&nbsp;Optional[str]&nbsp;=&nbsp;None)&nbsp;-&amp;gt;&nbsp;None<br>
&nbsp;<br>
<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%"><dl><dt>Method resolution order:</dt>
<dd><a href="gen_ai_hub.proxy.langchain.openai.html#OpenAI">OpenAI</a></dd>
<dd><a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a></dd>
<dd><a href="gen_ai_hub.proxy.langchain.base.html#BaseAuth">gen_ai_hub.proxy.langchain.base.BaseAuth</a></dd>
<dd><a href="langchain_openai.llms.base.html#OpenAI">langchain_openai.llms.base.OpenAI</a></dd>
<dd><a href="langchain_openai.llms.base.html#BaseOpenAI">langchain_openai.llms.base.BaseOpenAI</a></dd>
<dd><a href="langchain_core.language_models.llms.html#BaseLLM">langchain_core.language_models.llms.BaseLLM</a></dd>
<dd><a href="langchain_core.language_models.base.html#BaseLanguageModel">langchain_core.language_models.base.BaseLanguageModel</a></dd>
<dd><a href="langchain_core.runnables.base.html#RunnableSerializable">langchain_core.runnables.base.RunnableSerializable</a></dd>
<dd><a href="langchain_core.load.serializable.html#Serializable">langchain_core.load.serializable.Serializable</a></dd>
<dd><a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a></dd>
<dd><a href="pydantic.v1.utils.html#Representation">pydantic.v1.utils.Representation</a></dd>
<dd><a href="langchain_core.runnables.base.html#Runnable">langchain_core.runnables.base.Runnable</a></dd>
<dd><a href="typing.html#Generic">typing.Generic</a></dd>
<dd><a href="abc.html#ABC">abc.ABC</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Class methods defined here:<br>
<dl><dt><a name="OpenAI-validate_environment"><strong>validate_environment</strong></a>(values: Dict) -&gt; Dict<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Validates&nbsp;the&nbsp;environment.<br>
&nbsp;<br>
:param&nbsp;values:&nbsp;The&nbsp;input&nbsp;values<br>
:type&nbsp;values:&nbsp;Dict<br>
:return:&nbsp;The&nbsp;validated&nbsp;values<br>
:rtype:&nbsp;Dict</tt></dd></dl>

<hr>
Static methods defined here:<br>
<dl><dt><a name="OpenAI-__json_encoder__"><strong>__json_encoder__</strong></a> = pydantic_encoder(obj: Any) -&gt; Any</dt></dl>

<dl><dt><a name="OpenAI-__new__"><strong>__new__</strong></a>(cls, **data: Any)</dt><dd><tt>Initialize&nbsp;the&nbsp;<a href="#OpenAI">OpenAI</a>&nbsp;object.</tt></dd></dl>

<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>__abstractmethods__</strong> = frozenset()</dl>

<dl><dt><strong>__annotations__</strong> = {'model_name': typing.Optional[str], 'openai_api_version': typing.Optional[str]}</dl>

<dl><dt><strong>__class_vars__</strong> = set()</dl>

<dl><dt><strong>__config__</strong> = &lt;class 'pydantic.v1.config.Config'&gt;</dl>

<dl><dt><strong>__custom_root_type__</strong> = False</dl>

<dl><dt><strong>__exclude_fields__</strong> = {'async_client': True, 'callback_manager': True, 'callbacks': True, 'client': True, 'metadata': True, 'tags': True}</dl>

<dl><dt><strong>__fields__</strong> = {'allowed_special': ModelField(name='allowed_special', type=Union[Li...AbstractSet[str]], required=False, default=set()), 'async_client': ModelField(name='async_client', type=Optional[Any], required=False, default=None), 'batch_size': ModelField(name='batch_size', type=int, required=False, default=20), 'best_of': ModelField(name='best_of', type=int, required=False, default=1), 'cache': ModelField(name='cache', type=Optional[bool], required=False, default=None), 'callback_manager': ModelField(name='callback_manager', type=Optiona...seCallbackManager], required=False, default=None), 'callbacks': ModelField(name='callbacks', type=Union[List[lan...Manager, NoneType], required=False, default=None), 'client': ModelField(name='client', type=Optional[Any], required=False, default=None), 'config_id': ModelField(name='config_id', type=Optional[str], required=False, default=None), 'config_name': ModelField(name='config_name', type=Optional[str], required=False, default=None), ...}</dl>

<dl><dt><strong>__hash__</strong> = None</dl>

<dl><dt><strong>__include_fields__</strong> = None</dl>

<dl><dt><strong>__parameters__</strong> = ()</dl>

<dl><dt><strong>__post_root_validators__</strong> = [(False, &lt;function BaseLLM.raise_deprecation&gt;), (False, &lt;function OpenAI.validate_environment&gt;)]</dl>

<dl><dt><strong>__pre_root_validators__</strong> = [&lt;function BaseOpenAI.build_extra&gt;]</dl>

<dl><dt><strong>__private_attributes__</strong> = {'_lc_kwargs': ModelPrivateAttr(default=PydanticUndefined, default_factory=&lt;class 'dict'&gt;)}</dl>

<dl><dt><strong>__schema_cache__</strong> = {}</dl>

<dl><dt><strong>__signature__</strong> = &lt;Signature (*, name: Optional[str] = None, cache...None, api_version: Optional[str] = None) -&gt; None&gt;</dl>

<dl><dt><strong>__validators__</strong> = {'verbose': [&lt;pydantic.v1.class_validators.Validator object&gt;]}</dl>

<hr>
Class methods inherited from <a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a>:<br>
<dl><dt><a name="OpenAI-validate_clients"><strong>validate_clients</strong></a>(values: Dict) -&gt; Dict<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<hr>
Class methods inherited from <a href="langchain_openai.llms.base.html#OpenAI">langchain_openai.llms.base.OpenAI</a>:<br>
<dl><dt><a name="OpenAI-get_lc_namespace"><strong>get_lc_namespace</strong></a>() -&gt; 'List[str]'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Get&nbsp;the&nbsp;namespace&nbsp;of&nbsp;the&nbsp;langchain&nbsp;object.</tt></dd></dl>

<dl><dt><a name="OpenAI-is_lc_serializable"><strong>is_lc_serializable</strong></a>() -&gt; 'bool'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Return&nbsp;whether&nbsp;this&nbsp;model&nbsp;can&nbsp;be&nbsp;serialized&nbsp;by&nbsp;Langchain.</tt></dd></dl>

<hr>
Methods inherited from <a href="langchain_openai.llms.base.html#BaseOpenAI">langchain_openai.llms.base.BaseOpenAI</a>:<br>
<dl><dt><a name="OpenAI-create_llm_result"><strong>create_llm_result</strong></a>(self, choices: 'Any', prompts: 'List[str]', params: 'Dict[str, Any]', token_usage: 'Dict[str, int]', *, system_fingerprint: 'Optional[str]' = None) -&gt; 'LLMResult'</dt><dd><tt>Create&nbsp;the&nbsp;LLMResult&nbsp;from&nbsp;the&nbsp;choices&nbsp;and&nbsp;prompts.</tt></dd></dl>

<dl><dt><a name="OpenAI-get_sub_prompts"><strong>get_sub_prompts</strong></a>(self, params: 'Dict[str, Any]', prompts: 'List[str]', stop: 'Optional[List[str]]' = None) -&gt; 'List[List[str]]'</dt><dd><tt>Get&nbsp;the&nbsp;sub&nbsp;prompts&nbsp;for&nbsp;llm&nbsp;call.</tt></dd></dl>

<dl><dt><a name="OpenAI-get_token_ids"><strong>get_token_ids</strong></a>(self, text: 'str') -&gt; 'List[int]'</dt><dd><tt>Get&nbsp;the&nbsp;token&nbsp;IDs&nbsp;using&nbsp;the&nbsp;tiktoken&nbsp;package.</tt></dd></dl>

<dl><dt><a name="OpenAI-max_tokens_for_prompt"><strong>max_tokens_for_prompt</strong></a>(self, prompt: 'str') -&gt; 'int'</dt><dd><tt>Calculate&nbsp;the&nbsp;maximum&nbsp;number&nbsp;of&nbsp;tokens&nbsp;possible&nbsp;to&nbsp;generate&nbsp;for&nbsp;a&nbsp;prompt.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompt:&nbsp;The&nbsp;prompt&nbsp;to&nbsp;pass&nbsp;into&nbsp;the&nbsp;model.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;maximum&nbsp;number&nbsp;of&nbsp;tokens&nbsp;to&nbsp;generate&nbsp;for&nbsp;a&nbsp;prompt.<br>
&nbsp;<br>
Example:<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_tokens&nbsp;=&nbsp;openai.max_token_for_prompt("Tell&nbsp;me&nbsp;a&nbsp;joke.")</tt></dd></dl>

<hr>
Class methods inherited from <a href="langchain_openai.llms.base.html#BaseOpenAI">langchain_openai.llms.base.BaseOpenAI</a>:<br>
<dl><dt><a name="OpenAI-build_extra"><strong>build_extra</strong></a>(values: 'Dict[str, Any]') -&gt; 'Dict[str, Any]'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Build&nbsp;extra&nbsp;kwargs&nbsp;from&nbsp;additional&nbsp;params&nbsp;that&nbsp;were&nbsp;passed&nbsp;in.</tt></dd></dl>

<hr>
Static methods inherited from <a href="langchain_openai.llms.base.html#BaseOpenAI">langchain_openai.llms.base.BaseOpenAI</a>:<br>
<dl><dt><a name="OpenAI-modelname_to_contextsize"><strong>modelname_to_contextsize</strong></a>(modelname: 'str') -&gt; 'int'</dt><dd><tt>Calculate&nbsp;the&nbsp;maximum&nbsp;number&nbsp;of&nbsp;tokens&nbsp;possible&nbsp;to&nbsp;generate&nbsp;for&nbsp;a&nbsp;model.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;modelname:&nbsp;The&nbsp;modelname&nbsp;we&nbsp;want&nbsp;to&nbsp;know&nbsp;the&nbsp;context&nbsp;size&nbsp;for.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;maximum&nbsp;context&nbsp;size<br>
&nbsp;<br>
Example:<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_tokens&nbsp;=&nbsp;openai.<a href="#OpenAI-modelname_to_contextsize">modelname_to_contextsize</a>("gpt-3.5-turbo-instruct")</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_openai.llms.base.html#BaseOpenAI">langchain_openai.llms.base.BaseOpenAI</a>:<br>
<dl><dt><strong>lc_attributes</strong></dt>
<dd><tt>List&nbsp;of&nbsp;attribute&nbsp;names&nbsp;that&nbsp;should&nbsp;be&nbsp;included&nbsp;in&nbsp;the&nbsp;serialized&nbsp;kwargs.<br>
&nbsp;<br>
These&nbsp;attributes&nbsp;must&nbsp;be&nbsp;accepted&nbsp;by&nbsp;the&nbsp;constructor.</tt></dd>
</dl>
<dl><dt><strong>lc_secrets</strong></dt>
<dd><tt>A&nbsp;map&nbsp;of&nbsp;constructor&nbsp;argument&nbsp;names&nbsp;to&nbsp;secret&nbsp;ids.<br>
&nbsp;<br>
For&nbsp;example,<br>
&nbsp;&nbsp;&nbsp;&nbsp;{"openai_api_key":&nbsp;"OPENAI_API_KEY"}</tt></dd>
</dl>
<dl><dt><strong>max_context_size</strong></dt>
<dd><tt>Get&nbsp;max&nbsp;context&nbsp;size&nbsp;for&nbsp;this&nbsp;model.</tt></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="langchain_openai.llms.base.html#BaseOpenAI">langchain_openai.llms.base.BaseOpenAI</a>:<br>
<dl><dt><strong>Config</strong> = &lt;class 'langchain_openai.llms.base.BaseOpenAI.Config'&gt;<dd><tt>Configuration&nbsp;for&nbsp;this&nbsp;pydantic&nbsp;object.</tt></dl>

<hr>
Methods inherited from <a href="langchain_core.language_models.llms.html#BaseLLM">langchain_core.language_models.llms.BaseLLM</a>:<br>
<dl><dt><a name="OpenAI-__call__"><strong>__call__</strong></a>(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -&gt; 'str'</dt><dd><tt>[*Deprecated*]&nbsp;&nbsp;Check&nbsp;Cache&nbsp;and&nbsp;run&nbsp;the&nbsp;LLM&nbsp;on&nbsp;the&nbsp;given&nbsp;prompt&nbsp;and&nbsp;input.[*Deprecated*]&nbsp;Check&nbsp;Cache&nbsp;and&nbsp;run&nbsp;the&nbsp;LLM&nbsp;on&nbsp;the&nbsp;given&nbsp;prompt&nbsp;and&nbsp;input.<br>
&nbsp;<br>
Notes<br>
-----<br>
..&nbsp;deprecated::&nbsp;0.1.7<br>
&nbsp;&nbsp;&nbsp;Use&nbsp;invoke&nbsp;instead.</tt></dd></dl>

<dl><dt><a name="OpenAI-__str__"><strong>__str__</strong></a>(self) -&gt; 'str'</dt><dd><tt>Get&nbsp;a&nbsp;string&nbsp;representation&nbsp;of&nbsp;the&nbsp;object&nbsp;for&nbsp;printing.</tt></dd></dl>

<dl><dt>async <a name="OpenAI-abatch"><strong>abatch</strong></a>(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -&gt; 'List[str]'</dt><dd><tt>Default&nbsp;implementation&nbsp;runs&nbsp;ainvoke&nbsp;in&nbsp;parallel&nbsp;using&nbsp;asyncio.gather.<br>
&nbsp;<br>
The&nbsp;default&nbsp;implementation&nbsp;of&nbsp;batch&nbsp;works&nbsp;well&nbsp;for&nbsp;IO&nbsp;bound&nbsp;runnables.<br>
&nbsp;<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;batch&nbsp;more&nbsp;efficiently;<br>
e.g.,&nbsp;if&nbsp;the&nbsp;underlying&nbsp;runnable&nbsp;uses&nbsp;an&nbsp;API&nbsp;which&nbsp;supports&nbsp;a&nbsp;batch&nbsp;mode.</tt></dd></dl>

<dl><dt>async <a name="OpenAI-agenerate"><strong>agenerate</strong></a>(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, run_name: 'Optional[Union[str, List[str]]]' = None, **kwargs: 'Any') -&gt; 'LLMResult'</dt><dd><tt>Asynchronously&nbsp;pass&nbsp;a&nbsp;sequence&nbsp;of&nbsp;prompts&nbsp;to&nbsp;a&nbsp;model&nbsp;and&nbsp;return&nbsp;generations.<br>
&nbsp;<br>
This&nbsp;method&nbsp;should&nbsp;make&nbsp;use&nbsp;of&nbsp;batched&nbsp;calls&nbsp;for&nbsp;models&nbsp;that&nbsp;expose&nbsp;a&nbsp;batched<br>
API.<br>
&nbsp;<br>
Use&nbsp;this&nbsp;method&nbsp;when&nbsp;you&nbsp;want&nbsp;to:<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;take&nbsp;advantage&nbsp;of&nbsp;batched&nbsp;calls,<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;need&nbsp;more&nbsp;output&nbsp;from&nbsp;the&nbsp;model&nbsp;than&nbsp;just&nbsp;the&nbsp;top&nbsp;generated&nbsp;value,<br>
&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;are&nbsp;building&nbsp;chains&nbsp;that&nbsp;are&nbsp;agnostic&nbsp;to&nbsp;the&nbsp;underlying&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;(e.g.,&nbsp;pure&nbsp;text&nbsp;completion&nbsp;models&nbsp;vs&nbsp;chat&nbsp;models).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompts:&nbsp;List&nbsp;of&nbsp;string&nbsp;prompts.<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop:&nbsp;Stop&nbsp;words&nbsp;to&nbsp;use&nbsp;when&nbsp;generating.&nbsp;Model&nbsp;output&nbsp;is&nbsp;cut&nbsp;off&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;occurrence&nbsp;of&nbsp;any&nbsp;of&nbsp;these&nbsp;substrings.<br>
&nbsp;&nbsp;&nbsp;&nbsp;callbacks:&nbsp;Callbacks&nbsp;to&nbsp;pass&nbsp;through.&nbsp;Used&nbsp;for&nbsp;executing&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality,&nbsp;such&nbsp;as&nbsp;logging&nbsp;or&nbsp;streaming,&nbsp;throughout&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Arbitrary&nbsp;additional&nbsp;keyword&nbsp;arguments.&nbsp;These&nbsp;are&nbsp;usually&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;model&nbsp;provider&nbsp;API&nbsp;call.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;LLMResult,&nbsp;which&nbsp;contains&nbsp;a&nbsp;list&nbsp;of&nbsp;candidate&nbsp;Generations&nbsp;for&nbsp;each&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;and&nbsp;additional&nbsp;model&nbsp;provider-specific&nbsp;output.</tt></dd></dl>

<dl><dt>async <a name="OpenAI-agenerate_prompt"><strong>agenerate_prompt</strong></a>(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -&gt; 'LLMResult'</dt><dd><tt>Asynchronously&nbsp;pass&nbsp;a&nbsp;sequence&nbsp;of&nbsp;prompts&nbsp;and&nbsp;return&nbsp;model&nbsp;generations.<br>
&nbsp;<br>
This&nbsp;method&nbsp;should&nbsp;make&nbsp;use&nbsp;of&nbsp;batched&nbsp;calls&nbsp;for&nbsp;models&nbsp;that&nbsp;expose&nbsp;a&nbsp;batched<br>
API.<br>
&nbsp;<br>
Use&nbsp;this&nbsp;method&nbsp;when&nbsp;you&nbsp;want&nbsp;to:<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;take&nbsp;advantage&nbsp;of&nbsp;batched&nbsp;calls,<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;need&nbsp;more&nbsp;output&nbsp;from&nbsp;the&nbsp;model&nbsp;than&nbsp;just&nbsp;the&nbsp;top&nbsp;generated&nbsp;value,<br>
&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;are&nbsp;building&nbsp;chains&nbsp;that&nbsp;are&nbsp;agnostic&nbsp;to&nbsp;the&nbsp;underlying&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;(e.g.,&nbsp;pure&nbsp;text&nbsp;completion&nbsp;models&nbsp;vs&nbsp;chat&nbsp;models).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompts:&nbsp;List&nbsp;of&nbsp;PromptValues.&nbsp;A&nbsp;PromptValue&nbsp;is&nbsp;an&nbsp;object&nbsp;that&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;converted&nbsp;to&nbsp;match&nbsp;the&nbsp;format&nbsp;of&nbsp;any&nbsp;language&nbsp;model&nbsp;(string&nbsp;for&nbsp;pure<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text&nbsp;generation&nbsp;models&nbsp;and&nbsp;BaseMessages&nbsp;for&nbsp;chat&nbsp;models).<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop:&nbsp;Stop&nbsp;words&nbsp;to&nbsp;use&nbsp;when&nbsp;generating.&nbsp;Model&nbsp;output&nbsp;is&nbsp;cut&nbsp;off&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;occurrence&nbsp;of&nbsp;any&nbsp;of&nbsp;these&nbsp;substrings.<br>
&nbsp;&nbsp;&nbsp;&nbsp;callbacks:&nbsp;Callbacks&nbsp;to&nbsp;pass&nbsp;through.&nbsp;Used&nbsp;for&nbsp;executing&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality,&nbsp;such&nbsp;as&nbsp;logging&nbsp;or&nbsp;streaming,&nbsp;throughout&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Arbitrary&nbsp;additional&nbsp;keyword&nbsp;arguments.&nbsp;These&nbsp;are&nbsp;usually&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;model&nbsp;provider&nbsp;API&nbsp;call.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;LLMResult,&nbsp;which&nbsp;contains&nbsp;a&nbsp;list&nbsp;of&nbsp;candidate&nbsp;Generations&nbsp;for&nbsp;each&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;and&nbsp;additional&nbsp;model&nbsp;provider-specific&nbsp;output.</tt></dd></dl>

<dl><dt>async <a name="OpenAI-ainvoke"><strong>ainvoke</strong></a>(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -&gt; 'str'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;ainvoke,&nbsp;calls&nbsp;invoke&nbsp;from&nbsp;a&nbsp;thread.<br>
&nbsp;<br>
The&nbsp;default&nbsp;implementation&nbsp;allows&nbsp;usage&nbsp;of&nbsp;async&nbsp;code&nbsp;even&nbsp;if<br>
the&nbsp;runnable&nbsp;did&nbsp;not&nbsp;implement&nbsp;a&nbsp;native&nbsp;async&nbsp;version&nbsp;of&nbsp;invoke.<br>
&nbsp;<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;run&nbsp;asynchronously.</tt></dd></dl>

<dl><dt>async <a name="OpenAI-apredict"><strong>apredict</strong></a>(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'str'</dt><dd><tt>[*Deprecated*][*Deprecated*]&nbsp;<br>
&nbsp;<br>
Notes<br>
-----<br>
..&nbsp;deprecated::&nbsp;0.1.7<br>
&nbsp;&nbsp;&nbsp;Use&nbsp;ainvoke&nbsp;instead.</tt></dd></dl>

<dl><dt>async <a name="OpenAI-apredict_messages"><strong>apredict_messages</strong></a>(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'BaseMessage'</dt><dd><tt>[*Deprecated*][*Deprecated*]&nbsp;<br>
&nbsp;<br>
Notes<br>
-----<br>
..&nbsp;deprecated::&nbsp;0.1.7<br>
&nbsp;&nbsp;&nbsp;Use&nbsp;ainvoke&nbsp;instead.</tt></dd></dl>

<dl><dt>async <a name="OpenAI-astream"><strong>astream</strong></a>(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -&gt; 'AsyncIterator[str]'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;astream,&nbsp;which&nbsp;calls&nbsp;ainvoke.<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;support&nbsp;streaming&nbsp;output.</tt></dd></dl>

<dl><dt><a name="OpenAI-batch"><strong>batch</strong></a>(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -&gt; 'List[str]'</dt><dd><tt>Default&nbsp;implementation&nbsp;runs&nbsp;invoke&nbsp;in&nbsp;parallel&nbsp;using&nbsp;a&nbsp;thread&nbsp;pool&nbsp;executor.<br>
&nbsp;<br>
The&nbsp;default&nbsp;implementation&nbsp;of&nbsp;batch&nbsp;works&nbsp;well&nbsp;for&nbsp;IO&nbsp;bound&nbsp;runnables.<br>
&nbsp;<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;batch&nbsp;more&nbsp;efficiently;<br>
e.g.,&nbsp;if&nbsp;the&nbsp;underlying&nbsp;runnable&nbsp;uses&nbsp;an&nbsp;API&nbsp;which&nbsp;supports&nbsp;a&nbsp;batch&nbsp;mode.</tt></dd></dl>

<dl><dt><a name="OpenAI-dict"><strong>dict</strong></a>(self, **kwargs: 'Any') -&gt; 'Dict'</dt><dd><tt>Return&nbsp;a&nbsp;dictionary&nbsp;of&nbsp;the&nbsp;LLM.</tt></dd></dl>

<dl><dt><a name="OpenAI-generate"><strong>generate</strong></a>(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, run_name: 'Optional[Union[str, List[str]]]' = None, **kwargs: 'Any') -&gt; 'LLMResult'</dt><dd><tt>Pass&nbsp;a&nbsp;sequence&nbsp;of&nbsp;prompts&nbsp;to&nbsp;a&nbsp;model&nbsp;and&nbsp;return&nbsp;generations.<br>
&nbsp;<br>
This&nbsp;method&nbsp;should&nbsp;make&nbsp;use&nbsp;of&nbsp;batched&nbsp;calls&nbsp;for&nbsp;models&nbsp;that&nbsp;expose&nbsp;a&nbsp;batched<br>
API.<br>
&nbsp;<br>
Use&nbsp;this&nbsp;method&nbsp;when&nbsp;you&nbsp;want&nbsp;to:<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;take&nbsp;advantage&nbsp;of&nbsp;batched&nbsp;calls,<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;need&nbsp;more&nbsp;output&nbsp;from&nbsp;the&nbsp;model&nbsp;than&nbsp;just&nbsp;the&nbsp;top&nbsp;generated&nbsp;value,<br>
&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;are&nbsp;building&nbsp;chains&nbsp;that&nbsp;are&nbsp;agnostic&nbsp;to&nbsp;the&nbsp;underlying&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;(e.g.,&nbsp;pure&nbsp;text&nbsp;completion&nbsp;models&nbsp;vs&nbsp;chat&nbsp;models).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompts:&nbsp;List&nbsp;of&nbsp;string&nbsp;prompts.<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop:&nbsp;Stop&nbsp;words&nbsp;to&nbsp;use&nbsp;when&nbsp;generating.&nbsp;Model&nbsp;output&nbsp;is&nbsp;cut&nbsp;off&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;occurrence&nbsp;of&nbsp;any&nbsp;of&nbsp;these&nbsp;substrings.<br>
&nbsp;&nbsp;&nbsp;&nbsp;callbacks:&nbsp;Callbacks&nbsp;to&nbsp;pass&nbsp;through.&nbsp;Used&nbsp;for&nbsp;executing&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality,&nbsp;such&nbsp;as&nbsp;logging&nbsp;or&nbsp;streaming,&nbsp;throughout&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Arbitrary&nbsp;additional&nbsp;keyword&nbsp;arguments.&nbsp;These&nbsp;are&nbsp;usually&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;model&nbsp;provider&nbsp;API&nbsp;call.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;LLMResult,&nbsp;which&nbsp;contains&nbsp;a&nbsp;list&nbsp;of&nbsp;candidate&nbsp;Generations&nbsp;for&nbsp;each&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;and&nbsp;additional&nbsp;model&nbsp;provider-specific&nbsp;output.</tt></dd></dl>

<dl><dt><a name="OpenAI-generate_prompt"><strong>generate_prompt</strong></a>(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -&gt; 'LLMResult'</dt><dd><tt>Pass&nbsp;a&nbsp;sequence&nbsp;of&nbsp;prompts&nbsp;to&nbsp;the&nbsp;model&nbsp;and&nbsp;return&nbsp;model&nbsp;generations.<br>
&nbsp;<br>
This&nbsp;method&nbsp;should&nbsp;make&nbsp;use&nbsp;of&nbsp;batched&nbsp;calls&nbsp;for&nbsp;models&nbsp;that&nbsp;expose&nbsp;a&nbsp;batched<br>
API.<br>
&nbsp;<br>
Use&nbsp;this&nbsp;method&nbsp;when&nbsp;you&nbsp;want&nbsp;to:<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;take&nbsp;advantage&nbsp;of&nbsp;batched&nbsp;calls,<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;need&nbsp;more&nbsp;output&nbsp;from&nbsp;the&nbsp;model&nbsp;than&nbsp;just&nbsp;the&nbsp;top&nbsp;generated&nbsp;value,<br>
&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;are&nbsp;building&nbsp;chains&nbsp;that&nbsp;are&nbsp;agnostic&nbsp;to&nbsp;the&nbsp;underlying&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;(e.g.,&nbsp;pure&nbsp;text&nbsp;completion&nbsp;models&nbsp;vs&nbsp;chat&nbsp;models).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompts:&nbsp;List&nbsp;of&nbsp;PromptValues.&nbsp;A&nbsp;PromptValue&nbsp;is&nbsp;an&nbsp;object&nbsp;that&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;converted&nbsp;to&nbsp;match&nbsp;the&nbsp;format&nbsp;of&nbsp;any&nbsp;language&nbsp;model&nbsp;(string&nbsp;for&nbsp;pure<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text&nbsp;generation&nbsp;models&nbsp;and&nbsp;BaseMessages&nbsp;for&nbsp;chat&nbsp;models).<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop:&nbsp;Stop&nbsp;words&nbsp;to&nbsp;use&nbsp;when&nbsp;generating.&nbsp;Model&nbsp;output&nbsp;is&nbsp;cut&nbsp;off&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;occurrence&nbsp;of&nbsp;any&nbsp;of&nbsp;these&nbsp;substrings.<br>
&nbsp;&nbsp;&nbsp;&nbsp;callbacks:&nbsp;Callbacks&nbsp;to&nbsp;pass&nbsp;through.&nbsp;Used&nbsp;for&nbsp;executing&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality,&nbsp;such&nbsp;as&nbsp;logging&nbsp;or&nbsp;streaming,&nbsp;throughout&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Arbitrary&nbsp;additional&nbsp;keyword&nbsp;arguments.&nbsp;These&nbsp;are&nbsp;usually&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;model&nbsp;provider&nbsp;API&nbsp;call.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;LLMResult,&nbsp;which&nbsp;contains&nbsp;a&nbsp;list&nbsp;of&nbsp;candidate&nbsp;Generations&nbsp;for&nbsp;each&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;and&nbsp;additional&nbsp;model&nbsp;provider-specific&nbsp;output.</tt></dd></dl>

<dl><dt><a name="OpenAI-invoke"><strong>invoke</strong></a>(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -&gt; 'str'</dt><dd><tt>Transform&nbsp;a&nbsp;single&nbsp;input&nbsp;into&nbsp;an&nbsp;output.&nbsp;Override&nbsp;to&nbsp;implement.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;invoking&nbsp;the&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;config&nbsp;supports&nbsp;standard&nbsp;keys&nbsp;like&nbsp;'tags',&nbsp;'metadata'&nbsp;for&nbsp;tracing<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;purposes,&nbsp;'max_concurrency'&nbsp;for&nbsp;controlling&nbsp;how&nbsp;much&nbsp;work&nbsp;to&nbsp;do<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;parallel,&nbsp;and&nbsp;other&nbsp;keys.&nbsp;Please&nbsp;refer&nbsp;to&nbsp;the&nbsp;RunnableConfig<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;more&nbsp;details.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;output&nbsp;of&nbsp;the&nbsp;runnable.</tt></dd></dl>

<dl><dt><a name="OpenAI-predict"><strong>predict</strong></a>(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'str'</dt><dd><tt>[*Deprecated*][*Deprecated*]&nbsp;<br>
&nbsp;<br>
Notes<br>
-----<br>
..&nbsp;deprecated::&nbsp;0.1.7<br>
&nbsp;&nbsp;&nbsp;Use&nbsp;invoke&nbsp;instead.</tt></dd></dl>

<dl><dt><a name="OpenAI-predict_messages"><strong>predict_messages</strong></a>(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'BaseMessage'</dt><dd><tt>[*Deprecated*][*Deprecated*]&nbsp;<br>
&nbsp;<br>
Notes<br>
-----<br>
..&nbsp;deprecated::&nbsp;0.1.7<br>
&nbsp;&nbsp;&nbsp;Use&nbsp;invoke&nbsp;instead.</tt></dd></dl>

<dl><dt><a name="OpenAI-save"><strong>save</strong></a>(self, file_path: 'Union[Path, str]') -&gt; 'None'</dt><dd><tt>Save&nbsp;the&nbsp;LLM.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;file_path:&nbsp;Path&nbsp;to&nbsp;file&nbsp;to&nbsp;save&nbsp;the&nbsp;LLM&nbsp;to.<br>
&nbsp;<br>
Example:<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;llm.<a href="#OpenAI-save">save</a>(file_path="path/llm.yaml")</tt></dd></dl>

<dl><dt><a name="OpenAI-stream"><strong>stream</strong></a>(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -&gt; 'Iterator[str]'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;stream,&nbsp;which&nbsp;calls&nbsp;invoke.<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;support&nbsp;streaming&nbsp;output.</tt></dd></dl>

<hr>
Class methods inherited from <a href="langchain_core.language_models.llms.html#BaseLLM">langchain_core.language_models.llms.BaseLLM</a>:<br>
<dl><dt><a name="OpenAI-raise_deprecation"><strong>raise_deprecation</strong></a>(values: 'Dict') -&gt; 'Dict'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Raise&nbsp;deprecation&nbsp;warning&nbsp;if&nbsp;callback_manager&nbsp;is&nbsp;used.</tt></dd></dl>

<dl><dt><a name="OpenAI-set_verbose"><strong>set_verbose</strong></a>(verbose: 'Optional[bool]') -&gt; 'bool'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>If&nbsp;verbose&nbsp;is&nbsp;None,&nbsp;set&nbsp;it.<br>
&nbsp;<br>
This&nbsp;allows&nbsp;users&nbsp;to&nbsp;pass&nbsp;in&nbsp;None&nbsp;as&nbsp;verbose&nbsp;to&nbsp;access&nbsp;the&nbsp;global&nbsp;setting.</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_core.language_models.llms.html#BaseLLM">langchain_core.language_models.llms.BaseLLM</a>:<br>
<dl><dt><strong>OutputType</strong></dt>
<dd><tt>Get&nbsp;the&nbsp;input&nbsp;type&nbsp;for&nbsp;this&nbsp;runnable.</tt></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="langchain_core.language_models.llms.html#BaseLLM">langchain_core.language_models.llms.BaseLLM</a>:<br>
<dl><dt><strong>__orig_bases__</strong> = (langchain_core.language_models.base.BaseLanguageModel[str], &lt;class 'abc.ABC'&gt;)</dl>

<hr>
Methods inherited from <a href="langchain_core.language_models.base.html#BaseLanguageModel">langchain_core.language_models.base.BaseLanguageModel</a>:<br>
<dl><dt><a name="OpenAI-get_num_tokens"><strong>get_num_tokens</strong></a>(self, text: 'str') -&gt; 'int'</dt><dd><tt>Get&nbsp;the&nbsp;number&nbsp;of&nbsp;tokens&nbsp;present&nbsp;in&nbsp;the&nbsp;text.<br>
&nbsp;<br>
Useful&nbsp;for&nbsp;checking&nbsp;if&nbsp;an&nbsp;input&nbsp;will&nbsp;fit&nbsp;in&nbsp;a&nbsp;model's&nbsp;context&nbsp;window.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;text:&nbsp;The&nbsp;string&nbsp;input&nbsp;to&nbsp;tokenize.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;integer&nbsp;number&nbsp;of&nbsp;tokens&nbsp;in&nbsp;the&nbsp;text.</tt></dd></dl>

<dl><dt><a name="OpenAI-get_num_tokens_from_messages"><strong>get_num_tokens_from_messages</strong></a>(self, messages: 'List[BaseMessage]') -&gt; 'int'</dt><dd><tt>Get&nbsp;the&nbsp;number&nbsp;of&nbsp;tokens&nbsp;in&nbsp;the&nbsp;messages.<br>
&nbsp;<br>
Useful&nbsp;for&nbsp;checking&nbsp;if&nbsp;an&nbsp;input&nbsp;will&nbsp;fit&nbsp;in&nbsp;a&nbsp;model's&nbsp;context&nbsp;window.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;messages:&nbsp;The&nbsp;message&nbsp;inputs&nbsp;to&nbsp;tokenize.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;sum&nbsp;of&nbsp;the&nbsp;number&nbsp;of&nbsp;tokens&nbsp;across&nbsp;the&nbsp;messages.</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_core.language_models.base.html#BaseLanguageModel">langchain_core.language_models.base.BaseLanguageModel</a>:<br>
<dl><dt><strong>InputType</strong></dt>
<dd><tt>Get&nbsp;the&nbsp;input&nbsp;type&nbsp;for&nbsp;this&nbsp;runnable.</tt></dd>
</dl>
<hr>
Methods inherited from <a href="langchain_core.runnables.base.html#RunnableSerializable">langchain_core.runnables.base.RunnableSerializable</a>:<br>
<dl><dt><a name="OpenAI-configurable_alternatives"><strong>configurable_alternatives</strong></a>(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -&gt; 'RunnableSerializable[Input, Output]'</dt></dl>

<dl><dt><a name="OpenAI-configurable_fields"><strong>configurable_fields</strong></a>(self, **kwargs: 'AnyConfigurableField') -&gt; 'RunnableSerializable[Input, Output]'</dt></dl>

<hr>
Data descriptors inherited from <a href="langchain_core.runnables.base.html#RunnableSerializable">langchain_core.runnables.base.RunnableSerializable</a>:<br>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<hr>
Methods inherited from <a href="langchain_core.load.serializable.html#Serializable">langchain_core.load.serializable.Serializable</a>:<br>
<dl><dt><a name="OpenAI-__init__"><strong>__init__</strong></a>(self, **kwargs: Any) -&gt; None</dt><dd><tt>Create&nbsp;a&nbsp;new&nbsp;model&nbsp;by&nbsp;parsing&nbsp;and&nbsp;validating&nbsp;input&nbsp;data&nbsp;from&nbsp;keyword&nbsp;arguments.<br>
&nbsp;<br>
Raises&nbsp;ValidationError&nbsp;if&nbsp;the&nbsp;input&nbsp;data&nbsp;cannot&nbsp;be&nbsp;parsed&nbsp;to&nbsp;form&nbsp;a&nbsp;valid&nbsp;model.</tt></dd></dl>

<dl><dt><a name="OpenAI-__repr_args__"><strong>__repr_args__</strong></a>(self) -&gt; Any</dt><dd><tt>Returns&nbsp;the&nbsp;attributes&nbsp;to&nbsp;show&nbsp;in&nbsp;__str__,&nbsp;__repr__,&nbsp;and&nbsp;__pretty__&nbsp;this&nbsp;is&nbsp;generally&nbsp;overridden.<br>
&nbsp;<br>
Can&nbsp;either&nbsp;return:<br>
*&nbsp;name&nbsp;-&nbsp;value&nbsp;pairs,&nbsp;e.g.:&nbsp;`[('foo_name',&nbsp;'foo'),&nbsp;('bar_name',&nbsp;['b',&nbsp;'a',&nbsp;'r'])]`<br>
*&nbsp;or,&nbsp;just&nbsp;values,&nbsp;e.g.:&nbsp;`[(None,&nbsp;'foo'),&nbsp;(None,&nbsp;['b',&nbsp;'a',&nbsp;'r'])]`</tt></dd></dl>

<dl><dt><a name="OpenAI-to_json"><strong>to_json</strong></a>(self) -&gt; Union[langchain_core.load.serializable.SerializedConstructor, langchain_core.load.serializable.SerializedNotImplemented]</dt></dl>

<dl><dt><a name="OpenAI-to_json_not_implemented"><strong>to_json_not_implemented</strong></a>(self) -&gt; langchain_core.load.serializable.SerializedNotImplemented</dt></dl>

<hr>
Class methods inherited from <a href="langchain_core.load.serializable.html#Serializable">langchain_core.load.serializable.Serializable</a>:<br>
<dl><dt><a name="OpenAI-lc_id"><strong>lc_id</strong></a>() -&gt; List[str]<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>A&nbsp;unique&nbsp;identifier&nbsp;for&nbsp;this&nbsp;class&nbsp;for&nbsp;serialization&nbsp;purposes.<br>
&nbsp;<br>
The&nbsp;unique&nbsp;identifier&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;strings&nbsp;that&nbsp;describes&nbsp;the&nbsp;path<br>
to&nbsp;the&nbsp;object.</tt></dd></dl>

<hr>
Methods inherited from <a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a>:<br>
<dl><dt><a name="OpenAI-__eq__"><strong>__eq__</strong></a>(self, other: Any) -&gt; bool</dt><dd><tt>Return&nbsp;self==value.</tt></dd></dl>

<dl><dt><a name="OpenAI-__getstate__"><strong>__getstate__</strong></a>(self) -&gt; 'DictAny'</dt></dl>

<dl><dt><a name="OpenAI-__iter__"><strong>__iter__</strong></a>(self) -&gt; 'TupleGenerator'</dt><dd><tt>so&nbsp;`<a href="#OpenAI-dict">dict</a>(model)`&nbsp;works</tt></dd></dl>

<dl><dt><a name="OpenAI-__setattr__"><strong>__setattr__</strong></a>(self, name, value)</dt><dd><tt>Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</tt></dd></dl>

<dl><dt><a name="OpenAI-__setstate__"><strong>__setstate__</strong></a>(self, state: 'DictAny') -&gt; None</dt></dl>

<dl><dt><a name="OpenAI-copy"><strong>copy</strong></a>(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -&gt; 'Model'</dt><dd><tt>Duplicate&nbsp;a&nbsp;model,&nbsp;optionally&nbsp;choose&nbsp;which&nbsp;fields&nbsp;to&nbsp;include,&nbsp;exclude&nbsp;and&nbsp;change.<br>
&nbsp;<br>
:param&nbsp;include:&nbsp;fields&nbsp;to&nbsp;include&nbsp;in&nbsp;new&nbsp;model<br>
:param&nbsp;exclude:&nbsp;fields&nbsp;to&nbsp;exclude&nbsp;from&nbsp;new&nbsp;model,&nbsp;as&nbsp;with&nbsp;values&nbsp;this&nbsp;takes&nbsp;precedence&nbsp;over&nbsp;include<br>
:param&nbsp;update:&nbsp;values&nbsp;to&nbsp;change/add&nbsp;in&nbsp;the&nbsp;new&nbsp;model.&nbsp;Note:&nbsp;the&nbsp;data&nbsp;is&nbsp;not&nbsp;validated&nbsp;before&nbsp;creating<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;new&nbsp;model:&nbsp;you&nbsp;should&nbsp;trust&nbsp;this&nbsp;data<br>
:param&nbsp;deep:&nbsp;set&nbsp;to&nbsp;`True`&nbsp;to&nbsp;make&nbsp;a&nbsp;deep&nbsp;copy&nbsp;of&nbsp;the&nbsp;model<br>
:return:&nbsp;new&nbsp;model&nbsp;instance</tt></dd></dl>

<dl><dt><a name="OpenAI-json"><strong>json</strong></a>(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -&gt; str</dt><dd><tt>Generate&nbsp;a&nbsp;JSON&nbsp;representation&nbsp;of&nbsp;the&nbsp;model,&nbsp;`include`&nbsp;and&nbsp;`exclude`&nbsp;arguments&nbsp;as&nbsp;per&nbsp;`<a href="#OpenAI-dict">dict</a>()`.<br>
&nbsp;<br>
`encoder`&nbsp;is&nbsp;an&nbsp;optional&nbsp;function&nbsp;to&nbsp;supply&nbsp;as&nbsp;`default`&nbsp;to&nbsp;json.dumps(),&nbsp;other&nbsp;arguments&nbsp;as&nbsp;per&nbsp;`json.dumps()`.</tt></dd></dl>

<hr>
Class methods inherited from <a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a>:<br>
<dl><dt><a name="OpenAI-__get_validators__"><strong>__get_validators__</strong></a>() -&gt; 'CallableGenerator'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAI-__try_update_forward_refs__"><strong>__try_update_forward_refs__</strong></a>(**localns: Any) -&gt; None<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Same&nbsp;as&nbsp;update_forward_refs&nbsp;but&nbsp;will&nbsp;not&nbsp;raise&nbsp;exception<br>
when&nbsp;forward&nbsp;references&nbsp;are&nbsp;not&nbsp;defined.</tt></dd></dl>

<dl><dt><a name="OpenAI-construct"><strong>construct</strong></a>(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Creates&nbsp;a&nbsp;new&nbsp;model&nbsp;setting&nbsp;__dict__&nbsp;and&nbsp;__fields_set__&nbsp;from&nbsp;trusted&nbsp;or&nbsp;pre-validated&nbsp;data.<br>
Default&nbsp;values&nbsp;are&nbsp;respected,&nbsp;but&nbsp;no&nbsp;other&nbsp;validation&nbsp;is&nbsp;performed.<br>
Behaves&nbsp;as&nbsp;if&nbsp;`Config.extra&nbsp;=&nbsp;'allow'`&nbsp;was&nbsp;set&nbsp;since&nbsp;it&nbsp;adds&nbsp;all&nbsp;passed&nbsp;values</tt></dd></dl>

<dl><dt><a name="OpenAI-from_orm"><strong>from_orm</strong></a>(obj: Any) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAI-parse_file"><strong>parse_file</strong></a>(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAI-parse_obj"><strong>parse_obj</strong></a>(obj: Any) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAI-parse_raw"><strong>parse_raw</strong></a>(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAI-schema"><strong>schema</strong></a>(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -&gt; 'DictStrAny'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAI-schema_json"><strong>schema_json</strong></a>(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -&gt; str<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAI-update_forward_refs"><strong>update_forward_refs</strong></a>(**localns: Any) -&gt; None<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Try&nbsp;to&nbsp;update&nbsp;ForwardRefs&nbsp;on&nbsp;fields&nbsp;based&nbsp;on&nbsp;this&nbsp;Model,&nbsp;globalns&nbsp;and&nbsp;localns.</tt></dd></dl>

<dl><dt><a name="OpenAI-validate"><strong>validate</strong></a>(value: Any) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<hr>
Data descriptors inherited from <a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__fields_set__</strong></dt>
</dl>
<hr>
Methods inherited from <a href="pydantic.v1.utils.html#Representation">pydantic.v1.utils.Representation</a>:<br>
<dl><dt><a name="OpenAI-__pretty__"><strong>__pretty__</strong></a>(self, fmt: Callable[[Any], Any], **kwargs: Any) -&gt; Generator[Any, NoneType, NoneType]</dt><dd><tt>Used&nbsp;by&nbsp;devtools&nbsp;(<a href="https://python-devtools.helpmanual.io/">https://python-devtools.helpmanual.io/</a>)&nbsp;to&nbsp;provide&nbsp;a&nbsp;human&nbsp;readable&nbsp;representations&nbsp;of&nbsp;objects</tt></dd></dl>

<dl><dt><a name="OpenAI-__repr__"><strong>__repr__</strong></a>(self) -&gt; str</dt><dd><tt>Return&nbsp;repr(self).</tt></dd></dl>

<dl><dt><a name="OpenAI-__repr_name__"><strong>__repr_name__</strong></a>(self) -&gt; str</dt><dd><tt>Name&nbsp;of&nbsp;the&nbsp;instance's&nbsp;class,&nbsp;used&nbsp;in&nbsp;__repr__.</tt></dd></dl>

<dl><dt><a name="OpenAI-__repr_str__"><strong>__repr_str__</strong></a>(self, join_str: str) -&gt; str</dt></dl>

<dl><dt><a name="OpenAI-__rich_repr__"><strong>__rich_repr__</strong></a>(self) -&gt; 'RichReprResult'</dt><dd><tt>Get&nbsp;fields&nbsp;for&nbsp;Rich&nbsp;library</tt></dd></dl>

<hr>
Methods inherited from <a href="langchain_core.runnables.base.html#Runnable">langchain_core.runnables.base.Runnable</a>:<br>
<dl><dt><a name="OpenAI-__or__"><strong>__or__</strong></a>(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Callable[[Iterator[Any]], Iterator[Other]], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -&gt; 'RunnableSerializable[Input, Other]'</dt><dd><tt>Compose&nbsp;this&nbsp;runnable&nbsp;with&nbsp;another&nbsp;object&nbsp;to&nbsp;create&nbsp;a&nbsp;RunnableSequence.</tt></dd></dl>

<dl><dt><a name="OpenAI-__ror__"><strong>__ror__</strong></a>(self, other: 'Union[Runnable[Other, Any], Callable[[Other], Any], Callable[[Iterator[Other]], Iterator[Any]], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -&gt; 'RunnableSerializable[Other, Output]'</dt><dd><tt>Compose&nbsp;this&nbsp;runnable&nbsp;with&nbsp;another&nbsp;object&nbsp;to&nbsp;create&nbsp;a&nbsp;RunnableSequence.</tt></dd></dl>

<dl><dt><a name="OpenAI-assign"><strong>assign</strong></a>(self, **kwargs: 'Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any], Mapping[str, Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]]]]') -&gt; 'RunnableSerializable[Any, Any]'</dt><dd><tt>Assigns&nbsp;new&nbsp;fields&nbsp;to&nbsp;the&nbsp;dict&nbsp;output&nbsp;of&nbsp;this&nbsp;runnable.<br>
Returns&nbsp;a&nbsp;new&nbsp;runnable.</tt></dd></dl>

<dl><dt><a name="OpenAI-astream_events"><strong>astream_events</strong></a>(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: "Literal['v1']", include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'AsyncIterator[StreamEvent]'</dt><dd><tt>[*Beta*]&nbsp;&nbsp;Generate&nbsp;a&nbsp;stream&nbsp;of&nbsp;events.<br>
&nbsp;<br>
Use&nbsp;to&nbsp;create&nbsp;an&nbsp;iterator&nbsp;ove&nbsp;StreamEvents&nbsp;that&nbsp;provide&nbsp;real-time&nbsp;information<br>
about&nbsp;the&nbsp;progress&nbsp;of&nbsp;the&nbsp;runnable,&nbsp;including&nbsp;StreamEvents&nbsp;from&nbsp;intermediate<br>
results.<br>
&nbsp;<br>
A&nbsp;StreamEvent&nbsp;is&nbsp;a&nbsp;dictionary&nbsp;with&nbsp;the&nbsp;following&nbsp;schema:<br>
&nbsp;<br>
*&nbsp;``event``:&nbsp;str&nbsp;-&nbsp;Event&nbsp;names&nbsp;are&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;format:&nbsp;on_[runnable_type]_(start|stream|end).<br>
*&nbsp;``name``:&nbsp;str&nbsp;-&nbsp;The&nbsp;name&nbsp;of&nbsp;the&nbsp;runnable&nbsp;that&nbsp;generated&nbsp;the&nbsp;event.<br>
*&nbsp;``run_id``:&nbsp;str&nbsp;-&nbsp;randomly&nbsp;generated&nbsp;ID&nbsp;associated&nbsp;with&nbsp;the&nbsp;given&nbsp;execution&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;runnable&nbsp;that&nbsp;emitted&nbsp;the&nbsp;event.<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;child&nbsp;runnable&nbsp;that&nbsp;gets&nbsp;invoked&nbsp;as&nbsp;part&nbsp;of&nbsp;the&nbsp;execution&nbsp;of&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;parent&nbsp;runnable&nbsp;is&nbsp;assigned&nbsp;its&nbsp;own&nbsp;unique&nbsp;ID.<br>
*&nbsp;``tags``:&nbsp;Optional[List[str]]&nbsp;-&nbsp;The&nbsp;tags&nbsp;of&nbsp;the&nbsp;runnable&nbsp;that&nbsp;generated<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;event.<br>
*&nbsp;``metadata``:&nbsp;Optional[Dict[str,&nbsp;Any]]&nbsp;-&nbsp;The&nbsp;metadata&nbsp;of&nbsp;the&nbsp;runnable<br>
&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;generated&nbsp;the&nbsp;event.<br>
*&nbsp;``data``:&nbsp;Dict[str,&nbsp;Any]<br>
&nbsp;<br>
&nbsp;<br>
Below&nbsp;is&nbsp;a&nbsp;table&nbsp;that&nbsp;illustrates&nbsp;some&nbsp;evens&nbsp;that&nbsp;might&nbsp;be&nbsp;emitted&nbsp;by&nbsp;various<br>
chains.&nbsp;Metadata&nbsp;fields&nbsp;have&nbsp;been&nbsp;omitted&nbsp;from&nbsp;the&nbsp;table&nbsp;for&nbsp;brevity.<br>
Chain&nbsp;definitions&nbsp;have&nbsp;been&nbsp;included&nbsp;after&nbsp;the&nbsp;table.<br>
&nbsp;<br>
|&nbsp;event&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;chunk&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;input&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;output&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|----------------------|------------------|---------------------------------|-----------------------------------------------|-------------------------------------------------|<br>
|&nbsp;on_chat_model_start&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"messages":&nbsp;[[SystemMessage,&nbsp;HumanMessage]]}&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_chat_model_stream&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;AIMessageChunk(content="hello")&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_chat_model_end&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"messages":&nbsp;[[SystemMessage,&nbsp;HumanMessage]]}&nbsp;|&nbsp;{"generations":&nbsp;[...],&nbsp;"llm_output":&nbsp;None,&nbsp;...}&nbsp;|<br>
|&nbsp;on_llm_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{'input':&nbsp;'hello'}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_llm_stream&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;'Hello'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_llm_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;'Hello&nbsp;human!'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_chain_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;format_docs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_chain_stream&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;format_docs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;"hello&nbsp;world!,&nbsp;goodbye&nbsp;world!"&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_chain_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;format_docs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[Document(...)]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;"hello&nbsp;world!,&nbsp;goodbye&nbsp;world!"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_tool_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;some_tool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"x":&nbsp;1,&nbsp;"y":&nbsp;"2"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_tool_stream&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;some_tool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"x":&nbsp;1,&nbsp;"y":&nbsp;"2"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_tool_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;some_tool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"x":&nbsp;1,&nbsp;"y":&nbsp;"2"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_retriever_start&nbsp;&nbsp;&nbsp;|&nbsp;[retriever&nbsp;name]&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"query":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_retriever_chunk&nbsp;&nbsp;&nbsp;|&nbsp;[retriever&nbsp;name]&nbsp;|&nbsp;{documents:&nbsp;[...]}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_retriever_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[retriever&nbsp;name]&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"query":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{documents:&nbsp;[...]}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_prompt_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[template_name]&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"question":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_prompt_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[template_name]&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"question":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;ChatPromptValue(messages:&nbsp;[SystemMessage,&nbsp;...])&nbsp;|<br>
&nbsp;<br>
Here&nbsp;are&nbsp;declarations&nbsp;associated&nbsp;with&nbsp;the&nbsp;events&nbsp;shown&nbsp;above:<br>
&nbsp;<br>
`format_docs`:<br>
&nbsp;<br>
```python<br>
def&nbsp;format_docs(docs:&nbsp;List[Document])&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;'''Format&nbsp;the&nbsp;docs.'''<br>
&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;",&nbsp;".join([doc.page_content&nbsp;for&nbsp;doc&nbsp;in&nbsp;docs])<br>
&nbsp;<br>
format_docs&nbsp;=&nbsp;RunnableLambda(format_docs)<br>
```<br>
&nbsp;<br>
`some_tool`:<br>
&nbsp;<br>
```python<br>
@tool<br>
def&nbsp;some_tool(x:&nbsp;int,&nbsp;y:&nbsp;str)&nbsp;-&gt;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;'''Some_tool.'''<br>
&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;{"x":&nbsp;x,&nbsp;"y":&nbsp;y}<br>
```<br>
&nbsp;<br>
`prompt`:<br>
&nbsp;<br>
```python<br>
template&nbsp;=&nbsp;ChatPromptTemplate.from_messages(<br>
&nbsp;&nbsp;&nbsp;&nbsp;[("system",&nbsp;"You&nbsp;are&nbsp;Cat&nbsp;Agent&nbsp;007"),&nbsp;("human",&nbsp;"{question}")]<br>
).<a href="#OpenAI-with_config">with_config</a>({"run_name":&nbsp;"my_template",&nbsp;"tags":&nbsp;["my_template"]})<br>
```<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;reverse(s:&nbsp;str)&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;s[::-1]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;RunnableLambda(func=reverse)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;events&nbsp;=&nbsp;[<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;event&nbsp;async&nbsp;for&nbsp;event&nbsp;in&nbsp;chain.<a href="#OpenAI-astream_events">astream_events</a>("hello",&nbsp;version="v1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;will&nbsp;produce&nbsp;the&nbsp;following&nbsp;events&nbsp;(run_id&nbsp;has&nbsp;been&nbsp;omitted&nbsp;for&nbsp;brevity):<br>
&nbsp;&nbsp;&nbsp;&nbsp;[<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"data":&nbsp;{"input":&nbsp;"hello"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"event":&nbsp;"on_chain_start",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata":&nbsp;{},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"reverse",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;[],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"data":&nbsp;{"chunk":&nbsp;"olleh"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"event":&nbsp;"on_chain_stream",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata":&nbsp;{},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"reverse",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;[],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"data":&nbsp;{"output":&nbsp;"olleh"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"event":&nbsp;"on_chain_end",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata":&nbsp;{},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"reverse",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;[],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;]<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;version:&nbsp;The&nbsp;version&nbsp;of&nbsp;the&nbsp;schema&nbsp;to&nbsp;use.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Currently&nbsp;only&nbsp;version&nbsp;1&nbsp;is&nbsp;available.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;No&nbsp;default&nbsp;will&nbsp;be&nbsp;assigned&nbsp;until&nbsp;the&nbsp;API&nbsp;is&nbsp;stabilized.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_names:&nbsp;Only&nbsp;include&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_types:&nbsp;Only&nbsp;include&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_tags:&nbsp;Only&nbsp;include&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_names:&nbsp;Exclude&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_types:&nbsp;Exclude&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_tags:&nbsp;Exclude&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;These&nbsp;will&nbsp;be&nbsp;passed&nbsp;to&nbsp;astream_log&nbsp;as&nbsp;this&nbsp;implementation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;astream_events&nbsp;is&nbsp;built&nbsp;on&nbsp;top&nbsp;of&nbsp;astream_log.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;async&nbsp;stream&nbsp;of&nbsp;StreamEvents.[*Beta*]&nbsp;Generate&nbsp;a&nbsp;stream&nbsp;of&nbsp;events.<br>
&nbsp;<br>
Use&nbsp;to&nbsp;create&nbsp;an&nbsp;iterator&nbsp;ove&nbsp;StreamEvents&nbsp;that&nbsp;provide&nbsp;real-time&nbsp;information<br>
about&nbsp;the&nbsp;progress&nbsp;of&nbsp;the&nbsp;runnable,&nbsp;including&nbsp;StreamEvents&nbsp;from&nbsp;intermediate<br>
results.<br>
&nbsp;<br>
A&nbsp;StreamEvent&nbsp;is&nbsp;a&nbsp;dictionary&nbsp;with&nbsp;the&nbsp;following&nbsp;schema:<br>
&nbsp;<br>
*&nbsp;``event``:&nbsp;str&nbsp;-&nbsp;Event&nbsp;names&nbsp;are&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;format:&nbsp;on_[runnable_type]_(start|stream|end).<br>
*&nbsp;``name``:&nbsp;str&nbsp;-&nbsp;The&nbsp;name&nbsp;of&nbsp;the&nbsp;runnable&nbsp;that&nbsp;generated&nbsp;the&nbsp;event.<br>
*&nbsp;``run_id``:&nbsp;str&nbsp;-&nbsp;randomly&nbsp;generated&nbsp;ID&nbsp;associated&nbsp;with&nbsp;the&nbsp;given&nbsp;execution&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;runnable&nbsp;that&nbsp;emitted&nbsp;the&nbsp;event.<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;child&nbsp;runnable&nbsp;that&nbsp;gets&nbsp;invoked&nbsp;as&nbsp;part&nbsp;of&nbsp;the&nbsp;execution&nbsp;of&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;parent&nbsp;runnable&nbsp;is&nbsp;assigned&nbsp;its&nbsp;own&nbsp;unique&nbsp;ID.<br>
*&nbsp;``tags``:&nbsp;Optional[List[str]]&nbsp;-&nbsp;The&nbsp;tags&nbsp;of&nbsp;the&nbsp;runnable&nbsp;that&nbsp;generated<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;event.<br>
*&nbsp;``metadata``:&nbsp;Optional[Dict[str,&nbsp;Any]]&nbsp;-&nbsp;The&nbsp;metadata&nbsp;of&nbsp;the&nbsp;runnable<br>
&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;generated&nbsp;the&nbsp;event.<br>
*&nbsp;``data``:&nbsp;Dict[str,&nbsp;Any]<br>
&nbsp;<br>
&nbsp;<br>
Below&nbsp;is&nbsp;a&nbsp;table&nbsp;that&nbsp;illustrates&nbsp;some&nbsp;evens&nbsp;that&nbsp;might&nbsp;be&nbsp;emitted&nbsp;by&nbsp;various<br>
chains.&nbsp;Metadata&nbsp;fields&nbsp;have&nbsp;been&nbsp;omitted&nbsp;from&nbsp;the&nbsp;table&nbsp;for&nbsp;brevity.<br>
Chain&nbsp;definitions&nbsp;have&nbsp;been&nbsp;included&nbsp;after&nbsp;the&nbsp;table.<br>
&nbsp;<br>
|&nbsp;event&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;chunk&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;input&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;output&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|----------------------|------------------|---------------------------------|-----------------------------------------------|-------------------------------------------------|<br>
|&nbsp;on_chat_model_start&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"messages":&nbsp;[[SystemMessage,&nbsp;HumanMessage]]}&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_chat_model_stream&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;AIMessageChunk(content="hello")&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_chat_model_end&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"messages":&nbsp;[[SystemMessage,&nbsp;HumanMessage]]}&nbsp;|&nbsp;{"generations":&nbsp;[...],&nbsp;"llm_output":&nbsp;None,&nbsp;...}&nbsp;|<br>
|&nbsp;on_llm_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{'input':&nbsp;'hello'}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_llm_stream&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;'Hello'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_llm_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;'Hello&nbsp;human!'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_chain_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;format_docs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_chain_stream&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;format_docs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;"hello&nbsp;world!,&nbsp;goodbye&nbsp;world!"&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_chain_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;format_docs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[Document(...)]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;"hello&nbsp;world!,&nbsp;goodbye&nbsp;world!"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_tool_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;some_tool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"x":&nbsp;1,&nbsp;"y":&nbsp;"2"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_tool_stream&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;some_tool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"x":&nbsp;1,&nbsp;"y":&nbsp;"2"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_tool_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;some_tool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"x":&nbsp;1,&nbsp;"y":&nbsp;"2"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_retriever_start&nbsp;&nbsp;&nbsp;|&nbsp;[retriever&nbsp;name]&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"query":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_retriever_chunk&nbsp;&nbsp;&nbsp;|&nbsp;[retriever&nbsp;name]&nbsp;|&nbsp;{documents:&nbsp;[...]}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_retriever_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[retriever&nbsp;name]&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"query":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{documents:&nbsp;[...]}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_prompt_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[template_name]&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"question":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
|&nbsp;on_prompt_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[template_name]&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"question":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;ChatPromptValue(messages:&nbsp;[SystemMessage,&nbsp;...])&nbsp;|<br>
&nbsp;<br>
Here&nbsp;are&nbsp;declarations&nbsp;associated&nbsp;with&nbsp;the&nbsp;events&nbsp;shown&nbsp;above:<br>
&nbsp;<br>
`format_docs`:<br>
&nbsp;<br>
```python<br>
def&nbsp;format_docs(docs:&nbsp;List[Document])&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;'''Format&nbsp;the&nbsp;docs.'''<br>
&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;",&nbsp;".join([doc.page_content&nbsp;for&nbsp;doc&nbsp;in&nbsp;docs])<br>
&nbsp;<br>
format_docs&nbsp;=&nbsp;RunnableLambda(format_docs)<br>
```<br>
&nbsp;<br>
`some_tool`:<br>
&nbsp;<br>
```python<br>
@tool<br>
def&nbsp;some_tool(x:&nbsp;int,&nbsp;y:&nbsp;str)&nbsp;-&gt;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;'''Some_tool.'''<br>
&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;{"x":&nbsp;x,&nbsp;"y":&nbsp;y}<br>
```<br>
&nbsp;<br>
`prompt`:<br>
&nbsp;<br>
```python<br>
template&nbsp;=&nbsp;ChatPromptTemplate.from_messages(<br>
&nbsp;&nbsp;&nbsp;&nbsp;[("system",&nbsp;"You&nbsp;are&nbsp;Cat&nbsp;Agent&nbsp;007"),&nbsp;("human",&nbsp;"{question}")]<br>
).<a href="#OpenAI-with_config">with_config</a>({"run_name":&nbsp;"my_template",&nbsp;"tags":&nbsp;["my_template"]})<br>
```<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;reverse(s:&nbsp;str)&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;s[::-1]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;RunnableLambda(func=reverse)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;events&nbsp;=&nbsp;[<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;event&nbsp;async&nbsp;for&nbsp;event&nbsp;in&nbsp;chain.<a href="#OpenAI-astream_events">astream_events</a>("hello",&nbsp;version="v1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;will&nbsp;produce&nbsp;the&nbsp;following&nbsp;events&nbsp;(run_id&nbsp;has&nbsp;been&nbsp;omitted&nbsp;for&nbsp;brevity):<br>
&nbsp;&nbsp;&nbsp;&nbsp;[<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"data":&nbsp;{"input":&nbsp;"hello"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"event":&nbsp;"on_chain_start",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata":&nbsp;{},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"reverse",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;[],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"data":&nbsp;{"chunk":&nbsp;"olleh"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"event":&nbsp;"on_chain_stream",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata":&nbsp;{},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"reverse",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;[],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"data":&nbsp;{"output":&nbsp;"olleh"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"event":&nbsp;"on_chain_end",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata":&nbsp;{},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"reverse",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;[],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;]<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;version:&nbsp;The&nbsp;version&nbsp;of&nbsp;the&nbsp;schema&nbsp;to&nbsp;use.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Currently&nbsp;only&nbsp;version&nbsp;1&nbsp;is&nbsp;available.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;No&nbsp;default&nbsp;will&nbsp;be&nbsp;assigned&nbsp;until&nbsp;the&nbsp;API&nbsp;is&nbsp;stabilized.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_names:&nbsp;Only&nbsp;include&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_types:&nbsp;Only&nbsp;include&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_tags:&nbsp;Only&nbsp;include&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_names:&nbsp;Exclude&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_types:&nbsp;Exclude&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_tags:&nbsp;Exclude&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;These&nbsp;will&nbsp;be&nbsp;passed&nbsp;to&nbsp;astream_log&nbsp;as&nbsp;this&nbsp;implementation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;astream_events&nbsp;is&nbsp;built&nbsp;on&nbsp;top&nbsp;of&nbsp;astream_log.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;async&nbsp;stream&nbsp;of&nbsp;StreamEvents.<br>
&nbsp;<br>
Notes<br>
-----<br>
..&nbsp;beta::<br>
&nbsp;&nbsp;&nbsp;This&nbsp;API&nbsp;is&nbsp;in&nbsp;beta&nbsp;and&nbsp;may&nbsp;change&nbsp;in&nbsp;the&nbsp;future.</tt></dd></dl>

<dl><dt>async <a name="OpenAI-astream_log"><strong>astream_log</strong></a>(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'</dt><dd><tt>Stream&nbsp;all&nbsp;output&nbsp;from&nbsp;a&nbsp;runnable,&nbsp;as&nbsp;reported&nbsp;to&nbsp;the&nbsp;callback&nbsp;system.<br>
This&nbsp;includes&nbsp;all&nbsp;inner&nbsp;runs&nbsp;of&nbsp;LLMs,&nbsp;Retrievers,&nbsp;Tools,&nbsp;etc.<br>
&nbsp;<br>
Output&nbsp;is&nbsp;streamed&nbsp;as&nbsp;Log&nbsp;objects,&nbsp;which&nbsp;include&nbsp;a&nbsp;list&nbsp;of<br>
jsonpatch&nbsp;ops&nbsp;that&nbsp;describe&nbsp;how&nbsp;the&nbsp;state&nbsp;of&nbsp;the&nbsp;run&nbsp;has&nbsp;changed&nbsp;in&nbsp;each<br>
step,&nbsp;and&nbsp;the&nbsp;final&nbsp;state&nbsp;of&nbsp;the&nbsp;run.<br>
&nbsp;<br>
The&nbsp;jsonpatch&nbsp;ops&nbsp;can&nbsp;be&nbsp;applied&nbsp;in&nbsp;order&nbsp;to&nbsp;construct&nbsp;state.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;diff:&nbsp;Whether&nbsp;to&nbsp;yield&nbsp;diffs&nbsp;between&nbsp;each&nbsp;step,&nbsp;or&nbsp;the&nbsp;current&nbsp;state.<br>
&nbsp;&nbsp;&nbsp;&nbsp;with_streamed_output_list:&nbsp;Whether&nbsp;to&nbsp;yield&nbsp;the&nbsp;streamed_output&nbsp;list.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_names:&nbsp;Only&nbsp;include&nbsp;logs&nbsp;with&nbsp;these&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_types:&nbsp;Only&nbsp;include&nbsp;logs&nbsp;with&nbsp;these&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_tags:&nbsp;Only&nbsp;include&nbsp;logs&nbsp;with&nbsp;these&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_names:&nbsp;Exclude&nbsp;logs&nbsp;with&nbsp;these&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_types:&nbsp;Exclude&nbsp;logs&nbsp;with&nbsp;these&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_tags:&nbsp;Exclude&nbsp;logs&nbsp;with&nbsp;these&nbsp;tags.</tt></dd></dl>

<dl><dt>async <a name="OpenAI-atransform"><strong>atransform</strong></a>(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -&gt; 'AsyncIterator[Output]'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;atransform,&nbsp;which&nbsp;buffers&nbsp;input&nbsp;and&nbsp;calls&nbsp;astream.<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;start&nbsp;producing&nbsp;output&nbsp;while<br>
input&nbsp;is&nbsp;still&nbsp;being&nbsp;generated.</tt></dd></dl>

<dl><dt><a name="OpenAI-bind"><strong>bind</strong></a>(self, **kwargs: 'Any') -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;arguments&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="OpenAI-config_schema"><strong>config_schema</strong></a>(self, *, include: 'Optional[Sequence[str]]' = None) -&gt; 'Type[BaseModel]'</dt><dd><tt>The&nbsp;type&nbsp;of&nbsp;config&nbsp;this&nbsp;runnable&nbsp;accepts&nbsp;specified&nbsp;as&nbsp;a&nbsp;pydantic&nbsp;model.<br>
&nbsp;<br>
To&nbsp;mark&nbsp;a&nbsp;field&nbsp;as&nbsp;configurable,&nbsp;see&nbsp;the&nbsp;`configurable_fields`<br>
and&nbsp;`configurable_alternatives`&nbsp;methods.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;include:&nbsp;A&nbsp;list&nbsp;of&nbsp;fields&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;config&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;config.</tt></dd></dl>

<dl><dt><a name="OpenAI-get_graph"><strong>get_graph</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'Graph'</dt><dd><tt>Return&nbsp;a&nbsp;graph&nbsp;representation&nbsp;of&nbsp;this&nbsp;runnable.</tt></dd></dl>

<dl><dt><a name="OpenAI-get_input_schema"><strong>get_input_schema</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'Type[BaseModel]'</dt><dd><tt>Get&nbsp;a&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;input&nbsp;to&nbsp;the&nbsp;runnable.<br>
&nbsp;<br>
Runnables&nbsp;that&nbsp;leverage&nbsp;the&nbsp;configurable_fields&nbsp;and&nbsp;configurable_alternatives<br>
methods&nbsp;will&nbsp;have&nbsp;a&nbsp;dynamic&nbsp;input&nbsp;schema&nbsp;that&nbsp;depends&nbsp;on&nbsp;which<br>
configuration&nbsp;the&nbsp;runnable&nbsp;is&nbsp;invoked&nbsp;with.<br>
&nbsp;<br>
This&nbsp;method&nbsp;allows&nbsp;to&nbsp;get&nbsp;an&nbsp;input&nbsp;schema&nbsp;for&nbsp;a&nbsp;specific&nbsp;configuration.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;generating&nbsp;the&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;input.</tt></dd></dl>

<dl><dt><a name="OpenAI-get_name"><strong>get_name</strong></a>(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -&gt; 'str'</dt><dd><tt>Get&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;runnable.</tt></dd></dl>

<dl><dt><a name="OpenAI-get_output_schema"><strong>get_output_schema</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'Type[BaseModel]'</dt><dd><tt>Get&nbsp;a&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;output&nbsp;to&nbsp;the&nbsp;runnable.<br>
&nbsp;<br>
Runnables&nbsp;that&nbsp;leverage&nbsp;the&nbsp;configurable_fields&nbsp;and&nbsp;configurable_alternatives<br>
methods&nbsp;will&nbsp;have&nbsp;a&nbsp;dynamic&nbsp;output&nbsp;schema&nbsp;that&nbsp;depends&nbsp;on&nbsp;which<br>
configuration&nbsp;the&nbsp;runnable&nbsp;is&nbsp;invoked&nbsp;with.<br>
&nbsp;<br>
This&nbsp;method&nbsp;allows&nbsp;to&nbsp;get&nbsp;an&nbsp;output&nbsp;schema&nbsp;for&nbsp;a&nbsp;specific&nbsp;configuration.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;generating&nbsp;the&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;output.</tt></dd></dl>

<dl><dt><a name="OpenAI-get_prompts"><strong>get_prompts</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'List[BasePromptTemplate]'</dt></dl>

<dl><dt><a name="OpenAI-map"><strong>map</strong></a>(self) -&gt; 'Runnable[List[Input], List[Output]]'</dt><dd><tt>Return&nbsp;a&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;maps&nbsp;a&nbsp;list&nbsp;of&nbsp;inputs&nbsp;to&nbsp;a&nbsp;list&nbsp;of&nbsp;outputs,<br>
by&nbsp;calling&nbsp;<a href="#OpenAI-invoke">invoke</a>()&nbsp;with&nbsp;each&nbsp;input.</tt></dd></dl>

<dl><dt><a name="OpenAI-pick"><strong>pick</strong></a>(self, keys: 'Union[str, List[str]]') -&gt; 'RunnableSerializable[Any, Any]'</dt><dd><tt>Pick&nbsp;keys&nbsp;from&nbsp;the&nbsp;dict&nbsp;output&nbsp;of&nbsp;this&nbsp;runnable.<br>
Returns&nbsp;a&nbsp;new&nbsp;runnable.</tt></dd></dl>

<dl><dt><a name="OpenAI-pipe"><strong>pipe</strong></a>(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -&gt; 'RunnableSerializable[Input, Other]'</dt><dd><tt>Compose&nbsp;this&nbsp;runnable&nbsp;with&nbsp;another&nbsp;object&nbsp;to&nbsp;create&nbsp;a&nbsp;RunnableSequence.</tt></dd></dl>

<dl><dt><a name="OpenAI-transform"><strong>transform</strong></a>(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -&gt; 'Iterator[Output]'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;transform,&nbsp;which&nbsp;buffers&nbsp;input&nbsp;and&nbsp;then&nbsp;calls&nbsp;stream.<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;start&nbsp;producing&nbsp;output&nbsp;while<br>
input&nbsp;is&nbsp;still&nbsp;being&nbsp;generated.</tt></dd></dl>

<dl><dt><a name="OpenAI-with_config"><strong>with_config</strong></a>(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;config&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="OpenAI-with_fallbacks"><strong>with_fallbacks</strong></a>(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (&lt;class 'Exception'&gt;,), exception_key: 'Optional[str]' = None) -&gt; 'RunnableWithFallbacksT[Input, Output]'</dt><dd><tt>Add&nbsp;fallbacks&nbsp;to&nbsp;a&nbsp;runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fallbacks:&nbsp;A&nbsp;sequence&nbsp;of&nbsp;runnables&nbsp;to&nbsp;try&nbsp;if&nbsp;the&nbsp;original&nbsp;runnable&nbsp;fails.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exceptions_to_handle:&nbsp;A&nbsp;tuple&nbsp;of&nbsp;exception&nbsp;types&nbsp;to&nbsp;handle.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exception_key:&nbsp;If&nbsp;string&nbsp;is&nbsp;specified&nbsp;then&nbsp;handled&nbsp;exceptions&nbsp;will&nbsp;be&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;fallbacks&nbsp;as&nbsp;part&nbsp;of&nbsp;the&nbsp;input&nbsp;under&nbsp;the&nbsp;specified&nbsp;key.&nbsp;If&nbsp;None,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;exceptions&nbsp;will&nbsp;not&nbsp;be&nbsp;passed&nbsp;to&nbsp;fallbacks.&nbsp;If&nbsp;used,&nbsp;the&nbsp;base&nbsp;runnable<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;its&nbsp;fallbacks&nbsp;must&nbsp;accept&nbsp;a&nbsp;dictionary&nbsp;as&nbsp;input.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;will&nbsp;try&nbsp;the&nbsp;original&nbsp;runnable,&nbsp;and&nbsp;then&nbsp;each<br>
&nbsp;&nbsp;&nbsp;&nbsp;fallback&nbsp;in&nbsp;order,&nbsp;upon&nbsp;failures.</tt></dd></dl>

<dl><dt><a name="OpenAI-with_listeners"><strong>with_listeners</strong></a>(self, *, on_start: 'Optional[Listener]' = None, on_end: 'Optional[Listener]' = None, on_error: 'Optional[Listener]' = None) -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;lifecycle&nbsp;listeners&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
on_start:&nbsp;Called&nbsp;before&nbsp;the&nbsp;runnable&nbsp;starts&nbsp;running,&nbsp;with&nbsp;the&nbsp;Run&nbsp;object.<br>
on_end:&nbsp;Called&nbsp;after&nbsp;the&nbsp;runnable&nbsp;finishes&nbsp;running,&nbsp;with&nbsp;the&nbsp;Run&nbsp;object.<br>
on_error:&nbsp;Called&nbsp;if&nbsp;the&nbsp;runnable&nbsp;throws&nbsp;an&nbsp;error,&nbsp;with&nbsp;the&nbsp;Run&nbsp;object.<br>
&nbsp;<br>
The&nbsp;Run&nbsp;object&nbsp;contains&nbsp;information&nbsp;about&nbsp;the&nbsp;run,&nbsp;including&nbsp;its&nbsp;id,<br>
type,&nbsp;input,&nbsp;output,&nbsp;error,&nbsp;start_time,&nbsp;end_time,&nbsp;and&nbsp;any&nbsp;tags&nbsp;or&nbsp;metadata<br>
added&nbsp;to&nbsp;the&nbsp;run.</tt></dd></dl>

<dl><dt><a name="OpenAI-with_retry"><strong>with_retry</strong></a>(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (&lt;class 'Exception'&gt;,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Create&nbsp;a&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;retries&nbsp;the&nbsp;original&nbsp;runnable&nbsp;on&nbsp;exceptions.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;retry_if_exception_type:&nbsp;A&nbsp;tuple&nbsp;of&nbsp;exception&nbsp;types&nbsp;to&nbsp;retry&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;wait_exponential_jitter:&nbsp;Whether&nbsp;to&nbsp;add&nbsp;jitter&nbsp;to&nbsp;the&nbsp;wait&nbsp;time<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;between&nbsp;retries<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop_after_attempt:&nbsp;The&nbsp;maximum&nbsp;number&nbsp;of&nbsp;attempts&nbsp;to&nbsp;make&nbsp;before&nbsp;giving&nbsp;up<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;retries&nbsp;the&nbsp;original&nbsp;runnable&nbsp;on&nbsp;exceptions.</tt></dd></dl>

<dl><dt><a name="OpenAI-with_types"><strong>with_types</strong></a>(self, *, input_type: 'Optional[Type[Input]]' = None, output_type: 'Optional[Type[Output]]' = None) -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;input&nbsp;and&nbsp;output&nbsp;types&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_core.runnables.base.html#Runnable">langchain_core.runnables.base.Runnable</a>:<br>
<dl><dt><strong>config_specs</strong></dt>
<dd><tt>List&nbsp;configurable&nbsp;fields&nbsp;for&nbsp;this&nbsp;runnable.</tt></dd>
</dl>
<dl><dt><strong>input_schema</strong></dt>
<dd><tt>The&nbsp;type&nbsp;of&nbsp;input&nbsp;this&nbsp;runnable&nbsp;accepts&nbsp;specified&nbsp;as&nbsp;a&nbsp;pydantic&nbsp;model.</tt></dd>
</dl>
<dl><dt><strong>output_schema</strong></dt>
<dd><tt>The&nbsp;type&nbsp;of&nbsp;output&nbsp;this&nbsp;runnable&nbsp;produces&nbsp;specified&nbsp;as&nbsp;a&nbsp;pydantic&nbsp;model.</tt></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="langchain_core.runnables.base.html#Runnable">langchain_core.runnables.base.Runnable</a>:<br>
<dl><dt><strong>name</strong> = None</dl>

<hr>
Class methods inherited from <a href="typing.html#Generic">typing.Generic</a>:<br>
<dl><dt><a name="OpenAI-__class_getitem__"><strong>__class_getitem__</strong></a>(params)<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAI-__init_subclass__"><strong>__init_subclass__</strong></a>(*args, **kwargs)<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>This&nbsp;method&nbsp;is&nbsp;called&nbsp;when&nbsp;a&nbsp;class&nbsp;is&nbsp;subclassed.<br>
&nbsp;<br>
The&nbsp;default&nbsp;implementation&nbsp;does&nbsp;nothing.&nbsp;It&nbsp;may&nbsp;be<br>
overridden&nbsp;to&nbsp;extend&nbsp;subclasses.</tt></dd></dl>

</td></tr></table> <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="OpenAIEmbeddings">class <strong>OpenAIEmbeddings</strong></a>(<a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a>, <a href="langchain_openai.embeddings.base.html#OpenAIEmbeddings">langchain_openai.embeddings.base.OpenAIEmbeddings</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt><a href="#OpenAIEmbeddings">OpenAIEmbeddings</a>(*args,&nbsp;client:&nbsp;Any&nbsp;=&nbsp;None,&nbsp;async_client:&nbsp;Any&nbsp;=&nbsp;None,&nbsp;model:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;dimensions:&nbsp;Optional[int]&nbsp;=&nbsp;None,&nbsp;deployment:&nbsp;Optional[str]&nbsp;=&nbsp;'text-embedding-ada-002',&nbsp;api_version:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;base_url:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;openai_api_type:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;openai_proxy:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;embedding_ctx_length:&nbsp;int&nbsp;=&nbsp;8191,&nbsp;api_key:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;organization:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;allowed_special:&nbsp;Union[Literal['all'],&nbsp;Set[str]]&nbsp;=&nbsp;set(),&nbsp;disallowed_special:&nbsp;Union[Literal['all'],&nbsp;Set[str],&nbsp;Sequence[str]]&nbsp;=&nbsp;'all',&nbsp;chunk_size:&nbsp;int&nbsp;=&nbsp;16,&nbsp;max_retries:&nbsp;int&nbsp;=&nbsp;2,&nbsp;timeout:&nbsp;Union[float,&nbsp;Tuple[float,&nbsp;float],&nbsp;Any,&nbsp;NoneType]&nbsp;=&nbsp;None,&nbsp;headers:&nbsp;Any&nbsp;=&nbsp;None,&nbsp;tiktoken_enabled:&nbsp;bool&nbsp;=&nbsp;True,&nbsp;tiktoken_model_name:&nbsp;Optional[str]&nbsp;=&nbsp;'text-embedding-ada-002',&nbsp;show_progress_bar:&nbsp;bool&nbsp;=&nbsp;False,&nbsp;model_kwargs:&nbsp;Dict[str,&nbsp;Any]&nbsp;=&nbsp;None,&nbsp;skip_empty:&nbsp;bool&nbsp;=&nbsp;False,&nbsp;default_headers:&nbsp;Optional[Mapping[str,&nbsp;str]]&nbsp;=&nbsp;None,&nbsp;default_query:&nbsp;Optional[Mapping[str,&nbsp;object]]&nbsp;=&nbsp;None,&nbsp;retry_min_seconds:&nbsp;int&nbsp;=&nbsp;4,&nbsp;retry_max_seconds:&nbsp;int&nbsp;=&nbsp;20,&nbsp;http_client:&nbsp;Optional[Any]&nbsp;=&nbsp;None,&nbsp;proxy_client:&nbsp;Optional[Any]&nbsp;=&nbsp;None,&nbsp;deployment_id:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;config_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;config_id:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;proxy_model_name:&nbsp;Optional[str]&nbsp;=&nbsp;None)&nbsp;-&amp;gt;&nbsp;None<br>
&nbsp;<br>
<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%"><dl><dt>Method resolution order:</dt>
<dd><a href="gen_ai_hub.proxy.langchain.openai.html#OpenAIEmbeddings">OpenAIEmbeddings</a></dd>
<dd><a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a></dd>
<dd><a href="gen_ai_hub.proxy.langchain.base.html#BaseAuth">gen_ai_hub.proxy.langchain.base.BaseAuth</a></dd>
<dd><a href="langchain_openai.embeddings.base.html#OpenAIEmbeddings">langchain_openai.embeddings.base.OpenAIEmbeddings</a></dd>
<dd><a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a></dd>
<dd><a href="pydantic.v1.utils.html#Representation">pydantic.v1.utils.Representation</a></dd>
<dd><a href="langchain_core.embeddings.html#Embeddings">langchain_core.embeddings.Embeddings</a></dd>
<dd><a href="abc.html#ABC">abc.ABC</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="OpenAIEmbeddings-__init__"><strong>__init__</strong></a>(self, *args, **kwargs)</dt><dd><tt>Create&nbsp;a&nbsp;new&nbsp;model&nbsp;by&nbsp;parsing&nbsp;and&nbsp;validating&nbsp;input&nbsp;data&nbsp;from&nbsp;keyword&nbsp;arguments.<br>
&nbsp;<br>
Raises&nbsp;ValidationError&nbsp;if&nbsp;the&nbsp;input&nbsp;data&nbsp;cannot&nbsp;be&nbsp;parsed&nbsp;to&nbsp;form&nbsp;a&nbsp;valid&nbsp;model.</tt></dd></dl>

<hr>
Class methods defined here:<br>
<dl><dt><a name="OpenAIEmbeddings-validate_environment"><strong>validate_environment</strong></a>(values: Dict) -&gt; Dict<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Validates&nbsp;the&nbsp;environment.<br>
&nbsp;<br>
:param&nbsp;values:&nbsp;The&nbsp;input&nbsp;values<br>
:type&nbsp;values:&nbsp;Dict<br>
:return:&nbsp;The&nbsp;validated&nbsp;values<br>
:rtype:&nbsp;Dict</tt></dd></dl>

<hr>
Static methods defined here:<br>
<dl><dt><a name="OpenAIEmbeddings-__json_encoder__"><strong>__json_encoder__</strong></a> = pydantic_encoder(obj: Any) -&gt; Any</dt></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>__abstractmethods__</strong> = frozenset()</dl>

<dl><dt><strong>__annotations__</strong> = {'chunk_size': &lt;class 'int'&gt;, 'model': typing.Optional[str], 'openai_api_version': typing.Optional[str], 'tiktoken_model_name': typing.Optional[str]}</dl>

<dl><dt><strong>__class_vars__</strong> = set()</dl>

<dl><dt><strong>__config__</strong> = &lt;class 'pydantic.v1.config.Config'&gt;</dl>

<dl><dt><strong>__custom_root_type__</strong> = False</dl>

<dl><dt><strong>__exclude_fields__</strong> = {'async_client': True, 'client': True}</dl>

<dl><dt><strong>__fields__</strong> = {'allowed_special': ModelField(name='allowed_special', type=Union[Li...'all'], Set[str]], required=False, default=set()), 'async_client': ModelField(name='async_client', type=Optional[Any], required=False, default=None), 'chunk_size': ModelField(name='chunk_size', type=int, required=False, default=16), 'client': ModelField(name='client', type=Optional[Any], required=False, default=None), 'config_id': ModelField(name='config_id', type=Optional[str], required=False, default=None), 'config_name': ModelField(name='config_name', type=Optional[str], required=False, default=None), 'default_headers': ModelField(name='default_headers', type=Optional[Mapping[str, str]], required=False, default=None), 'default_query': ModelField(name='default_query', type=Optional[Mapping[str, object]], required=False, default=None), 'deployment': ModelField(name='deployment', type=Optional[str], required=False, default='text-embedding-ada-002'), 'deployment_id': ModelField(name='deployment_id', type=Optional[str], required=False, default=None), ...}</dl>

<dl><dt><strong>__hash__</strong> = None</dl>

<dl><dt><strong>__include_fields__</strong> = None</dl>

<dl><dt><strong>__post_root_validators__</strong> = [(False, &lt;function OpenAIEmbeddings.validate_environment&gt;)]</dl>

<dl><dt><strong>__pre_root_validators__</strong> = [&lt;function OpenAIEmbeddings.build_extra&gt;]</dl>

<dl><dt><strong>__private_attributes__</strong> = {}</dl>

<dl><dt><strong>__schema_cache__</strong> = {}</dl>

<dl><dt><strong>__signature__</strong> = &lt;Signature (*args, client: Any = None, async_cli... proxy_model_name: Optional[str] = None) -&gt; None&gt;</dl>

<dl><dt><strong>__validators__</strong> = {}</dl>

<hr>
Class methods inherited from <a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a>:<br>
<dl><dt><a name="OpenAIEmbeddings-validate_clients"><strong>validate_clients</strong></a>(values: Dict) -&gt; Dict<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<hr>
Methods inherited from <a href="langchain_openai.embeddings.base.html#OpenAIEmbeddings">langchain_openai.embeddings.base.OpenAIEmbeddings</a>:<br>
<dl><dt>async <a name="OpenAIEmbeddings-aembed_documents"><strong>aembed_documents</strong></a>(self, texts: 'List[str]', chunk_size: 'Optional[int]' = 0) -&gt; 'List[List[float]]'</dt><dd><tt>Call&nbsp;out&nbsp;to&nbsp;<a href="#OpenAI">OpenAI</a>'s&nbsp;embedding&nbsp;endpoint&nbsp;async&nbsp;for&nbsp;embedding&nbsp;search&nbsp;docs.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;texts:&nbsp;The&nbsp;list&nbsp;of&nbsp;texts&nbsp;to&nbsp;embed.<br>
&nbsp;&nbsp;&nbsp;&nbsp;chunk_size:&nbsp;The&nbsp;chunk&nbsp;size&nbsp;of&nbsp;embeddings.&nbsp;If&nbsp;None,&nbsp;will&nbsp;use&nbsp;the&nbsp;chunk&nbsp;size<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;specified&nbsp;by&nbsp;the&nbsp;class.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;List&nbsp;of&nbsp;embeddings,&nbsp;one&nbsp;for&nbsp;each&nbsp;text.</tt></dd></dl>

<dl><dt>async <a name="OpenAIEmbeddings-aembed_query"><strong>aembed_query</strong></a>(self, text: 'str') -&gt; 'List[float]'</dt><dd><tt>Call&nbsp;out&nbsp;to&nbsp;<a href="#OpenAI">OpenAI</a>'s&nbsp;embedding&nbsp;endpoint&nbsp;async&nbsp;for&nbsp;embedding&nbsp;query&nbsp;text.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;text:&nbsp;The&nbsp;text&nbsp;to&nbsp;embed.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Embedding&nbsp;for&nbsp;the&nbsp;text.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-embed_documents"><strong>embed_documents</strong></a>(self, texts: 'List[str]', chunk_size: 'Optional[int]' = 0) -&gt; 'List[List[float]]'</dt><dd><tt>Call&nbsp;out&nbsp;to&nbsp;<a href="#OpenAI">OpenAI</a>'s&nbsp;embedding&nbsp;endpoint&nbsp;for&nbsp;embedding&nbsp;search&nbsp;docs.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;texts:&nbsp;The&nbsp;list&nbsp;of&nbsp;texts&nbsp;to&nbsp;embed.<br>
&nbsp;&nbsp;&nbsp;&nbsp;chunk_size:&nbsp;The&nbsp;chunk&nbsp;size&nbsp;of&nbsp;embeddings.&nbsp;If&nbsp;None,&nbsp;will&nbsp;use&nbsp;the&nbsp;chunk&nbsp;size<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;specified&nbsp;by&nbsp;the&nbsp;class.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;List&nbsp;of&nbsp;embeddings,&nbsp;one&nbsp;for&nbsp;each&nbsp;text.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-embed_query"><strong>embed_query</strong></a>(self, text: 'str') -&gt; 'List[float]'</dt><dd><tt>Call&nbsp;out&nbsp;to&nbsp;<a href="#OpenAI">OpenAI</a>'s&nbsp;embedding&nbsp;endpoint&nbsp;for&nbsp;embedding&nbsp;query&nbsp;text.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;text:&nbsp;The&nbsp;text&nbsp;to&nbsp;embed.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Embedding&nbsp;for&nbsp;the&nbsp;text.</tt></dd></dl>

<hr>
Class methods inherited from <a href="langchain_openai.embeddings.base.html#OpenAIEmbeddings">langchain_openai.embeddings.base.OpenAIEmbeddings</a>:<br>
<dl><dt><a name="OpenAIEmbeddings-build_extra"><strong>build_extra</strong></a>(values: 'Dict[str, Any]') -&gt; 'Dict[str, Any]'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Build&nbsp;extra&nbsp;kwargs&nbsp;from&nbsp;additional&nbsp;params&nbsp;that&nbsp;were&nbsp;passed&nbsp;in.</tt></dd></dl>

<hr>
Data and other attributes inherited from <a href="langchain_openai.embeddings.base.html#OpenAIEmbeddings">langchain_openai.embeddings.base.OpenAIEmbeddings</a>:<br>
<dl><dt><strong>Config</strong> = &lt;class 'langchain_openai.embeddings.base.OpenAIEmbeddings.Config'&gt;<dd><tt>Configuration&nbsp;for&nbsp;this&nbsp;pydantic&nbsp;object.</tt></dl>

<hr>
Methods inherited from <a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a>:<br>
<dl><dt><a name="OpenAIEmbeddings-__eq__"><strong>__eq__</strong></a>(self, other: Any) -&gt; bool</dt><dd><tt>Return&nbsp;self==value.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__getstate__"><strong>__getstate__</strong></a>(self) -&gt; 'DictAny'</dt></dl>

<dl><dt><a name="OpenAIEmbeddings-__iter__"><strong>__iter__</strong></a>(self) -&gt; 'TupleGenerator'</dt><dd><tt>so&nbsp;`<a href="#OpenAIEmbeddings-dict">dict</a>(model)`&nbsp;works</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__repr_args__"><strong>__repr_args__</strong></a>(self) -&gt; 'ReprArgs'</dt><dd><tt>Returns&nbsp;the&nbsp;attributes&nbsp;to&nbsp;show&nbsp;in&nbsp;__str__,&nbsp;__repr__,&nbsp;and&nbsp;__pretty__&nbsp;this&nbsp;is&nbsp;generally&nbsp;overridden.<br>
&nbsp;<br>
Can&nbsp;either&nbsp;return:<br>
*&nbsp;name&nbsp;-&nbsp;value&nbsp;pairs,&nbsp;e.g.:&nbsp;`[('foo_name',&nbsp;'foo'),&nbsp;('bar_name',&nbsp;['b',&nbsp;'a',&nbsp;'r'])]`<br>
*&nbsp;or,&nbsp;just&nbsp;values,&nbsp;e.g.:&nbsp;`[(None,&nbsp;'foo'),&nbsp;(None,&nbsp;['b',&nbsp;'a',&nbsp;'r'])]`</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__setattr__"><strong>__setattr__</strong></a>(self, name, value)</dt><dd><tt>Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__setstate__"><strong>__setstate__</strong></a>(self, state: 'DictAny') -&gt; None</dt></dl>

<dl><dt><a name="OpenAIEmbeddings-copy"><strong>copy</strong></a>(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -&gt; 'Model'</dt><dd><tt>Duplicate&nbsp;a&nbsp;model,&nbsp;optionally&nbsp;choose&nbsp;which&nbsp;fields&nbsp;to&nbsp;include,&nbsp;exclude&nbsp;and&nbsp;change.<br>
&nbsp;<br>
:param&nbsp;include:&nbsp;fields&nbsp;to&nbsp;include&nbsp;in&nbsp;new&nbsp;model<br>
:param&nbsp;exclude:&nbsp;fields&nbsp;to&nbsp;exclude&nbsp;from&nbsp;new&nbsp;model,&nbsp;as&nbsp;with&nbsp;values&nbsp;this&nbsp;takes&nbsp;precedence&nbsp;over&nbsp;include<br>
:param&nbsp;update:&nbsp;values&nbsp;to&nbsp;change/add&nbsp;in&nbsp;the&nbsp;new&nbsp;model.&nbsp;Note:&nbsp;the&nbsp;data&nbsp;is&nbsp;not&nbsp;validated&nbsp;before&nbsp;creating<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;new&nbsp;model:&nbsp;you&nbsp;should&nbsp;trust&nbsp;this&nbsp;data<br>
:param&nbsp;deep:&nbsp;set&nbsp;to&nbsp;`True`&nbsp;to&nbsp;make&nbsp;a&nbsp;deep&nbsp;copy&nbsp;of&nbsp;the&nbsp;model<br>
:return:&nbsp;new&nbsp;model&nbsp;instance</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-dict"><strong>dict</strong></a>(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) -&gt; 'DictStrAny'</dt><dd><tt>Generate&nbsp;a&nbsp;dictionary&nbsp;representation&nbsp;of&nbsp;the&nbsp;model,&nbsp;optionally&nbsp;specifying&nbsp;which&nbsp;fields&nbsp;to&nbsp;include&nbsp;or&nbsp;exclude.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-json"><strong>json</strong></a>(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -&gt; str</dt><dd><tt>Generate&nbsp;a&nbsp;JSON&nbsp;representation&nbsp;of&nbsp;the&nbsp;model,&nbsp;`include`&nbsp;and&nbsp;`exclude`&nbsp;arguments&nbsp;as&nbsp;per&nbsp;`<a href="#OpenAIEmbeddings-dict">dict</a>()`.<br>
&nbsp;<br>
`encoder`&nbsp;is&nbsp;an&nbsp;optional&nbsp;function&nbsp;to&nbsp;supply&nbsp;as&nbsp;`default`&nbsp;to&nbsp;json.dumps(),&nbsp;other&nbsp;arguments&nbsp;as&nbsp;per&nbsp;`json.dumps()`.</tt></dd></dl>

<hr>
Class methods inherited from <a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a>:<br>
<dl><dt><a name="OpenAIEmbeddings-__get_validators__"><strong>__get_validators__</strong></a>() -&gt; 'CallableGenerator'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAIEmbeddings-__try_update_forward_refs__"><strong>__try_update_forward_refs__</strong></a>(**localns: Any) -&gt; None<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Same&nbsp;as&nbsp;update_forward_refs&nbsp;but&nbsp;will&nbsp;not&nbsp;raise&nbsp;exception<br>
when&nbsp;forward&nbsp;references&nbsp;are&nbsp;not&nbsp;defined.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-construct"><strong>construct</strong></a>(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Creates&nbsp;a&nbsp;new&nbsp;model&nbsp;setting&nbsp;__dict__&nbsp;and&nbsp;__fields_set__&nbsp;from&nbsp;trusted&nbsp;or&nbsp;pre-validated&nbsp;data.<br>
Default&nbsp;values&nbsp;are&nbsp;respected,&nbsp;but&nbsp;no&nbsp;other&nbsp;validation&nbsp;is&nbsp;performed.<br>
Behaves&nbsp;as&nbsp;if&nbsp;`Config.extra&nbsp;=&nbsp;'allow'`&nbsp;was&nbsp;set&nbsp;since&nbsp;it&nbsp;adds&nbsp;all&nbsp;passed&nbsp;values</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-from_orm"><strong>from_orm</strong></a>(obj: Any) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAIEmbeddings-parse_file"><strong>parse_file</strong></a>(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAIEmbeddings-parse_obj"><strong>parse_obj</strong></a>(obj: Any) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAIEmbeddings-parse_raw"><strong>parse_raw</strong></a>(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAIEmbeddings-schema"><strong>schema</strong></a>(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -&gt; 'DictStrAny'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAIEmbeddings-schema_json"><strong>schema_json</strong></a>(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -&gt; str<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAIEmbeddings-update_forward_refs"><strong>update_forward_refs</strong></a>(**localns: Any) -&gt; None<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Try&nbsp;to&nbsp;update&nbsp;ForwardRefs&nbsp;on&nbsp;fields&nbsp;based&nbsp;on&nbsp;this&nbsp;Model,&nbsp;globalns&nbsp;and&nbsp;localns.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-validate"><strong>validate</strong></a>(value: Any) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<hr>
Data descriptors inherited from <a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__fields_set__</strong></dt>
</dl>
<hr>
Methods inherited from <a href="pydantic.v1.utils.html#Representation">pydantic.v1.utils.Representation</a>:<br>
<dl><dt><a name="OpenAIEmbeddings-__pretty__"><strong>__pretty__</strong></a>(self, fmt: Callable[[Any], Any], **kwargs: Any) -&gt; Generator[Any, NoneType, NoneType]</dt><dd><tt>Used&nbsp;by&nbsp;devtools&nbsp;(<a href="https://python-devtools.helpmanual.io/">https://python-devtools.helpmanual.io/</a>)&nbsp;to&nbsp;provide&nbsp;a&nbsp;human&nbsp;readable&nbsp;representations&nbsp;of&nbsp;objects</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__repr__"><strong>__repr__</strong></a>(self) -&gt; str</dt><dd><tt>Return&nbsp;repr(self).</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__repr_name__"><strong>__repr_name__</strong></a>(self) -&gt; str</dt><dd><tt>Name&nbsp;of&nbsp;the&nbsp;instance's&nbsp;class,&nbsp;used&nbsp;in&nbsp;__repr__.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__repr_str__"><strong>__repr_str__</strong></a>(self, join_str: str) -&gt; str</dt></dl>

<dl><dt><a name="OpenAIEmbeddings-__rich_repr__"><strong>__rich_repr__</strong></a>(self) -&gt; 'RichReprResult'</dt><dd><tt>Get&nbsp;fields&nbsp;for&nbsp;Rich&nbsp;library</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__str__"><strong>__str__</strong></a>(self) -&gt; str</dt><dd><tt>Return&nbsp;str(self).</tt></dd></dl>

</td></tr></table> <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="ProxyOpenAI">class <strong>ProxyOpenAI</strong></a>(<a href="gen_ai_hub.proxy.langchain.base.html#BaseAuth">gen_ai_hub.proxy.langchain.base.BaseAuth</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt><a href="#ProxyOpenAI">ProxyOpenAI</a>(*,&nbsp;proxy_client:&nbsp;Optional[Any]&nbsp;=&nbsp;None,&nbsp;deployment_id:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;config_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;config_id:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;proxy_model_name:&nbsp;Optional[str]&nbsp;=&nbsp;None)&nbsp;-&amp;gt;&nbsp;None<br>
&nbsp;<br>
<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%"><dl><dt>Method resolution order:</dt>
<dd><a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a></dd>
<dd><a href="gen_ai_hub.proxy.langchain.base.html#BaseAuth">gen_ai_hub.proxy.langchain.base.BaseAuth</a></dd>
<dd><a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a></dd>
<dd><a href="pydantic.v1.utils.html#Representation">pydantic.v1.utils.Representation</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Class methods defined here:<br>
<dl><dt><a name="ProxyOpenAI-validate_clients"><strong>validate_clients</strong></a>(values: Dict) -&gt; Dict<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<hr>
Static methods defined here:<br>
<dl><dt><a name="ProxyOpenAI-__json_encoder__"><strong>__json_encoder__</strong></a> = pydantic_encoder(obj: Any) -&gt; Any</dt></dl>

<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>__abstractmethods__</strong> = frozenset()</dl>

<dl><dt><strong>__class_vars__</strong> = set()</dl>

<dl><dt><strong>__config__</strong> = &lt;class 'pydantic.v1.config.Config'&gt;</dl>

<dl><dt><strong>__custom_root_type__</strong> = False</dl>

<dl><dt><strong>__exclude_fields__</strong> = None</dl>

<dl><dt><strong>__fields__</strong> = {'config_id': ModelField(name='config_id', type=Optional[str], required=False, default=None), 'config_name': ModelField(name='config_name', type=Optional[str], required=False, default=None), 'deployment_id': ModelField(name='deployment_id', type=Optional[str], required=False, default=None), 'proxy_client': ModelField(name='proxy_client', type=Optional[Any], required=False, default=None), 'proxy_model_name': ModelField(name='proxy_model_name', type=Optional[str], required=False, default=None)}</dl>

<dl><dt><strong>__hash__</strong> = None</dl>

<dl><dt><strong>__include_fields__</strong> = None</dl>

<dl><dt><strong>__post_root_validators__</strong> = []</dl>

<dl><dt><strong>__pre_root_validators__</strong> = []</dl>

<dl><dt><strong>__private_attributes__</strong> = {}</dl>

<dl><dt><strong>__schema_cache__</strong> = {}</dl>

<dl><dt><strong>__signature__</strong> = &lt;Signature (*, proxy_client: Optional[Any] = Non... proxy_model_name: Optional[str] = None) -&gt; None&gt;</dl>

<dl><dt><strong>__validators__</strong> = {}</dl>

<hr>
Data and other attributes inherited from <a href="gen_ai_hub.proxy.langchain.base.html#BaseAuth">gen_ai_hub.proxy.langchain.base.BaseAuth</a>:<br>
<dl><dt><strong>__annotations__</strong> = {'config_id': typing.Optional[str], 'config_name': typing.Optional[str], 'deployment_id': typing.Optional[str], 'proxy_client': typing.Optional[typing.Any], 'proxy_model_name': typing.Optional[str]}</dl>

<hr>
Methods inherited from <a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a>:<br>
<dl><dt><a name="ProxyOpenAI-__eq__"><strong>__eq__</strong></a>(self, other: Any) -&gt; bool</dt><dd><tt>Return&nbsp;self==value.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__getstate__"><strong>__getstate__</strong></a>(self) -&gt; 'DictAny'</dt></dl>

<dl><dt><a name="ProxyOpenAI-__init__"><strong>__init__</strong></a>(__pydantic_self__, **data: Any) -&gt; None</dt><dd><tt>Create&nbsp;a&nbsp;new&nbsp;model&nbsp;by&nbsp;parsing&nbsp;and&nbsp;validating&nbsp;input&nbsp;data&nbsp;from&nbsp;keyword&nbsp;arguments.<br>
&nbsp;<br>
Raises&nbsp;ValidationError&nbsp;if&nbsp;the&nbsp;input&nbsp;data&nbsp;cannot&nbsp;be&nbsp;parsed&nbsp;to&nbsp;form&nbsp;a&nbsp;valid&nbsp;model.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__iter__"><strong>__iter__</strong></a>(self) -&gt; 'TupleGenerator'</dt><dd><tt>so&nbsp;`<a href="#ProxyOpenAI-dict">dict</a>(model)`&nbsp;works</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__repr_args__"><strong>__repr_args__</strong></a>(self) -&gt; 'ReprArgs'</dt><dd><tt>Returns&nbsp;the&nbsp;attributes&nbsp;to&nbsp;show&nbsp;in&nbsp;__str__,&nbsp;__repr__,&nbsp;and&nbsp;__pretty__&nbsp;this&nbsp;is&nbsp;generally&nbsp;overridden.<br>
&nbsp;<br>
Can&nbsp;either&nbsp;return:<br>
*&nbsp;name&nbsp;-&nbsp;value&nbsp;pairs,&nbsp;e.g.:&nbsp;`[('foo_name',&nbsp;'foo'),&nbsp;('bar_name',&nbsp;['b',&nbsp;'a',&nbsp;'r'])]`<br>
*&nbsp;or,&nbsp;just&nbsp;values,&nbsp;e.g.:&nbsp;`[(None,&nbsp;'foo'),&nbsp;(None,&nbsp;['b',&nbsp;'a',&nbsp;'r'])]`</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__setattr__"><strong>__setattr__</strong></a>(self, name, value)</dt><dd><tt>Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__setstate__"><strong>__setstate__</strong></a>(self, state: 'DictAny') -&gt; None</dt></dl>

<dl><dt><a name="ProxyOpenAI-copy"><strong>copy</strong></a>(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -&gt; 'Model'</dt><dd><tt>Duplicate&nbsp;a&nbsp;model,&nbsp;optionally&nbsp;choose&nbsp;which&nbsp;fields&nbsp;to&nbsp;include,&nbsp;exclude&nbsp;and&nbsp;change.<br>
&nbsp;<br>
:param&nbsp;include:&nbsp;fields&nbsp;to&nbsp;include&nbsp;in&nbsp;new&nbsp;model<br>
:param&nbsp;exclude:&nbsp;fields&nbsp;to&nbsp;exclude&nbsp;from&nbsp;new&nbsp;model,&nbsp;as&nbsp;with&nbsp;values&nbsp;this&nbsp;takes&nbsp;precedence&nbsp;over&nbsp;include<br>
:param&nbsp;update:&nbsp;values&nbsp;to&nbsp;change/add&nbsp;in&nbsp;the&nbsp;new&nbsp;model.&nbsp;Note:&nbsp;the&nbsp;data&nbsp;is&nbsp;not&nbsp;validated&nbsp;before&nbsp;creating<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;new&nbsp;model:&nbsp;you&nbsp;should&nbsp;trust&nbsp;this&nbsp;data<br>
:param&nbsp;deep:&nbsp;set&nbsp;to&nbsp;`True`&nbsp;to&nbsp;make&nbsp;a&nbsp;deep&nbsp;copy&nbsp;of&nbsp;the&nbsp;model<br>
:return:&nbsp;new&nbsp;model&nbsp;instance</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-dict"><strong>dict</strong></a>(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) -&gt; 'DictStrAny'</dt><dd><tt>Generate&nbsp;a&nbsp;dictionary&nbsp;representation&nbsp;of&nbsp;the&nbsp;model,&nbsp;optionally&nbsp;specifying&nbsp;which&nbsp;fields&nbsp;to&nbsp;include&nbsp;or&nbsp;exclude.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-json"><strong>json</strong></a>(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -&gt; str</dt><dd><tt>Generate&nbsp;a&nbsp;JSON&nbsp;representation&nbsp;of&nbsp;the&nbsp;model,&nbsp;`include`&nbsp;and&nbsp;`exclude`&nbsp;arguments&nbsp;as&nbsp;per&nbsp;`<a href="#ProxyOpenAI-dict">dict</a>()`.<br>
&nbsp;<br>
`encoder`&nbsp;is&nbsp;an&nbsp;optional&nbsp;function&nbsp;to&nbsp;supply&nbsp;as&nbsp;`default`&nbsp;to&nbsp;json.dumps(),&nbsp;other&nbsp;arguments&nbsp;as&nbsp;per&nbsp;`json.dumps()`.</tt></dd></dl>

<hr>
Class methods inherited from <a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a>:<br>
<dl><dt><a name="ProxyOpenAI-__get_validators__"><strong>__get_validators__</strong></a>() -&gt; 'CallableGenerator'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ProxyOpenAI-__try_update_forward_refs__"><strong>__try_update_forward_refs__</strong></a>(**localns: Any) -&gt; None<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Same&nbsp;as&nbsp;update_forward_refs&nbsp;but&nbsp;will&nbsp;not&nbsp;raise&nbsp;exception<br>
when&nbsp;forward&nbsp;references&nbsp;are&nbsp;not&nbsp;defined.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-construct"><strong>construct</strong></a>(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Creates&nbsp;a&nbsp;new&nbsp;model&nbsp;setting&nbsp;__dict__&nbsp;and&nbsp;__fields_set__&nbsp;from&nbsp;trusted&nbsp;or&nbsp;pre-validated&nbsp;data.<br>
Default&nbsp;values&nbsp;are&nbsp;respected,&nbsp;but&nbsp;no&nbsp;other&nbsp;validation&nbsp;is&nbsp;performed.<br>
Behaves&nbsp;as&nbsp;if&nbsp;`Config.extra&nbsp;=&nbsp;'allow'`&nbsp;was&nbsp;set&nbsp;since&nbsp;it&nbsp;adds&nbsp;all&nbsp;passed&nbsp;values</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-from_orm"><strong>from_orm</strong></a>(obj: Any) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ProxyOpenAI-parse_file"><strong>parse_file</strong></a>(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ProxyOpenAI-parse_obj"><strong>parse_obj</strong></a>(obj: Any) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ProxyOpenAI-parse_raw"><strong>parse_raw</strong></a>(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ProxyOpenAI-schema"><strong>schema</strong></a>(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -&gt; 'DictStrAny'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ProxyOpenAI-schema_json"><strong>schema_json</strong></a>(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -&gt; str<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ProxyOpenAI-update_forward_refs"><strong>update_forward_refs</strong></a>(**localns: Any) -&gt; None<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Try&nbsp;to&nbsp;update&nbsp;ForwardRefs&nbsp;on&nbsp;fields&nbsp;based&nbsp;on&nbsp;this&nbsp;Model,&nbsp;globalns&nbsp;and&nbsp;localns.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-validate"><strong>validate</strong></a>(value: Any) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<hr>
Data descriptors inherited from <a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__fields_set__</strong></dt>
</dl>
<hr>
Data and other attributes inherited from <a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a>:<br>
<dl><dt><strong>Config</strong> = &lt;class 'pydantic.v1.config.BaseConfig'&gt;</dl>

<hr>
Methods inherited from <a href="pydantic.v1.utils.html#Representation">pydantic.v1.utils.Representation</a>:<br>
<dl><dt><a name="ProxyOpenAI-__pretty__"><strong>__pretty__</strong></a>(self, fmt: Callable[[Any], Any], **kwargs: Any) -&gt; Generator[Any, NoneType, NoneType]</dt><dd><tt>Used&nbsp;by&nbsp;devtools&nbsp;(<a href="https://python-devtools.helpmanual.io/">https://python-devtools.helpmanual.io/</a>)&nbsp;to&nbsp;provide&nbsp;a&nbsp;human&nbsp;readable&nbsp;representations&nbsp;of&nbsp;objects</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__repr__"><strong>__repr__</strong></a>(self) -&gt; str</dt><dd><tt>Return&nbsp;repr(self).</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__repr_name__"><strong>__repr_name__</strong></a>(self) -&gt; str</dt><dd><tt>Name&nbsp;of&nbsp;the&nbsp;instance's&nbsp;class,&nbsp;used&nbsp;in&nbsp;__repr__.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__repr_str__"><strong>__repr_str__</strong></a>(self, join_str: str) -&gt; str</dt></dl>

<dl><dt><a name="ProxyOpenAI-__rich_repr__"><strong>__rich_repr__</strong></a>(self) -&gt; 'RichReprResult'</dt><dd><tt>Get&nbsp;fields&nbsp;for&nbsp;Rich&nbsp;library</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__str__"><strong>__str__</strong></a>(self) -&gt; str</dt><dd><tt>Return&nbsp;str(self).</tt></dd></dl>

</td></tr></table></td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#eeaa77">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Functions</strong></big></font></td></tr>
    
<tr><td bgcolor="#eeaa77"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><dl><dt><a name="-get_client_parms"><strong>get_client_parms</strong></a>(values)</dt></dl>
 <dl><dt><a name="-init_chat_model"><strong>init_chat_model</strong></a>(proxy_client: gen_ai_hub.proxy.core.base.BaseProxyClient, deployment: gen_ai_hub.proxy.core.base.BaseDeployment, temperature: float = 0.0, max_tokens: int = 256, top_k: Optional[int] = None, top_p: float = 1.0)</dt></dl>
 <dl><dt><a name="-init_embedding_model"><strong>init_embedding_model</strong></a>(proxy_client: gen_ai_hub.proxy.core.base.BaseProxyClient, deployment: gen_ai_hub.proxy.core.base.BaseDeployment)</dt></dl>
</td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#55aa55">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Data</strong></big></font></td></tr>
    
<tr><td bgcolor="#55aa55"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><strong>Any</strong> = typing.Any<br>
<strong>DEFAULT_API_VERSION</strong> = None<br>
<strong>Dict</strong> = typing.Dict<br>
<strong>List</strong> = typing.List<br>
<strong>Optional</strong> = typing.Optional<br>
<strong>catalog</strong> = &lt;gen_ai_hub.proxy.langchain.init_models.Catalog object&gt;<br>
<strong>openai_err</strong> = None</td></tr></table>
</body></html>